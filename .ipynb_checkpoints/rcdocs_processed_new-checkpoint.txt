https://rc-docs.northeastern.edu/en/latest/welcome/services.html

Services We Provide#

Facilitation of HPC resources#

What type of support does the Research Computing team at Northeastern University provide for users of the high-performance computing (HPC) resources?
The Research Computing team at NU offer technical support to all researchers using NU’s high-performance computing (HPC) resources. Our technical experts assist users in harnessing the full potential of the cluster’s computational resources. Our services are designed to empower you to take full advantage of the HPC resources available for your research.

We have many file storage solutions available to members of the Northeastern Community and provide tailored solutions to cater to both long-term high-capacity storage needs and dynamic storage requirements for active research. Our technical team works closely with researchers to design data management strategies, facilitating seamless data movement, backup, and restoration.

How does Research Computing at Northeastern University support classroom integration of high-performance computing into curricula?
At Research Computing we extend our expertise beyond research 
contexts to provide comprehensive support for classroom use of NU’s computational resources. Our skilled team collaborates with educators to integrate high-performance computing into curricula, enhancing students’ learning experiences. We offer tailored workshops and training sessions that empower instructors to incorporate computational techniques into their courses. From introductory coding exercises to advanced simulations, we assist in designing educational content that leverages the power of our computing cluster. Our technical staff ensures that students have access to the necessary tools and resources, enabling them to explore real-world applications of computational methods.


How can I schedule a consultation with the Research Computing team at Northeastern University for assistance with research computing and data storage, and what are the details of these consultations?
We encourage you to schedule a consultation with one of our staff members to receive personal, one-on-one assistance for your research computing and data storage needs. Consultations are available to any Northeastern student, faculty, or staff member. We can help you get started on the cluster, optimize your code, benchmark, install and use software packages, detail data storage options, and more.
We offer consultations by appointment most weekdays during regular business hours (9 a.m. to 5 p.m.). Just so you know, we follow the Northeastern University holiday schedule, so no consultations are available on holidays or during breaks. All consultations are conducted online through the Teams app.
Use our Bookings page (link - https://rc.northeastern.edu/support/consulting/) to view our availability and schedule an appointment. You must sign in using your @northeastern.edu email address (for example, a.student@northeastern.edu).

Training sessions.
The RC Team provides free online training sessions for cluster users and Northeastern Students interested in learning more.
If you’re new to Discovery or HPC, we strongly recommend watching the Discovery Introduction Video.  You’ll need to sign in using your Northeastern username and password to view the video.
Recorded RC training sessions have been added to the Research Computing Training Course in Canvas. Once you’ve clicked the link to enroll, you must log in using your Northeastern credentials.
The training sessions cover a wide range of skill levels, from beginner to advanced, and offer you the opportunity to explore training sessions at your own pace in the areas that you’re most interested in (e.g., Linux and Shell Scripting, Slurm Arrays, Software Installation, GPUs, Bioinformatics, Deep Learning, and LAMMPs). You can perform hands-on training exercises by accessing RC files shared on GitHub repositories.
If you are a faculty member using Discovery with your class and would like us to develop or present a personalized training session for your class, please reach out to us at rc.northeastern.edu. All Discovery users can also reach out with suggestions for future training topics!

Collaboration.
The team at RC @ NU is composed of full-time PhD scientists whose expertise spans diverse fields (e.g., Machine Learning, Data Analytics, Molecular Dynamics, Bioinformatics, and Big Data). We are eagerly open to collaboration with researchers in multiple phases of their research including:

Grant/proposal development and idea generation
Data cleaning and transforming
Data analysis
Statistical modeling
Data visualization (exploratory and publication quality)
Container building/development
HPC workflow optimization
Final publication preparation
Course Development
Reproducibility

Please reach out via a consultation or by emailing rchelp@northeastern.edu if you’re interested any of the above (or more) services.

Citation Information#
If you feel your research has benefited from support from RC @ NU or want to acknowledge the cluster please use one of the statements below:
This work was completed in part using the Discovery cluster, supported by Northeastern University’s Research Computing team.
This work was completed using the Discovery cluster at MGHPC for Northeastern University.

https://rc-docs.northeastern.edu/en/latest/welcome/gettinghelp.html

How can i Get Help
If you need help, you can contact the Research Computing (RC) team via email, ServiceNow ticket, or schedule an appointment through our Bookings page.

What is the Email contact for Research computing RC team ? 
To contact the RC team, email us at rchelp@northeastern.edu. This will generate a ticket in ServiceNow. Could you be sure to include details about your question or issue, including any commands or scripts you use so that we can direct you to the right person?


How do I submit a service request?
Submit a ticket#
To submit a ticket in ServiceNow, select from the RC ServiceNow catalog. You may need to sign in with your Northeastern username and password to view the catalog.
RC users can request services using our ticket system. You can go ahead and select the appropriate category below to access the online ticket.

 Get Assistance with RC link - https://bit.ly/NURC-Assistance

 RC Access Form - https://bit.ly/NURC-AccessRequest

 Software Request Form link - https://bit.ly/NURC-Software

 Documentation Request link - https://bit.ly/NURC-Documentation

 Partition Access Request link - https://bit.ly/NURC-PartitionAccess

 Storage Request link - https://bit.ly/NURC-NewStorage

 Storage Extension Request link - https://bit.ly/NURC-StorageExtension

 Data Transfer Consultation link - https://bit.ly/NURC-DataTransfer

 Classroom Request Form link - https://bit.ly/NURC-Classroom

 Unsubscribe link - https://bit.ly/NURC-Unsubscribe


Update Ticket
To check for updates on a submitted ticket, please follow these steps:


Log in to your ServiceNow (https://service.northeastern.edu/tech)  account.
Select “My Tickets” to access a list of all your active tickets.
In the ticket list, you can view the latest updates made to each ticket.


In addition, you will receive an email notification from ServiceNow mentioning your incident number. You can directly access the ServiceNow portal to view the updates on your ticket by following these steps:

Open the email and locate the incident number mentioned in the message.
Select the incident number to redirect you to the ServiceNow portal.
In the ServiceNow portal, you can see the updates made to your ticket.



Training
The RC Team provides free online training sessions for cluster users and Northeastern Students interested in learning more.
If you’re new to Discovery, we strongly recommend watching the Discovery Introduction Video (https://www.linkedin.com/checkpoint/enterprise/login/74653650?pathWildcard=74653650&application=learning&redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Flearning%2Fcontent%2F1139340%3Fu%3D74653650).  You’ll need to sign in using your Northeastern username and password to view the video.
Recorded RC training sessions have been added to the Research Computing Training Course (https://northeastern.instructure.com/enroll/LNNCHN)in Canvas. Once you have clicked the link to enroll, you must log in using your Northeastern credentials.
The training sessions cover a wide range of skill levels, from beginner to advanced, and offer you the opportunity to explore training sessions at your own pace in the areas that you’re most interested in (e.g., Linux and Shell Scripting, Slurm Arrays, Software Installation, GPUs, Bioinformatics, Deep Learning, and LAMMPs). You can perform hands-on training exercises by accessing RC files shared on GitHub repositories.
If you are a faculty member using Discovery with your class and would like us to develop or present a personalized training session for your class, please reach out to us at rc.northeastern.edu. All Discovery users can also reach out with suggestions for future training topics!

Status Page
We use the ITS Systems Status page to post important information, such as power outages and maintenance windows. You can subscribe to this page to receive email updates on the status of ITS systems.

https://rc-docs.northeastern.edu/en/latest/welcome/introtocluster.html

Introduction to HPC and Slurm

Where is the high-performance computing (HPC) cluster
Our cluster is a high-performance computing (HPC) resource available to the Northeastern University research community. The cluster is in the Massachusetts Green HPC Center (MGHPC) in Holyoke, MA. MGHPC is a 90,000 square food, 15-megawatt research computing and data center facility that houses computing resources for five institutions: Northeastern University, Boston University, Harvard University, the Massachusetts Institute of Technology (MIT), and the University of Massachesetts (UMass).

what is Slurm used for?
We use the job scheduling manager slurm to perform vital actions in the cluster, including allocating access to a compute node for a duration of time so users can do their work. Slurm also allows the work that is performed on the cluster to be monitored (e.g., start-time, end-time, and memory resources used), and it manages the use of computing resources through a queue of pending jobs.
To learn more about how to use slurm on the Northeastern cluster please see our  slurm documentation or reach out to rchelp@northeastern.edu for any additional questions.




https://rc-docs.northeastern.edu/en/latest/gettingstarted/get_access.html

Getting Access#

Request Account#

Important
If you previously had access to Discovery but are now working with a different PI, you should submit a ServiceNow RC Access Request form and enter the name of your current PI in the Sponsor field. This will link your account to your current PI and expedite updating your account with any of your current PI’s resources on Discovery, such as shared storage or a private partition.

Valid NU Credentials
Access to the cluster is limited to Northeastern affiliates with a valid NU username and password. Research Computing cannot create or renew Northeastern accounts. You must work with your sponsor to obtain or update your credentials.
For non-Northeastern personnel, request a Northeastern sponsored via How do I submit a sponsored account request?

How do I request an account to access the Discovery high-performance computing resource at Northeastern University. What steps should I follow if I'm new or a visiting researcher?

To access Discovery, you must first have an account. You can request an account through ServiceNow but need a Northeastern username and password. If you are new to the university or a visiting researcher, you should work with your sponsor to obtain a Northeastern username and password.
To request an account, follow these steps:

Visit the ServiceNow RC Access Request form.
Complete the form, check the acknowledgment box, and submit it.

Your request may take up to 24 hours after your sponsor approves it (see Sponsor Approval Process below). You will receive an email confirmation when your access has been granted. Once you have access, if you are unfamiliar with Discovery, high-performance computing, or Linux, you may want to take one of our training courses. Visit the Research Computing website for more information about our training and services.

What is the sponsor approval process for HPC access requests?
Sponsor Approval Process#

PI and instructor access
If you are a PI, professor, or instructor at Northeastern and need access to the cluster, use the access form in the above procedure and enter your name in the Sponsor Name field.

HPC users need a sponsor, usually a NU PI or professor, to approve their request. PIs, professors, and instructors can sponsor themselves. Students (undergraduate or graduate), visiting researchers, or staff members must have a sponsor approve their request. When you fill out the ServiceNow form, an email is sent to the specified sponsor upon submitting the request. Sponsors will receive email reminders until they approve the request through the link in the email to ServiceNow. We recommend letting your sponsor know to look for the email with the approval link before submitting an access request.

Cluster Usage
DO NOT USE the login node for CPU-intensive activities, as this will impact the performance of this node for all cluster users. It will also not provide the best performance for the tasks you are trying to accomplish.


Important
If you are attempting to run a job, you should move to a compute node. You can do this interactively using the srun command or non-interactively using the sbatch command.


Routine Cluster Maintenance#
Routine cluster maintenance is performed on the first Tuesday of each month. RC sends maintenance emails to inform users of upcoming maintenance window, a description of the maintenance, and how users will be affected.



MGHPCC Annual Shutdown
The Massachusetts Green High Performance Computing Center (MGHPCC) conducts an annual shutdown for maintenance work. During this shutdown, all RC-managed services are powered down and unavailable for approximately four days. RC will send frequent reminders leading up to the shutdown to ensure that users are able to plan accordingly.

IT Statuspage
All routine cluster maintenance, emergency maintenance, and annual shutdown maintenance information will be posted to the [IT Statuspage]. Please subscribe to ensure you receive updates on the status of all ITS systems.

What steps should I take to prepare my computational jobs for upcoming maintenance periods on the HPC cluster?
Preparing Cluster Maintenance
To ensure that your job scripts account for the scheduled shutdown period of the cluster, use the t2sd script in the --time option when submitting your jobs. This script calculates the remaining time until the cluster becomes unavailable and sets the appropriate time limit for your job. Here is an example of how to use it.

If you usually use the srun command:

srun --time=$(t2sd) <srun args>

If you usually use the sbatch command to submit batch jobs:

sbatch --time=$(t2sd) script.sbatch

Note that if you usually run your jobs on a partition with short time limits (e.g., debug or express), you only need to add the $(t2sd) option once it is closer to the start of the maintenance window. Use $(t2sd) only if the time remaining before the start of the maintenance period is less than the default time limit of the partition.
For instance, the default time limit for the express partition is 60 minutes. If you want to run a job on the express partition a day before the maintenance is scheduled to start, you would not need to add the $(t2sd) option. However, if you wanted to run your job on the express partition 2 hours before the maintenance start time, you would need to include the $(t2sd) option to account for the remaining time.


Important
Ensuring that your job scripts account for the scheduled maintenance of the cluster is applicable to jobs running on private partitions as well.

Moreover, we can help you set up a default and maximum time configuration on your partition. This configuration can significantly alleviate the issues you may experience with job runtime. By defining default and maximum time limits, you can establish a predefined window for job execution without explicitly specifying the runtime for each job.
However, note that even with the default and maximum time configuration in place, there will always be a time equal to the default time limit where explicitly specifying the job’s runtime becomes helpful. This allows for better control and management of job scheduling within the available resources.
If you want to set up the default and maximum time configuration on your partition or have any concerns or questions regarding job runtime management, please let us know. We are here to assist you further.
Following these instructions ensures that your job scripts consider the maintenance period and set appropriate time limits. If you have any further questions, feel free to ask!

https://rc-docs.northeastern.edu/en/latest/gettingstarted/accountmanager.html

Account Manager
The Account Manager is your gateway to accessing the High-Performance Computing (HPC) cluster. It offers various tools for managing your account, resetting your password, and obtaining the credentials for cluster access.

What is the  steps to log in to the Account Manager? How can i login to Account Manager?
To log in to the Account Manager, follow these steps:

Open a web browser and go to the Account Manager URL: https://account.example.com.
Enter your username and password.
Click the “Login” button.


Managing Your Account
What are the steps to change your password in the Account Manager?
To change your password, follow these steps:

Log in to the Account Manager.
Navigate to the “Change Password” section.
Enter your current password and the new password.
Click the “Change Password” button.



How do you generate SSH keys through the Account Manager for secure access to the cluster? 
SSH keys are required for secure access to the cluster. You can generate SSH keys through the Account Manager by following these steps:

Log in to the Account Manager.
Navigate to the “SSH Keys” section.
Click the “Generate SSH Key” button.
Choose a key type (RSA or DSA) and set the desired key size.
Click the “Generate Key” button.
The public and private keys will be displayed. Save the private key securely.

Key Type: [RSA / DSA]
Key Size: [2048 / 4096 / ...]
Public Key:
[Public Key Contents]
Private Key:
[Private Key Contents - Save Securely]

How do you download access credentials for connecting to the cluster through the Account Manager? 
Downloading Access Credentials 
Access credentials are necessary for connecting to the cluster. You can download your credentials through the Account Manager by following these steps:

Log in to the Account Manager.
Navigate to the “Access Credentials” section.
Click the “Download Credentials” button.
Save the credentials file securely.

How can you get support and assistance?
If you encounter any issues while using the Account Manager or have questions about managing your account, please reach out to our support team at rchelp@northeastern.edu. We’re here to assist you in getting started with your HPC journey!
By utilizing the Account Manager, you can seamlessly manage your account, generate SSH keys, and access the necessary credentials to connect to the cluster. Happy computing!

https://rc-docs.northeastern.edu/en/latest/gettingstarted/connectingtocluster/mac.html

How do you connect to the Discovery cluster on a Mac using the Terminal application?
Connecting on Mac
Mac computers come with a Secure Shell (SSH) program called Terminal that you use to connect to the HPC using SSH. If you need to use software that uses a GUI, such as Matlab or Maestro, make sure to use the -Y option in the second step below using X11 forwarding.

Note
If you use Mac OS X version 10.8 or higher, and you have XQuartz running in the background to do X11 forwarding, you should execute the following command in Terminal once before connecting:
defaults write org.macosforge.xquartz.X11 enable_iglx -bool true
You should keep XQuartz running in the background. If you close and restart XQuartz, you will need to execute the above command again after restarting. Do not use the Terminal application from within XQuartz to sign in to Discovery. Use the default Terminal program that comes with your Mac (see Step 1 in the procedure below).

How do you connect to the Discovery cluster on a Mac using the Terminal application?
Connecting To Cluster On A Mac

Go to Finder > Applications > Utilities, and then double click Terminal.
At the prompt, type ssh <username>@login.discovery.neu.edu, where <username> is your Northeastern username. If you need to use X11 forwarding, type ssh -Y <username>@login.discovery.neu.edu.
Type your Northeastern password and press Enter.

You are now connected to Discovery at a login node.
Watch this video of how to connect to Discovery on a Mac. If you do not see any controls on the video, right-click on the video to see viewing options.


X11 on Mac OS
From a Mac Terminal log in using the -Y option (ssh -Y <yourusername>@login.discovery.neu.edu).

Tip
If you used the -Y option to enable X11 forwarding on your Mac, you can test to see if it is working by typing xeyes. This will run a small program that makes a pair of eyes appear to follow your cursor.

https://rc-docs.northeastern.edu/en/latest/gettingstarted/connectingtocluster/windows.html


What are the steps to connect to the Discovery cluster on a Windows computer?
Connecting on Windows
Before you can connect to the HPC on a Windows computer, you’ll need to download a terminal program,
such as MobaXterm or PuTTY. We recommend MobaXterm, as you can also use it for file transfer,
whereas with other SSH programs, you would need a separate file transfer program.

To connect to cluster with MobaXterm#

Open MobaXterm.
Click Session, then click SSH as the connection type.
In Remote Host, type login.discovery.neu.edu, make sure Port is set to 22, and click OK.
(OPTIONAL: You can type your Northeastern username and password on MobaXterm, and it will save that information every time you sign in. If you opt to do this, you will be connected to Discovery after you click OK.)
At the prompt, type your Northeastern username and press Enter.
Type your Northeastern password and press Enter. Note that the cursor does not move as you type your password. This is expected behavior.

You are now connected to the cluster’s login node.

X11 on Windows#
If you use MobaXterm on Windows, X11 forwarding is turned on by default.

Tip
You can test to see if x11 forwarding is working by typing xeyes. This will run a small program that makes a pair of eyes appear to follow your cursor.


https://rc-docs.northeastern.edu/en/latest/first_steps/passwordlessssh.html

How do you set up passwordless SSH for GUI-based applications
Passwordless SSH
You must set up passwordless ssh to ensure that GUI-based applications launch without issues. Please make sure that your keys are added to the authorized.key file in your ~/.ssh directory. This needs to be done anytime you regenerate your SSH keys. If you are having an issue opening an application that needs X11 forwarding, such as MATLAB or Schrodinger, and you recently regenerated your keys, make sure to add your keys to the authorized.key file.

How do you set up passwordless SSH on a Mac for accessing the Discovery cluster?

Note
Ensure you’re on your local computer for steps 1 through 4—type exit to return to your local computer if connected to the cluster.
To set up passwordless SSH on a Mac:
On a Mac, open the Terminal application and type cd ~/.ssh. This moves you to the ssh folder on your local computer.
Type ssh-keygen -t rsa to generate two files, id_rsa and id_rsa.pub.
Press Enter on all the prompts (do not generate a passphrase).
Type ssh-copy-id -i ~/.ssh/id_rsa.pub <yourusername>@login.discovery.neu.edu to copy id_rsa.pub to your /home/.ssh folder on Discovery. You can enter your NU password if prompted. This copies the token from id_rsa.pub file to the authorized_keys file, which will either be generated or appended if it already exists.
Connect to Discovery via ssh <yourusername>@login.discovery.neu.edu. You should now be connected without having to enter your password.

Now on the cluster,

Type cd ~/.ssh to move to your ssh folder.
Type ssh-keygen -t rsa to generate your key files.
Press Enter on all the prompts (do not generate a passphrase). If prompted to overwrite a file, type Y.
Type cat id_rsa.pub >> authorized_keys. This adds the contents of your public key file to a new line in the ~/.ssh/authorized_keys file.

How do you set up passwordless SSH on a Windows for accessing the Discovery cluster
To set up passwordless ssh on windows:

Sign in to the cluster using MobaXterm.
Type cd ~/.ssh to move to your ssh folder.
Type ssh-keygen -t rsa to generate your key files.
Press Enter on all the prompts (do not generate a passphrase). If prompted to overwrite a file, type Y.
Type cat id_rsa.pub >> authorized_keys. This adds the contents of your public key file to a new line in the ~/.ssh/authorized_keys.
Now on the cluster,

Type cd ~/.ssh to move to your ssh folder.
Type ssh-keygen -t rsa to generate your key files.
Press Enter on all the prompts (do not generate a passphrase). If prompted to overwrite a file, type Y.
Type cat id_rsa.pub >> authorized_keys. This adds the contents of your public key file to a new line in the ~/.ssh/authorized_keys file.

Note
Errors that you can see on Windows when launching a GUI-based program include the following:

Error: unable to open display localhost:19.0
Launch failed: non-zero return code

If you are getting these types of errors, it could be because of the following reasons:

You still need to set up passwordless SSH. If so, you can follow the steps below to set up passwordless SSH.
When requesting a compute node from the login node, you may have forgotten to include the --x11 option. Please see this example srun command.

https://rc-docs.northeastern.edu/en/latest/first_steps/shellenvironment.html


Shell Environment on the Cluster#

How can I configure my user environment and settings on the Discovery platform?
The Discovery Shell environment and .bashrc#
Discovery uses a Linux-based Operating System (CentOS), where the Shell program interfaces with the user. Bash (Bourne Again SHell) is one of the most popular Shell implementations, the default Shell on Discovery.
The Shell script .bashrc is used by Bash to initialize your Shell environment. For example, it is typically used to define aliases, functions, and load modules. Note that environment variables settings (such as PATH) generally go in the .bash_profile or .profile files. Your .bashrc, .bash_profile, and .profile files live in your $HOME directory. You can change your .bashrc with a text editor like nano.

Caution
Making edits to your .bashrc file can result in many issues. Some changes may prevent you from launching apps or executing commands. Modifying your PATH variable may result in the inability to use basic Shell commands (such as cd or ls) if not done correctly.
Before making changes to your .bashrc file, make a backup of the default .bashrc file, so you can restore it if necessary. If you need help with editing .bashrc, reach out to mailto:rchelp@northeastern.edu or schedule a consultation with a staff member who can help suggest edits and troubleshoot any issues you might be having.

About your .bashrc file#
When your account is created, you have a default .bashrc file in your home directory. See the figure below for an example of a default .bashrc file.

Important
We recommend keeping .bashrc unchanged when using Discovery. You can source environment Shell scripts or load modules directly inside your job instead. This approach can prevent some runtime errors from loading incompatible modules, setting environment variables incorrectly, or mixing multiple software and Conda environments.

Conda and .bashrc#
In addition to editing your .bashrc file as outlined in the example above, programs you install can also modify your .bashrc file. For example, if you follow the procedure outlined in Using Miniconda, there may be a section added to your .bashrc file (if you didn’t use the -b batch option) that automatically loads your conda environment every time you sign in to Discovery. 

You should not modify this section in the .bashrc file directly. If it was changed, remove this section manually using a file editor.

Caution
We recommend removing the conda initialization section from your .bashrc as it may interfere with the correct startup environment when using Open OnDemand apps. You should always load your Conda environment after your job already started.

If you need help with your .bashrc file or would like it restored to its default, reach out to the RC team at mailto:rchelp@northeastern.edu, and we can provide you with
a new default .bashrc file and help troubleshoot issues with the file.

How do i edit bashrc file
Editing your .bashrc file
The basic workflow for editing your .bashrc file is to sign in to Discovery, go to your $HOME directory, open the file in a text editor on the command line, make your edits, save the file, sign out of Discovery, then sign back in again. Your changes will take effect when you have signed back in again.
Example procedure for editing your .bashrc file:

Sign in to Discovery.
(Optional) Type pwd to ensure you are in your /home directory.
(Optional) Type ls -a to view the contents of your /home directory, including hidden files. Your .bashrc file is hidden (hidden files are preceded by a . ). Using the -a option with ls displays hidden files.
(Recommended) Type cp .bashrc .bashrc-default to make a copy of your .bashrc file called .bashrc-default.
Type nano .bashrc to open your .bashrc file in the nano text editor.
Type the edits that you want to make to your file. In this example, an alias was added to create a shortcut to the user’s /scratch space.
Save the file and exit the editor.
Sign out of Discovery and sign back in for the changes to take effect.


How can I set up and use a custom Shell script to configure my environment?
How can I set up and use a custom Shell script to configure my environment for running jobs on a GPU partition?
Sourcing a Shell script example#
A safe alternative to .bashrc is to source a Shell script inside your runtime job environment. Below is an example script to load an Anaconda module and source a Conda environment, which will be used inside the Slurm script.
Create a Shell script myenv.bash:
#!/bin/bash
module load anaconda3/2021.05
module load cuda/11.1
source activate pytorch_env_training

Then, source the Shell script inside your sbatch Slurm script (see Batch Jobs: sbatch):
#!/bin/bash
#SBATCH --partition=gpu
#SBATCH --nodes=1
#SBATCH --gres=gpu:1
#SBATCH --time=01:00:00
#SBATCH --job-name=gpu_run
#SBATCH --mem=4GB
#SBATCH --ntasks=1
#SBATCH --output=myjob.%j.out
#SBATCH --error=myjob.%j.err

source myenv.bash
python <myprogram>


https://rc-docs.northeastern.edu/en/latest/first_steps/usingbash.html

Cluster via Command-Line#

Overview#
Bash (Bourne Again SHell) is a popular shell and command-line interface. Specifically, a shell is an interface between the user and the underlying operating system, allowing users to interact with the system and perform tasks. Bash provides a range of features for running commands, managing files, navigating systems, and performing other tasks.

Bash commands perform various tasks within the shell environment. Commands span basic functionalities (e.g., ls, cd, cp, mv, and rm) to more advanced ones(e.g., grep and awk). We cover these commands and more in this tutorial. Bash can also be used in scripts, allowing users to automate tasks and perform more complex operations via loops, conditional logic, and defining functions, which we cover at the end of this page.
In summary, shell commands perform various tasks with the terminal.

What is the Terminal?
The terminal - the command line interface (CLI) - is a text-based interface for interacting with an operating system. It is a way for users to interact with the system and perform tasks by typing commands and receiving text-based output.
In contrast to graphical user interfaces (GUIs), the terminal provides a more direct and powerful way to interact with the system. Tasks requiring several steps in a GUI can often be accomplished much more quickly and efficiently in the terminal.

Whether you are a beginner or an advanced user, the terminal provides a powerful and versatile interface for interacting with your operating system. With some theory and practice, you can use the terminal to accomplish a wide range of tasks and take control of your system in new and powerful ways.
Various terminal options (i.e., flavors) are offered for different operating systems. Power Shell is available for Windows, Linux, and MacOS.
Let us explore options and specifics for each operating system; Mac, Linux, and Windows terminals.

How can you launch the Terminal on macOS?
macOS comes with a default terminal program, but more advanced terminals are available; iTerm2 is one of the more popular choices.
To launch the terminal:

Press Command(⌘) + Space on your Mac keyboard (alternatively, press F4)
Type “Terminal”
When you see Terminal in the Spotlight search list, click it to open the app.

iTerm2 can be installed via the terminal using Homebrew:
brew install --cask iterm2

If Homebrew is not already installed, run the following command in the terminal before installing iTerm2:
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

How can you launch the Terminal on Linux?
Linux also comes with a default terminal program, but more advanced terminals are available; Terminator is a popular choice.
To download Terminator, open a terminal (Ctrl+Alt+T is the shortcut to do so). Next, execute the following:
sudo add-apt-repository ppa:gnome-terminator
sudo apt-get update
sudo apt-get install terminator

How can you launch the Terminal on Windows?
Windows users must install a terminal; you can visit Windows Apps and download the Windows Terminal directly from Microsoft (Download Windows Terminal).
Additionally, Mobaxterm, an enhanced terminal for Windows with an X11 server, tabbed SSH client, and network tools dubbed the ultimate toolbox for remote computing, is a great tool for connecting to the login node, exploring the Discovery file system, and transferring files. Check out their demo, software features, and download.

How do I access  usage information for commands and utilities on the terminal?
Bash Manuals
The man command is used in the terminal to display manual pages for various tools and utilities. A manual or “man page” is detailed documentation for a specific command or utility that provides information about its usage, options, and examples.
To use the man command, simply type man followed by the command name or utility for which you want to view the manual page. For example, to view the manual page for the ls command, you would type the following:
man ls

Show manual page for ls
LS(1)                                                                           General Commands Manual                                                                          LS(1)



How can I  connect to a remote machine from my local computer's terminal?
ssh - Connect to a remote machine using Secure Shell (SSH):

ssh <user-name>@login.discovery.neu.edu



NAME
What command can I use to view the contents of a directory in a terminal?
  ls – list directory contents

SYNOPSIS
     ls [-@ABCFGHILOPRSTUWabcdefghiklmnopqrstuvwxy1%,] [--color=when] [-D format] [file ...]

DESCRIPTION
    For each operand that names a file of a type other than directory, ls displays its name as well as any requested, associated information.  For each operand that names a file of
     type directory, ls displays the names of files contained within that directory, as well as any requested, associated information.

     If no operands are given, the contents of the current directory are displayed.  If more than one operand is given, non-directory operands are displayed first; directory and non-
     directory operands are sorted separately and in lexicographical order.

 The following options are available for ls command in linux:

 -@      Display extended attribute keys and sizes in long (-l) output.

 -A      Include directory entries whose names begin with a dot (‘.’) except for . and ...  Automatically set for the super-user unless -I is specified.

 -B      Force printing of non-printable characters (as defined by ctype(3) and current locale settings) in file names as \xxx, where xxx is the numeric value of the character in
         octal.  This option is not defined in IEEE Std 1003.1-2008 (“POSIX.1”).

 -C      Force multi-column output; this is the default when output is to a terminal.

 -D format
         When printing in the long (-l) format, use format to format the date and time output.  The argument format is a string used by strftime(3).  Depending on the choice of
         format string, this may result in a different number of columns in the output.  This option overrides the -T option.  This option is not defined in IEEE Std 1003.1-2008
         (“POSIX.1”).

 -F      Display a slash (‘/’) immediately after each pathname that is a directory, an asterisk (‘*’) after each that is executable, an at sign (‘@’) after each symbolic link, an
         equals sign (‘=’) after each socket, a percent sign (‘%’) after each whiteout, and a vertical bar (‘|’) after each that is a FIFO.

 -G      Enable colorized output.  This option is equivalent to defining CLICOLOR or COLORTERM in the environment and setting --color=auto.  (See below.)  This functionality can
         be compiled out by removing the definition of COLORLS.  This option is not defined in IEEE Std 1003.1-2008 (“POSIX.1”).

 -H      Symbolic links on the command line are followed.  This option is assumed if none of the -F, -d, or -l options are specified.

 -I      Prevent -A from being automatically set for the super-user.  This option is not defined in IEEE Std 1003.1-2008 (“POSIX.1”).

 -L      Follow all symbolic links to final target and list the file or directory the link references rather than the link itself.  This option cancels the -P option.

 -O      Include the file flags in a long (-l) output.  This option is incompatible with IEEE Std 1003.1-2008 (“POSIX.1”).  See chflags(1) for a list of file flags and their
         meanings.

 -P      If argument is a symbolic link, list the link itself rather than the object the link references.  This option cancels the -H and -L options.

 -R      Recursively list subdirectories encountered.

 -S      Sort by size (largest file first) before sorting the operands in lexicographical order.

 -T      When printing in the long (-l) format, display complete time information for the file, including month, day, hour, minute, second, and year.  The -D option gives even
         more control over the output format.  This option is not defined in IEEE Std 1003.1-2008 (“POSIX.1”).

 -U      Use time when file was created for sorting or printing.  This option is not defined in IEEE Std 1003.1-2008 (“POSIX.1”).

 -W      Display whiteouts when scanning directories.  This option is not defined in IEEE Std 1003.1-2008 (“POSIX.1”).

 -a      Include directory entries whose names begin with a dot (‘.’).

 -b      As -B, but use C escape codes whenever possible.  This option is not defined in IEEE Std 1003.1-2008 (“POSIX.1”).

 -c      Use time when file status was last changed for sorting or printing.

 --color=when
         Output colored escape sequences based on when, which may be set to either always, auto, or never.

         always will make ls always output color.  If TERM is unset or set to an invalid terminal, then ls will fall back to explicit ANSI escape sequences without the help of
         termcap(5).  always is the default if --color is specified without an argument.

         auto will make ls output escape sequences based on termcap(5), but only if stdout is a tty and either the -G flag is specified or the COLORTERM environment variable is
         set and not empty.

         never will disable color regardless of environment variables.  never is the default when neither --color nor -G is specified.

         For compatibility with GNU coreutils, ls supports yes or force as equivalent to always, no or none as equivalent to never, and tty or if-tty as equivalent to auto.

 -d      Directories are listed as plain files (not searched recursively).

 -e      Print the Access Control List (ACL) associated with the file, if present, in long (-l) output.

 -f      Output is not sorted.  This option turns on -a.  It also negates the effect of the -r, -S and -t options.  As allowed by IEEE Std 1003.1-2008 (“POSIX.1”), this option
         has no effect on the -d, -l, -R and -s options.

 -g      This option has no effect.  It is only available for compatibility with 4.3BSD, where it was used to display the group name in the long (-l) format output.  This option
         is incompatible with IEEE Std 1003.1-2008 (“POSIX.1”).

 -h      When used with the -l option, use unit suffixes: Byte, Kilobyte, Megabyte, Gigabyte, Terabyte and Petabyte in order to reduce the number of digits to four or fewer using
         base 2 for sizes.  This option is not defined in IEEE Std 1003.1-2008 (“POSIX.1”).

 -i      For each file, print the file's file serial number (inode number).

 -k      This has the same effect as setting environment variable BLOCKSIZE to 1024, except that it also nullifies any -h options to its left.

 -l      (The lowercase letter “ell”.) List files in the long format, as described in the The Long Format subsection below.

 -m      Stream output format; list files across the page, separated by commas.

 -n      Display user and group IDs numerically rather than converting to a user or group name in a long (-l) output.  This option turns on the -l option.

 -o      List in long format, but omit the group id.

 -p      Write a slash (‘/’) after each filename if that file is a directory.

 -q      Force printing of non-graphic characters in file names as the character ‘?’; this is the default when output is to a terminal.

 -r      Reverse the order of the sort.

 -s      Display the number of blocks used in the file system by each file.  Block sizes and directory totals are handled as described in The Long Format subsection below, except
         (if the long format is not also requested) the directory totals are not output when the output is in a single column, even if multi-column output is requested.  (-l)
         format, display complete time information for the file, including month, day, hour, minute, second, and year.  The -D option gives even more control over the output
         (if the long format is not also requested) the directory totals are not output when the output is in a single column, even if multi-column output is requested.  (-l)
         format, display complete time information for the file, including month, day, hour, minute, second, and year.  The -D option gives even more control over the output
         format.  This option is not defined in IEEE Std 1003.1-2008 (“POSIX.1”).

 -t      Sort by descending time modified (most recently modified first).  If two files have the same modification timestamp, sort their names in ascending lexicographical order.
         The -r option reverses both of these sort orders.

         Note that these sort orders are contradictory: the time sequence is in descending order, the lexicographical sort is in ascending order.  This behavior is mandated by
         IEEE Std 1003.2 (“POSIX.2”).  This feature can cause problems listing files stored with sequential names on FAT file systems, such as from digital cameras, where it is
         possible to have more than one image with the same timestamp.  In such a case, the photos cannot be listed in the sequence in which they were taken.  To ensure the same
         sort order for time and for lexicographical sorting, set the environment variable LS_SAMESORT or use the -y option.  This causes ls to reverse the lexicographical sort
         order when sorting files with the same modification timestamp.

 -u      Use time of last access, instead of time of last modification of the file for sorting (-t) or long printing (-l).

 -v      Force unedited printing of non-graphic characters; this is the default when output is not to a terminal.

 -w      Force raw printing of non-printable characters.  This is the default when output is not to a terminal.  This option is not defined in IEEE Std 1003.1-2001 (“POSIX.1”).

 -x      The same as -C, except that the multi-column output is produced with entries sorted across, rather than down, the columns.

 -y      When the -t option is set, sort the alphabetical output in the same order as the time output.  This has the same effect as setting LS_SAMESORT.  See the description of
         the -t option for more details.  This option is not defined in IEEE Std 1003.1-2001 (“POSIX.1”).

 -%      Distinguish dataless files and directories with a '%' character in long

 -1      (The numeric digit “one”.) Force output to be one entry per line.  This is the default when output is not to a terminal.  (-l) output, and don't materialize dataless
         directories when listing them.

 -,      (Comma) When the -l option is set, print file sizes grouped and separated by thousands using the non-monetary separator returned by localeconv(3), typically a comma or

 -,      (Comma) When the -l option is set, print file sizes grouped and separated by thousands using the non-monetary separator returned by localeconv(3), typically a comma or
         period.  If no locale is set, or the locale does not have a non-monetary separator, this option has no effect.  This option is not defined in IEEE Std 1003.1-2001
         (“POSIX.1”).

 The -1, -C, -x, and -l options all override each other; the last one specified determines the format used.

 The -c, -u, and -U options all override each other; the last one specified determines the file time used.

 The -S and -t options override each other; the last one specified determines the sort order used.

 The -B, -b, -w, and -q options all override each other; the last one specified determines the format used for non-printable characters.

 The -H, -L and -P options all override each other (either partially or fully); they are applied in the order specified.

 By default, ls lists one entry per line to standard output; the exceptions are to terminals or when the -C or -x options are specified.xw

The manual page will be displayed in a pager program such as less, which allows you to scroll through the text and search for specific information.
The man command is a valuable tool for learning about new commands and utilities, and it is an essential resource for understanding how to use the terminal effectively. Whether you are a beginner or an advanced user, the man command provides the information you need to get the most out of your tools and utilities.

Basic Commands#

How do you use the  to connect to the Discovery cluster from a remote machine?
ssh - Connect to a remote machine using Secure Shell (SSH):
ssh <user-name>@login.discovery.neu.edu

how do i check current working directory.
pwd - linux command to Print the current working directory.
pwd
/path/to/directory1

Note
pwd is also stored as an environment variable, i.e., ${PWD}. Running echo ${PWD} prints the same output as pwd, but has the advantage of being accessed as part of a file pointer (e.g., ls ${PWD}/directory2 to print all contents of directory2 in the working directory.)

ls - linux command to List the contents of a directory.
ls
file1.txt file2.txt directory1

cd - linux command to Change the current working directory.
cd ~/directory1
pwd
<$HOME>/directory1

What command should I use to create a new directory in Linux?
mkdir - linux command to Create a new directory.
 mkdir directory2
 ls
 file1.txt file2.txt directory1 directory2

What is the linux command to delete a file or directory.
rm - linux command to Remove a file or directory.
 rm file1.txt
 ls
 file2.txt directory1 directory2

Note
To remove a directory, use rmdir if the folder is empty. Otherwise, recursively delete the directory and its contents via rm -r <FOLDER_PATH>.

What is the linux command to Copy a file or directory.
cp - linux command to Copy a file or directory.
 cp file2.txt file3.txt
 ls
 file2.txt file3.txt directory1 directory2

Note
Similar to removing, cp works for files; copying a folder and its contents must be done recursively via cp -r <FOLDER_PATH> <DESTINATION>.

mv - linux command to Move or rename a file or directory.
mv file2.txt file4.txt
ls
file3.txt file4.txt directory1 directory2

cat - linux command to Concatenate and display the contents of one or more files.
cat file3.txt
This is the contents of file3.txt

grep - linux command to Search for a pattern in a file or input.
grep "the" file3.txt
This is the contents of file3.txt

sort - linux command to Sort the lines of a file or input.
sort file3.txt
This is the contents of file3.txt

uniq - linux command to Remove duplicates from a sorted file or input.
sort file3.txt | uniq
This is the contents of file3.txt

wc - linux command to Count the number of lines, words, and characters in a file or input.
 wc file3.txt
 1   4  26 file3.txt

head - linux command to Display the first lines of a file or input.
 head file3.txt
 This is the contents of file3.txt

tail - linux command to Display the last lines of a file or input.
tail file3.txt
This is the contents of file3.txt

less - linux command to View the contents of a file one page at a time.
less file3.txt

top - linux command to Show the currently running processes and system information.
top

To exit, press q.
ps - linux command to Show information about the currently running processes.
ps

Note
<pid> (PID) in the command should be replaced with the actual process ID of the process you want to terminate; the output of the kill command will typically be empty unless there is an error in executing the command.

It is essential to be cautious when using the kill command, as terminating a process can cause data loss or corruption. Therefore, before using kill, you should always try to gracefully stop the process by sending a termination signal, such as SIGTERM, first. If that does not work, you can try a stronger signal, such as SIGKILL.
kill - Terminate a process by its process ID:
kill <pid>

Advanced Commands#
In this section, we will provide examples of some helpful advanced commands, and then take a closer look at three essential advanced commands.
sed - Stream editor for filtering and transforming text.
cat file1.txt
This is line 1
This is line 2
This is line 3

sed 's/line 1/Line 1/' file1.txt
This is Line 1
This is line 2
This is line 3

Reference: https://manpages.ubuntu.com/manpages/kinetic/en/man1/sed.1.html

How do you use the 'gzip' and 'gunzip' commands to compress and decompress files?
gzip - Compress or decompress files.
gzip file1.txt
ls
file1.txt.gz

gunzip file1.txt.gz
ls
file1.txt
https://manpages.ubuntu.com/manpages/kinetic/en/man1/gzip.1.html

tar - Create or extract compressed archive files.
tar cvf archive.tar file1.txt file2.txt
ls
archive.tar file1.txt file2.txt

tar xvf archive.tar
ls
file1.txt file2.txt

Reference: https://manpages.ubuntu.com/manpages/kinetic/en/man1/tar.1.html

Regular expressions:
grep -E '^[A-Z][a-z]+$' file1.txt
John
Jane

Parameter expansion:
name="John Doe"
echo ${name// /_}
John_Doe

Command line options:
ls -lh
total 8.0K
drwxrwxr-x 2 user user 4.0K Feb 14 13:29 directory1
-rw-rw-r-- 1 user user   12 Feb 14 13:29 file1.txt
-rw-rw-r-- 1 user user   14 Feb 14 13:29 file2.txt

Parameter substitution:
echo ${name:4:3}
Doe

Arithmetic operations:
echo $((2 + 2))
4

File tests:
file=file1.txt
if [ -f $file ]; then
>   echo "$file is a regular file"
> fi
file1.txt is a regular file

String tests:
 string="hello"
 if [ "$string" == "hello" ]; then
 >   echo "The strings match"
 > fi
 The strings match

Command substitution with process substitution:
diff <(ls /path/to/dir1) <(ls /path/to/dir2)

The next few subsections provide more details on a few advanced bash tools that often come in handy.

What is the purpose of the rsync command?
rsync
The rsync command is a powerful and versatile file transfer utility commonly used to synchronize files and directories between different locations. It can transfer files over a network connection and run in various modes, including local and remote transfers and backup operations. One of the key benefits of using rsync is its ability to transfer only the differences between the source and destination files, which can significantly reduce the amount of data transfer time required. Additionally, rsync supports various advanced features, including the ability to perform incremental backups and preserve symbolic links, making it a popular tool for system administrators and other advanced users.

Important
File transfers must be done using the transfer node on the Discovery, i.e., do not copy to or from the login node accessible via xfer.discovery.neu.edu. See Transfer Data for more information.

We have listed a few examples of rsync synchronizing files and directories between two locations, but many more options are available. Consult the rsync(1) manual page for more information on effectively using rsync.
Syncing a local directory to a remote server:
rsync -avz /local/path user@xfer.discovery.neu.edu:/remote/path

Syncing a remote server to a local directory:
rsync -avz user@xfer.discovery.neu.edu:/remote/path /local/path

Syncing a local directory to a remote server with compression:
rsync -avz --compress /local/path user@xfer.discovery.neu.edu:/remote/path

Syncing a remote server to a local directory while preserving permissions:
rsync -avz --perms user@xfer.discovery.neu.edu:/remote/path /local/path

Syncing only files that have been modified in the last hour:
rsync -avz --update --min-age=3600 /local/path user@xfer.discovery.neu.edu:/remote/path

Syncing a local directory to a remote server while excluding certain files:
rsync -avz --exclude='*.log' /local/path user@xfer.discovery.neu.edu:/remote/path

Syncing a remote server to a local directory while preserving symbolic links:
rsync -avz --links user@xfer.discovery.neu.edu:/remote/path /local/path

How do i search for file or folders in linux?
find is a command line tool used to search for files and directories within a specified location. It starts at a specified directory and recursively searches through its subdirectories. The user can select a range of criteria to match (e.g., file name, size, modification time), and find will return a list of all files and directories that match the specified criteria. find provides a range of options for further processing the results, such as executing a command on each matching file, printing the results, or performing other operations. As a result, it is a versatile tool to search for specific files and to clean up old files.
Here are several advanced examples of using the find command to search for files and directories; see find(1) manual page for more information on how to use the command effectively.
To search for files and directories:
find /path/to/search -name "*.txt"
/path/to/search/file1.txt
/path/to/search/file2.txt

Finding files based on size
find /path/to/dir -size +10M

This will find all files in /path/to/dir that are larger than 10 MB.
Finding files based on modification time:
find /path/to/dir -mtime +7

This will find all files in /path/to/dir that have been modified more than 7 days ago.
Finding files based on type:
find /path/to/dir -type f

This will find all files in /path/to/dir that are regular files (not directories).
Finding files based on the name
find /path/to/dir -name "*.txt"

This will find all files in /path/to/dir with a .txt file extension.
Executing commands on matching files:
find /path/to/dir -name "*.txt" -exec chmod 644 {} \;

This will find all files in /path/to/dir with a .txt file extension and execute the chmod command on each file, changing its permissions to 644.

awk
awk is a text-processing tool widely used for data extraction, report generation, and other text-related tasks. It operates by reading a file line-by-line and processing each line based on a set of rules defined by the user. The regulations specify the conditions under which certain actions are performed, such as printing specific fields, performing calculations, or modifying the text in some way. awk is particularly useful for processing tabular data, such as those found in CSV files, and extracting and manipulating data in various ways. Additionally, awk provides a rich set of string and numerical manipulation functions, making it a powerful tool for working with large data sets.
Below are a few examples of awk processing and manipulating text data, but there are many more options and features available. Consult the awk(1) manual page for more information on effectively using the tool.
We will use the sample file _resources/awk-example.sh to work through this section.
Download, or create and name a file as shown in the following block. Also, be sure to store it in the working directory.
cat awk-example.txt
John Doe 25
Jane Doe 30
Jim Smith 40
Sara Johnson 35
Michael Brown 29

This file contains a list of names and ages, with each line representing a different person and their age. The first column is the name, and the second column is the age. The columns are separated by a space.
This sample file can be used in the examples provided in the previous response to demonstrate the usage of awk command.
Print the entire contents of a file:
awk '{print}' awk-example.txt
John Doe 25
Jane Doe 30
Jim Smith 40
Sara Johnson 35
Michael Brown 29

Print specific columns from a tab-delimited file:

Assuming the file is not tab-delimited.#
awk -F "\t" '{print $2}' awk-example.txt
25
30
40
35
29

Sum a column of numbers:
awk '{sum+=$2} END {print sum}' awk-example.txt
169

Print only lines that match a pattern:
awk '/Doe/ {print}' awk-example.txt
John Doe 25
Jane Doe 30

Format the output:
awk '{printf "Name: %s, Age: %d\n", $1, $2}' awk-example.txt
Name: John Doe, Age: 25
Name: Jane Doe, Age: 30
Name: Jim Smith, Age: 40
Name: Sara Johnson, Age: 35
Name: Michael Brown, Age: 29

awk '/Sara/ {print "Sara found"}' awk-example.txt
Sara found

Printing the first field of each line in a file:
awk '{print $1}' awk-example.txt
John
Jane
Jim
Sara
Michael

Printing the second field of each line in a file, only if the first field is equal to a specific value:
awk '$2 == "Doe" {print $1}' awk-example.txt
John
Jane

Printing the sum of all numbers in the third field (Age) of a file:
awk '{sum+=$3} END {print sum}' awk-example.txt
159

Printing the average of all numbers in the fourth field of a file:
awk '{sum+=$3; count++} END {print sum/count}' awk-example.txt
31.8

Printing the line number and the line text for each line in a file that contains a specific word:
awk '/Doe/ {print NR, $0}' awk-example.txt
1 John Doe 25
2 Jane Doe 30

Printing the line number and the line text for each line in a file that starts with a specific string:
awk '$1 ~ /^J/ {print NR, $0}' awk-example.txt
1 John Doe 25
2 Jane Doe 30
3 Jim Smith 40

Printing the line number, the line text, and the length of each line in a file:
awk '{print NR, $0, length($0)}' awk-example.txt
1 John Doe 25 11
2 Jane Doe 30 11
3 Jim Smith 40 12
4 Sara Johnson 35 15
5 Michael Brown 29 16

Git configurations tips and tricks#
Git is a distributed version control system for software development and other collaborative projects that allows multiple users to work on a project simultaneously while keeping track of changes and enabling easy collaboration. With Git, users can commit their changes to a local repository and push them to a remote repository so that others can access and merge their changes into the main project. Git also provides a robust set of tools for managing branches, resolving conflicts, and performing other tasks related to version control.
Git provides a range of configuration options that allow users to customize their behavior to suit their needs, including setting the username and email, specifying a preferred text editor, and setting up aliases for frequently used commands. In addition, users can either configure Git globally, which will apply the configuration to all of their Git repositories, or configure locally, which will apply the configuration only to a specific repository. This flexibility allows users to work with Git in a way that suits their workflow.

Custom Configurations#
Below you will find a few examples of Git configuration options. See Git User Manual for more information on how to customize Git to your needs.
Setting your username and email
git config --global user.name "Your Name"
git config --global user.email "[email protected]"

Setting your preferred text editor
git config --global core.editor nano

Setting your preferred diff tool
git config --global diff.tool emacs
git config --global difftool.prompt false

Setting up aliases for frequently used Git commands
git config --global alias.st status
git config --global alias.co checkout
git config --global alias.ci commit

Setting up a default push behavior:
git config --global push.default simple

Enabling colored output for Git commands:
git config --global color.ui true

Ignoring files globally across all your Git repositories as follows:
git config --global core.excludesfile ~/.gitignore_global

Enabling automatic line wrapping in Git log output as follows:
git config --global log.autoWrap true

Text Editors#
There are a few popular text editors to create, view, and modify text files from a terminal: Emacs, Vim, and Nano. Here, we briefly describe these text editors, all of which are available by default on Discovery.

Emacs#
Emacs is a popular text editor that is widely used to program, write, and read text files. You should consult the Emacs Manual or online resources for more information on how to use the text editor effectively.
To start Emacs, open a terminal and type the following command:
emacs

Once open, the following table summarizes common keyboard shortcuts (i.e., commands) for working in the text editor.

Common Emacs Commands (C- -> hold Ctrl)#

Functionality
Command

Open file
C-x C-f

Save the file
C-x C-s

Close file
C-x C-w

Cut text
C-k

Paste text
C-y

Undo
C-/

Redo
C-x C-/

Search text
C-s

Quit Emacs
C-s

Moving cursor
C-x C-c

previous line
C-p

next line
C-n

forward character
C-f

backward character
C-b

For more commands, see Emacs Cheat Sheet.

Vim#
Vim is a popular text editor that is widely used for programming, writing, and other text-related tasks. Consult the VIM Manual for more information on using the text editor effectively.
Vim starts in normal mode: a mode that allows for the navigation through the text and performs various operations (e.g., search), but in read-online mode (i.e., cannot edit text).
Open a terminal and type the following command:
vim

To open an existing file, type the following command:
vim filename

Common Vim Commands#

Functionality
Command

Enter insert mode
i

Enter normal mode
esc

Save the file
:w

Close file
:q

Cut text (from the front)
v

Cut text (from the end)
d

Paste text
p

Undo
u

Redo
Ctrl+r

Search text
/text

Quit VIM
:q

Moving cursor
C-x C-c

Left
h

Down
j

Up
k

Right
l

GNU Nano#
Nano is a simple, easy-to-use text editor commonly used in Unix-like operating systems. Consult the GNU Nano Manual or online resources for more information on how to use the text editor effectively.
Nano can launch one of two ways from a terminal: (1) to open to an empty, unnamed file, run:
nano

To open a file by name, whether it already exists or needs to be created, run:
nano filename.txt

If the file does not exist, it will open an empty file that will persist upon saving.

Common Nano Commands#

Functionality
Command

Save the file
Ctrl + O

Close file
Ctrl + X

Cut text (from the front)
Alt + A

Cut text (from the end)
Ctrl + K

Paste text
Ctrl + U

Undo
Ctrl + T

Redo
Ctrl + Y

Search text
Ctrl + W

Quit Nano
Ctrl + X

Shell Scripting#
Shell scripts are a feature of Bash that allows you to automate tasks and perform complex operations. A shell script is a text file containing a series of bash commands that the shell can execute to perform a specific task.
Here is a simple example of a shell script that prints the message Hello, World! to the screen:
#!/bin/bash

echo "Hello, World!"

Notice the line #!/bin/bash at the top of a shell script (i.e., the shebang line). This line specifies which shell interpreter will be used when running the script. In this case, line #!/bin/bash specifies that the script uses the bash shell.

Note
The shebang line is the first line of the script and must start with the characters #!. The path that follows the shebang (/bin/bash in this case) specifies the location of the shell interpreter. In most cases, /bin/bash is the correct path for the bash shell.

First we must make the file executable to run this script. This is done as follows:
chmod +x hello_world.sh

Then, run the script as follows:
./hello_world.sh

This will print the message Hello, World! on the screen.
Shell scripts can do many tasks, including backups, system maintenance, and the commands covered in this tutorial. For example, you could create a script to automate the backup of your home directory by copying all of its files to a remote server. The script could include commands for compressing the files, copying them to the server, and logging the results.


https://rc-docs.northeastern.edu/en/latest/hardware/hardware_overview.html

What are the hardware specifications and network capabilities of the computing cluster 
Hardware Overview
The computing cluster provides access to over 1,024 CPU nodes, 50,000 CPU cores, and over 200 GPUs and is connected to the university network over 10 Gbps Ethernet (GbE) for high-speed data transfer. Compute nodes are wired with 10 GbE or a high-performance HDR200 InfiniBand (IB) interconnect running at 200 Gbps (with some nodes running HDR100 IB if the HDR200 IB is not supported).

CPU nodes
Below shows the feature names, number of nodes by partition type (public and private), and the RAM memory range per node. The feature name follows archspec microarchitecture specification.

1. The "Skylake" feature, adhering to the archspec microarchitecture specification, comprises 170 private nodes and no public nodes. The RAM memory available per node in this category varies significantly, ranging from 186 GB to a substantial 3094 GB.

2. Under the "Zen2" specification, there are 40 public nodes and 292 private nodes. Each of these nodes is equipped with a RAM memory capacity ranging between 256 GB and 2000 GB.

3. The "Zen" feature follows closely, also with 40 public nodes and a slightly higher count of 300 private nodes. The RAM memory per node for Zen is identical to that of Zen2, spanning from 256 GB to 2000 GB.

4. "Ivybridge" features a total of 194 nodes, with 64 in the public domain and 130 in the private sector. The RAM memory per node in this category varies more moderately, ranging from 31 GB to 1031 GB.

5. The "Sandybridge" specification is unique in this list, having 8 public nodes and no private nodes. Each of these public nodes boasts a fixed RAM capacity of 384 GB.

6. For the "Haswell" architecture, there is a substantial presence of 230 public nodes and 62 private nodes. The RAM memory available in these nodes varies between 109 GB and 1031 GB.

7. The "Broadwell" feature stands out with the highest total number of nodes, having 756 public nodes and 226 private nodes. The RAM memory per node in this category varies from 128 GB to 515 GB.

8. Lastly, the "Cascadelake" feature includes 260 public nodes and 88 private nodes. Similar to Skylake, it offers a wide range of RAM memory per node, from 186 GB to 3094 GB.



If you are looking for information about GPUs, see Working with GPUs.
If you are interested in more information about the different partitions on Discovery, including the number of nodes per partition, running time limits, job submission limits, and RAM limits, see Partitions.

hwo do i set contraints whn using sbatch.
Using the --constraint flag
When using srun or sbatch, you can specify hardware features as part of your job by using the --constraint= flag. This may be particularly useful when benchmarking, optimizing, or if you are using code that was compiled on a certain micro-architecture. Currently, you can use the --constraint= flag to restrict your job to a specific feature name (e.g., haswell, ivybridge) or you can use the flag: ib to only include nodes that are connected by InfiniBand (IB) with a job that needs to use multiple nodes.
A few examples using srun:
1     srun --constraint=haswell --pty /bin/bash
2     srun --constraint=ivybridge --pty /bin/bash
3     srun --constraint=ib --pty /bin/bash
4     srun --constraint="[ivybridge|zen2]" --pty /bin/bash #this uses the OR operator | to select either an ivybridge or zen2 node.

You can add these same flags as an additional line in your sbatch script via (#SBATCH --constraint=haswell)

Note
Using the –constraint flag can mean that you will wait longer for your job to start, as the scheduler (Slurm) will need to find and allocate the appropriate hardware that you have specified for your job. For more information about running jobs, see Introduction to Slurm. Finally, at this time only the OR operator | is supported when using --constraint.

https://rc-docs.northeastern.edu/en/latest/hardware/partitions.html

What is a partition?
Partitions
A partition is a logical collections of nodes that comprise different hardware resources and limits to help meet the wide variety of jobs that get scheduled on the cluster. Occasionally, the Research Computing team might need to make updates to the partitions based on monitoring job submissions to help reduce job wait times. As our cluster grows, changes to the partitions also help to ensure the fair, efficient distribution of resources for all jobs being submitted to the cluster.

What types of partitions are available on the Discovery platform?
the partitions On Discovery cluster are as follows:
General access (debug, express, short, gpu)
Application only (long, large, multigpu)
PI owned (accessed only by members of the PIs’ group)

What are the differences between the general access partitions and application-only partitions?
The general access partitions and application only partitions span the hardware on the cluster, with gpu and multigpu partitions spanning the GPUs on the cluster and the other partitions spanning the CPUs. For example, if you use the debug partition you are using the same hardware as short, just with different time, job, and core limits. Refer to the tables below for detailed information on the current partitions. Note that PI-owned partitions only include the hardware that those PIs own and are only accessible to the members of the PI’s group.

Note
In the following table, the Running Jobs Per User/Per Research Group. Core and RAM limits are set per user, across all running jobs (not pending). Keep in mind that the number of running jobs is limited by the available resources on the cluster at the time of the job submission and may not adhere to the number stated below.
It is possible for you to see the message below in the output of squeue -u $USER, even though you submitted fewer jobs than the number of submitted jobs indicated in the table. This implies that Slurm has reached the hard-coded limit of 10,000 total jobs/per account at that time. Here the account refers to the Slurm account that you, as a cluster user, is associated with. So you will continue to receive this error message until some of your jobs start running and/or get completed before you can submit more jobs.
job violates accounting/QOS policy




Now, let's delve into the partitions:

what are the CPU partitions? 
There are 5 partitions for CPU - debug , express, short,long, large. 

what are the GPU partitions? 
There are 2 partitions for GPU - single GPU and multi GPU 

What are the specifications for the debug partition? what is the time limit for debug partition? what is the number of jobs allowed on debug partition? what is the RAM limit debug partition? what is debug partition used for?
The debug partition does not require approval for job submissions. It has a fixed time limit set to 20 minutes for both default and maximum durations. Users can run between 10 to 25 jobs concurrently while being allowed to submit up to 5,000 jobs. Each user has a core limit of 128 and a RAM limit of 256GB. This partition is ideally used for serial and parallel jobs that finish within 20 minutes, making it perfect for testing code.

What are the specifications for the express partition? what is the time limit for express partition? what is the number of jobs allowed on express partition? what is the RAM limit express partition? what is express partition used for?
The express partition, like the debug, does not need approval. It has a default time limit of 30 minutes, but users can run jobs for up to 60 minutes. Here, between 50 to 250 jobs can run concurrently, with a cap of 5,000 job submissions. Users have a generous core limit of 2,048 and a RAM cap of 25TB. It's tailored for jobs, either serial or parallel, that complete in under an hour.

What are the specifications for the short partition? what is the time limit for short partition? what is the number of jobs allowed on short partition? what is the RAM limit short partition? what is short partition used for?
The short partition, also not requiring approval, has a default time limit of 4 hours, but jobs can be extended to run for a full day. Concurrent running jobs range from 50 to 500, and the submission limit is again 5,000 jobs. Users have a core limit of 1,024 and the same RAM limit of 25TB. This partition is great for serial jobs or small parallel jobs (with a maximum of 2 nodes) that need up to a day to run.

What are the specifications for the long partition? what is the time limit for long partition? what is the number of jobs allowed on long partition? what is the RAM limit long partition? what is long partition used for?
Then we have the long partition. This one does need approval. Jobs can be run for a day by default, but can stretch to 5 days at most. You can run 25 to 250 jobs at once. Here, the submission differs: a user can submit up to 1,000 jobs, but a group has a ceiling of 5,000 jobs. Core and RAM limits remain at 1,024 and 25TB respectively. It's mostly for jobs that take more than a day, either serial or parallel. However, to utilize this, you need to demonstrate that your code doesn't have the capability to checkpoint.

What are the specifications for the large partition? what is the time limit for large partition? what is the number of jobs allowed on large partition? what is the RAM limit large partition? what is large partition used for?
Lastly, the large partition, which requires approval, is tailored for jobs with a fixed duration of 6 hours, both as a default and maximum. A user can run 100 jobs at any given time and can submit 1,000 jobs individually, while the group limit is 5,000. There are no specific RAM or core limits for this partition. It's mainly for parallel jobs that can effectively use more than 2 nodes. To leverage this partition, one needs to showcase that their code is optimized for operation on multiple nodes.

What are the specifications for the gpu partition? what is the time limit for gpu partition? what is the number of jobs allowed on gpu partition? what is the RAM limit gpu partition? what is gpu partition used for?
The "gpu" partition does not require approval for use. It has a default time limit of 4 hours, which can be extended to a maximum of 8 hours. As for job limits, there are currently 25 running jobs out of a possible 250, and 50 submitted jobs out of a potential 100. The resources allocated to this partition include a GPU limit of 1 and a CPU limit of 8. The primary use case for the "gpu" partition is for jobs that can run on a single GPU processor.

What are the specifications for the multigpu partition? what is the time limit for multigpu partition? what is the number of jobs allowed on multigpu partition? what is the RAM limit multigpu partition? what is multigpu partition used for? Do i need approval for multigpu partition? 
On the other hand, the "multigpu" partition does necessitate approval before use. Its default time limit is 4 hours, but jobs can run for up to 24 hours if needed. There are presently 25 running jobs with a limit of 100, and 50 submitted jobs out of a maximum of 100. The "multigpu" partition is more resource-intensive, with a GPU limit of 12 and a CPU limit of 12. It is best suited for jobs that require more than one GPU and take up to 24 hours to complete.


How can I view information about different partitions on a high-performance computing cluster?
how to View partition information
Slurm commands allow you to view information about the partitions. Three commands that can show you partition information are sinfo, sacct, and scontrol. The following are common options to use with these commands:
sinfo -p <partition name> #displays the state of the nodes on a specific partition
sinfo -p <partition name> --Format=time,nodes,cpus,socketcorethread,memory,nodeai,features #displays more detailed information using the Format option, including features like the type of processors
sacct --partition <partition name> #displays the jobs that have been run on this partition
scontrol show partition <partition name> #displays the Slurm configuration of the partition

For more information about these commands, see our Using Slurm and the official Slurm documentation.

How do I specify a partition when submitting jobs? what considerations should I keep in mind regarding resource allocation?
Allocating partitions in your jobs
To specify a partition when running jobs, use the option
--partition=<partition name> with either srun or sbatch. When using a partition with your job and specifying the options of
--nodes= and --ntasks=, make sure that you are requesting options that best fit your job. It can actually have the opposite effect on
jobs that are better suited to running with smaller requirements, as you have to wait for the extra resources that your job will not
use. See Introduction to Slurm for more information on using Slurm to run jobs.

Note
Requesting the maximum number of nodes or tasks will not make your job run faster or give you higher priority in the job queue.

Tip
You should always try to have job requests that will attempt to allocate the best resources for the job you want to run. For example, if you are running a job that is not parallelized, you only need to request one node (--nodes=1). For some parallel jobs, such as a small MPI job, you can also use one node (--nodes=1) with the –-ntasks= option set to correspond to the number of MPI ranks (tasks) in your code. For example, for a job that has 12 MPI ranks, request 1 node and 12 tasks within that node (--nodes=1 –-ntasks=12). If you request 12 nodes, Slurm is going to run code between those nodes, which could slow your job down significantly if it is not optimized to run between nodes.
If your code is optimized to run on more than two nodes and needs less than one hour to run, you can use the express partition. If your code needs to run on more than 2 nodes for more than one hour, you should apply to use the large partition. See the section Partition Access Request below for more information.

How can I request access to the large partitions, long partitions, or multigpu partitions for research computing?
Partition Access Request
If you need access to the large, long, or multigpu partition, you need to submit a ServiceNow ticket link - https://service.northeastern.edu/tech?id=sc_cat_item&sys_id=0c34d402db0b0010a37cd206ca9619b7 . Access is not automatically granted. You will need to provide details and test results that demonstrate your need for access for these partitions. If you need temporary access to multigpu to perform testing before applying for permanent access, you should also submit a ServiceNow ticket. All requests are evaluated by members of the RC team, and multigpu requests are also evaluated by two faculty members.



https://rc-docs.northeastern.edu/en/latest/using-ood/introduction.html

How can I access Open OnDemand (OOD)?  what are the key resources it provides for interacting with the Discovery cluster?
Introduction to OOD
Open OnDemand (OOD) is a web portal to the Discovery cluster. A Discovery account is necessary for you to access OOD. If you need an account, see Request an account. If you already have an account, in a web browser go to http://ood.discovery.neu.edu and sign in with your Northeastern username and password.
OOD provides you with several resources for interacting with the Discovery cluster:

Launch a terminal within your web browser without needing a separate terminal program. This is an advantage if you use Windows, as otherwise, you need to download and use a separately installed program, such as MobaXterm.
Use software applications like SAS Studio that run in your browser without further configuration. See Interactive Open OnDemand Applications  link - https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/index.html#interactive-ood-apps for more information.
View, download, copy, and delete files using the OOD File Explorer feature.

Note
OOD is a web-based application. You access it by using a web browser. Like many web-based applications, it has compatibility issues with specific web browsers. Use OOD with newer Chrome, Firefox, or Internet Explorer versions for optimal results. OOD does not currently support Safari or mobile devices (phones and tablets).

https://rc-docs.northeastern.edu/en/latest/using-ood/accessingood.html

How do I access the HPC cluster through the web browser application Open OnDemand? what are the steps to log to Open OnDemand?
Accessing Open OnDemand
Open OnDemand (OOD) is a web portal to the HPC cluster.
This topic is for connecting to the HPC cluster through the browser application Open OnDemand. If you want to access the HPC directly on your system rather than through a browser, please see Connecting To Cluster, whether Mac or Windows.
A cluster account is necessary for you to access OOD. If you need an account, see Getting Access. After you have created a cluster account, access the cluster through Open OnDemand (OOD) via the following steps:

In a web browser, go to http://ood.discovery.neu.edu.
At the prompt, enter your Northeastern username and password. Note that your username is the first part of your email without the @northeastern, such as j.smith.
Press Enter or click Sign in.


https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/index.html

Interactive Open OnDemand Applications
What is Open OnDemand (OOD) web portal?
The Open OnDemand (OOD) web portal provides a range of applications. Upon clicking launch, the Slurm scheduler assigns a compute node with a specified number of cores and memory. By default, applications run for one hour. If you require more than an hour, you may have to wait for Slurm to allocate resources for the duration of your request.

Applications on Open OnDemand (OOD)
What are some of the applications available through the Open OnDemand interface? The Open OnDemand interface offers several applications, which as of June 2023, include:

JupyterLab
RStudio (Rocker)
Matlab
Schrodinger (Maestro)
Desktop
Gaussian (GaussView)
KNIME
TensorBoard
SAS

These applications can be accessed from the Open OnDemand (OOD) web interface’s Interactive Apps drop-down menu.

Note
Specific applications in the Interactive Apps section, particularly those with graphical user interfaces (GUIs), may require X11 forwarding and the setup of passwordless SSH. For tips and troubleshooting information on X11 forwarding setup and usage, please look at the [Using X11] section of our documentation.

Additionally, we offer a selection of modified standard applications intended to support specific coursework. These applications are under the Courses menu on the Open OnDemand (OOD) web interface. Please note that these course-specific applications are only accessible to students enrolled in the respective courses.

Note
Certain apps are reserved for specific research groups and are not publicly accessible, as indicated by the “Restricted” label next to the application name. If you receive an access error when attempting to open a restricted app, and you believe you should have access to it, please email rchelp@northeastern.edu with the following information: your username, research group, the app you are trying to access, and a screenshot of the error message. We will investigate and address the issue.

How do I access Open OnDemand and use applications on the Open OnDemand web portal?
Go to [Open On Demand] in a web browser. If prompted, enter your MyNortheastern username and password.
Select Interactive Apps, then select the application you want to use.
Keep the default options for most apps, then click Launch. You might have to wait a minute or two for a compute node to be available for your requested time and resource.

https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/desktopood.html

Desktop App
Open OnDemand provides a containerized desktop to run on the HPC cluster.
The following tools and programs are accessible on our Desktop App:

Slurm (for running Slurm commands via the terminal in the desktop and interacting on compute nodes)
Module command (for loading and running HPC-ready modules)
File explorer (able to traverse and view files that you have access to on the HPC)
Firefox web browser
VLC media player
LibreOffice suite of applications (word, spreadsheet, and presentation processing)

Note
The desktop application is a Singularity container; a Singularity container cannot run inside the desktop application. It fails if users run a container-based module or program via the desktop application.

https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/fileexplore.html

How can I work with files and directories using the Open OnDemand (OOD) File Explorer? what should I keep in mind regarding the file size limit for my home directory on the Discovery cluster?
Open OnDemand (OOD) File Explorer
When working with the resources in Open OnDemand (OOD), your files are stored in your home directory on the storage space on the Discovery cluster. Like any file navigation system, you can work with your files and directories through the Open OnDemand (OOD) Files feature, as detailed below. For example, you can download a Jupyter Notebook file in Open OnDemand (OOD) that you have been working on to your local hard drive, rename a file, or delete a file you no longer need.

Note
Your home directory has a file size limit of 75GB. Please check your home directory regularly, and remove any files you do not need to make sure you have enough space.

How do I access and manage files in my home directory? 
In a web browser, go to ood.discovery.neu.edu. If prompted, enter your MyNortheastern username and password.
Select Files > Home Directory. The contents of your home directory display in a new tab.
To download a file to your hard drive, navigate to the file you want to download,
select the file, and click Download. If prompted by your browser,
click OK to save your file to your hard drive.
To navigate to another folder on the Discovery file system, click Go To,
enter the path to the folder you want to access and click OK.

Note
From the Files > Home Directory view, the Edit button will not launch your .ipynb file in a Jupyter Notebook. It will open the file in a text editor. You must be in Jupyter Notebook to launch a .ipynb file from your /home directory. See Interactive Open OnDemand Applications to access a Jupyter Notebook through Open OnDemand (OOD).

https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/jupyterlab.html

How can I set up and use JupyterLab Notebook on Open OnDemand (OOD)? and what are the steps for creating and activating a conda virtual environment to work with Python packages in JupyterLab Notebook?
JupyterLab Notebook is one of the interactive apps on Open OnDemand (OOD). This section will provide a walk through of setting up and using this app. The general workflow is to create a virtual Python environment, ensure that JupyterLab Notebook uses your virtual environment, and reference this environment when you start the JupyterLab Notebook OOD interactive app.
To find the JupyterLab Notebook on Open OnDemand (OOD), follow these steps:

Go to [Open On Demand].
Click on Interactive Apps.
Select JupyterLab Notebook from the drop-down list.

The Open OnDemand (OOD) form for launching JupyterLab Notebook will appear.

Conda virtual environment
You can import Python packages in your JupyterLab Notebook session by creating a conda virtual environment and activating that environment when starting a JupyterLab Notebook instance.

First, set up a virtual Python environment. See Creating Environments for how to set up a virtual Python environment on the HPC using the terminal.
Type source activate <yourenvironmentname> where <yourenvironmentname> is the name of your custom environment.
Type conda install jupyterlab -y to install JupyterLab in your environment.

How do i launch JupyterLab Notebook on Open OnDemand (OOD)?
Using Open OnDemand (OOD) to launch JupyterLab Notebook

Go to [Open On Demand].
Click Interactive Apps, then select JupyterLab Notebook.
Enter your Working Directory (e.g., /home/<username> or /work/<project>) that you want JupyterLab Notebook to launch in.
Select from the Partition drop-down menu the partition you want to use for your session. Refer to Partitions for the resource restrictions for the different partitions. If you need a GPU, select the gpu partition.
Select the compute node features for the job:

In the Time field, enter the number of hour(s) needed for the job.
Enter the memory you need for the job in the Memory (in Gb) field.
If you selected the gpu partition from the drop-down menu, select the GPU you would like to use and the version of CUDA that you would like to use for your session under the respective drop-down menus.

Select the Anaconda version you used to create your virtual Python environment in the System-wide Conda Module field.
Check the Custom Anaconda Environment box, and enter the name of your custom virtual Python environment in the Name of Custom Conda Environment field.
Click Launch to join the queue for a compute node. This might take a few minutes, depending on what you asked for.
When allocated a compute node, click Connect to Jupyter.

When your JupyterLab Notebook is running and open, type conda list in a cell and run the cell to confirm that the environment is your custom conda environment (you should see this on the first line). This command will also list all of your available packages.



https://rc-docs.northeastern.edu/en/latest/runningjobs/understandingqueuing.html

Explain how the queuing system works in a high-performance computing (HPC) environment. 
Understanding the Queuing System
The queuing system in a high-performance computing (HPC) environment manages and schedules computing tasks. Our HPC cluster uses the Slurm Workload Manager as our queuing system. This section aims to help you understand how the queuing system works and how to interact effectively.

How does the Slurm scheduler manage jobs in the queue?
Introduction to Queuing Systems
The Slurm scheduler manages jobs in the queue. When you submit a job, it gets placed in the queue. The scheduler then assigns resources to the job when they become available, according to the job’s priority and the available resources.

How are jobs  submitted to the queue
Job Submission and Scheduling
Jobs are submitted to the queue via a script specifying the resources required (e.g., number of CPUs, memory, and GPUs) and the commands to be executed. Once submitted, the queuing system schedules the job based on the resources requested, the current system load, and scheduling policies.


What are the Scheduling Policies? How does the fair-share scheduling policy work in cluster, and what are some of the factors that ensure fair use of cluster resources, such as single job size, run time limit, and priority decay?
Scheduling Policies
Our cluster uses a fair-share scheduling policy. This means that usage is tracked for each user or group, and the system attempts to balance resource allocation over time. If a user or group has been using many resources, their job priority may be temporarily reduced to allow others to use the system. Conversely, users or groups that have used fewer resources will have their jobs prioritized.
The following policies ensure fair use of the cluster resources:

Single job size: The maximum number of nodes a single job depends on the partition (see Partitions).
Run time limit: The maximum run time for a job depends on the partition (see Partitions).
Priority decay: If a job remains in the queue without running for an extended period, its priority may slowly decrease.

what are the Job Priority policies? 
Several factors determine job priority:

Fair-share: This is based on the historical resource usage of your group. The more resources your group has used, the lower your job’s priority becomes, and vice versa.
Job size: Smaller jobs (regarding requested nodes) typically have higher priority.
Queue wait time: The longer a job has been in the queue, the higher its priority becomes.

Job States
Each job in the queue has a state. The main job states are:

Pending (PD): The job is waiting for resources to become available.
Running (R): The job is currently running.
Completed (CG): The job has been completed successfully.

A complete list of job states can be found in the Slurm documentation.

how do i check the state or status of jobs or job steps and view the details of a specific job in the Slurm queuing system?
You can use the following commands to interact with the queue:

squeue: Displays the state of jobs or job steps. It has a wide variety of filtering, sorting, and formatting options. For example, to display your jobs:

squeue -u your_username

scontrol: Used to view and modify Slurm configuration and state. For example, to show the details of a specific job:

scontrol show job your_job_id

Tips for Efficient Queue Usage

Request only the resources you need: Overestimating your job’s requirements can result in longer queue times.
Break up large jobs: Large jobs tend to wait in the queue longer than small jobs. Break up large jobs into smaller ones.
Use idle resources: Sometimes, idle resources can be used. If your job is flexible regarding start time and duration, you can use the --begin and --time options to take advantage of these idle resources.

https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html

Job Scheduling Policies and Priorities
In an HPC environment, efficient job scheduling is crucial for allocating computing resources and ensuring optimal cluster utilization. Job scheduling policies and priorities determine the order in which jobs are executed and the resources they receive. Understanding these policies is essential for maximizing job efficiency and minimizing wait times.

what are the job Scheduling Policies?

FIFO (First-In-First-Out)
Jobs are executed in the order they are submitted. Although simple, this policy may lead to long wait times for large, resource-intensive jobs if smaller jobs are constantly being submitted.

Fair Share
This policy ensures that all users receive a fair share of cluster resources over time. Users with high resource usage may experience reduced priority, allowing others to access resources more regularly.

Priority-Based
Jobs are assigned priorities based on user-defined criteria or system-wide rules. Higher-priority jobs are executed before lower-priority ones, allowing for resource allocation based on user requirements.

what are the Job Priorities

User Priority
Users can assign priority values to their jobs. Higher values result in increased job priority and faster access to resources.

Resource Requirements#
Jobs with larger resource requirements may be assigned higher priority, as they require more significant resources to execute efficiently.

Walltime Limit#
Jobs with shorter estimated execution times may receive higher priority, ensuring they are executed promptly and freeing up resources for other jobs.

what are the  Balancing Policies

Backfilling
This policy allows smaller jobs to “backfill” into available resources ahead of larger jobs, optimizing resource utilization and reducing wait times.

Preemption
Higher-priority jobs can preempt lower-priority ones, temporarily pausing the lower-priority job’s execution to make resources available for the higher-priority job.

What are some strategies for optimizing job execution in the cluster?
Set Realistic Priorities: Assign accurate priorities to your jobs to reflect their importance and resource requirements.
Use Resource Quotas: Be mindful of the resources you request to prevent over- or underutilization.
Leverage Backfilling: Submit smaller, shorter jobs that can backfill into available resources while waiting for larger jobs to start.

Understanding these scheduling policies and priorities empowers you to make informed decisions when submitting jobs, ensuring that your computational tasks are executed efficiently and promptly. If you need further guidance on selecting the right scheduling policy for your job or optimizing your resource usage, our support team is available at rchelp@northeastern.edu or consult our Frequently Asked Questions (FAQs).
Optimize your job execution by maximizing our cluster’s scheduling capabilities. 

https://rc-docs.northeastern.edu/en/latest/runningjobs/interactiveandbatch.html

Interactive and Batch Mode
In our High-Performance Computing (HPC) environment, users can run jobs in two primary modes: Interactive and Batch. This page provides an in-depth guide to both, assisting users in selecting the appropriate mode for their specific tasks.

Interactive Mode
Interactive mode allows users to run jobs that need immediate execution and feedback.

Getting Started with Interactive Mode
To launch an interactive session, use the following command:
# Request an interactive session
srun --pty /bin/bash

This command allocates resources and gives you a shell prompt on the allocated node.

Interactive Mode Use Cases#

Development and Testing: Ideal for code development and testing.
Short Tasks: Best for tasks that require less time and immediate results.



Batch Mode#
Batch mode enables users to write scripts that manage job execution, making it suitable for more complex or longer-running jobs.

Creating Batch Scripts#
A typical batch script includes directives for resource allocation, job names, and commands. Here is an example:
#!/bin/bash
#SBATCH --job-name=my_job
#SBATCH --nodes=1
#SBATCH --ntasks=4
#SBATCH --time=01:00:00

# Commands to execute
module load my_program
srun my_program.exe

Save this script with a .sh extension, e.g., my_script.sh.

Submitting Batch Jobs#
You can submit your batch script using the sbatch command.
sbatch my_script.sh

Monitoring Batch Jobs#
You can monitor the status of your batch job using the squeue command.
squeue -u username

Where username is your actual username.

Use Cases#

Long-Running Jobs: Suitable for extensive simulations or calculations.
Scheduled Tasks: Execute jobs at specific times or under certain conditions.
Automated Workflows: Manage complex workflows using multiple scripts.


Interactive and Batch modes cater to different needs and scenarios in the HPC environment. You can explore both modes to choose the one that best aligns with your tasks. For more detailed guides and support, please consult the above guides or contact our support team at rchelp@northeastern.edu.
Happy computing!

https://rc-docs.northeastern.edu/en/latest/runningjobs/workingwithgpus.html

Working with GPUs

When diving into the realm of high-performance computing, the Graphics Processing Unit (GPU) stands as a paramount resource. This article intends to shed light on the GPU resources available on the cluster, with a particular focus on the NVIDIA GPUs equipped within partitions.

**NVIDIA GPUs in the Spotlight**

Several types of NVIDIA GPUs are available, each coming with its unique specifications and capabilities. Here's a rundown:
What are the specifications of P100 GPU? What is the memory detail of P100 GPU? 
- **P100 (Pascal Architecture)**: This GPU offers 12GB of memory. Interestingly, it does not have Tensor Cores and is equipped with 3,584 CUDA Cores. For public nodes, 12 units come with 3-4 GPUs each, while private nodes host 3 units, each with 4 GPUs.

What are the specifications of V100 GPU? What is the memory detail of V100 GPU?
- V100 PCle (Volta Architecture)**: Boasting 32GB of memory, this unit houses 640 Tensor Cores and 5,120 CUDA Cores. On the public nodes, there are 4 units with 2 GPUs each. The private nodes, on the other hand, have a more varied distribution, with one unit having 2 GPUs with 16GB, and another housing 16GB with units offering 4 GPUs.

What are the specifications of T4 GPU? What is the memory detail of T4 GPU?
- T4 (Turing Architecture)**: This GPU provides 15GB of memory, accompanied by 320 Tensor Cores and 2,560 CUDA Cores. There are 2 units on the public nodes, each with either 3 or 4 GPUs. Conversely, the private node has a single unit with 4 GPUs.

What are the specifications of A100 GPU? What is the memory detail of A100 GPU?
- **A100 (Ampere Architecture)**: With memory options of either 41GB or 82GB, this GPU offers 432 Tensor Cores and a staggering 6,912 CUDA Cores. Public nodes house 3 units, each with 4 GPUs, while private nodes present 15 units with 2-8 GPUs each.

What are the specifications of Quadro RTX 8000 GPU? What is the memory detail of Quadro RTX 8000 GPU?
Quadro RTX 8000 (Turing Architecture)**: This GPU has 46GB of memory, 576 Tensor Cores, and 4,608 CUDA Cores. Notably, it's exclusively found on private nodes, with 2 units each offering 3 GPUs.

What are the specifications of A30 GPU? What is the memory detail of A30 GPU?
- **A30 & RTX A5000 & RTX A6000 (All Ampere Architecture)**: These GPUs provide varying capacities in terms of memory, Tensor Cores, and CUDA Cores. They are primarily found on private nodes, with different units and GPU distributions.

**Understanding Partitions: GPU vs. Multigpu**

The 'gpu' partition is the general go-to GPU resource for HPC users. If, however, one is looking for more firepower, the 'multigpu' comes into play, allowing access to more than a single GPU.

Access to the 'gpu' partition is available for anyone holding a cluster account. If you're eyeing the 'multigpu', though, there's a catch. A ServiceNow ticket has to be submitted, wherein you request temporary access. But remember, this access is only granted given a sufficient need and preparation on your part.

What are the limitations and criteria for accessing the 'multigpu' partition?
**A Note on Multigpu**
Below are the details on multigpu resource.
The 'multigpu' partition isn't an evergreen resource. It's available for a limited time window to cater to pressing requirements. Importantly, this partition is exclusively for those instances that genuinely require multi-GPU capabilities. Given its short-lived availability (like 48 hours), it's prudent to harness its full capacity during its uptime. Should you request access, a member from the RC team will scrutinize your application. They are primarily ensuring the genuine need for this partition. Do bear in mind, user limits hinge on the real-time availability of the 'multigpu' resources and allocations are always made based on user requirements.


Important
Consider the compatibility of the GPU, as some programs do not work on the older k40m or k80 GPUs.
Execute the following command to display the non-Kepler GPUs that are available:
sinfo -p gpu --Format=nodes,cpus,memory,features,statecompact,nodelist,gres

This indicates the state (idle or not) of gpu-types and could be helpful to find one that is idle. However, the command does not give real-time information of the state and should be used carefully.

Requesting GPUs with Slurm#
Use srun for interactive and sbatch for batch mode. The srun example below is requesting 1 node and 1 GPU with 4GB of memory in the gpu partition. You must use the --gres= option to request a gpu:
srun --partition=gpu --nodes=1 --pty --gres=gpu:1 --ntasks=1 --mem=4GB --time=01:00:00 /bin/bash

Note
On the gpu partition, requesting more than 1 GPU (--gres=gpu:1) will cause your request to fail. Additionally, one cannot request all the CPUs on that gpu node as they are reserved for other GPUs.

The sbatch example below is similar to the srun example above, but it submits the job in the background, gives it a name, and directs the output to a file:
#!/bin/bash
#SBATCH --partition=gpu
#SBATCH --nodes=1
#SBATCH --gres=gpu:1
#SBATCH --time=01:00:00
#SBATCH --job-name=gpu_run
#SBATCH --mem=4GB
#SBATCH --ntasks=1
#SBATCH --output=myjob.%j.out
#SBATCH --error=myjob.%j.err

## <your code>

Specifying a GPU type#
You can add a specific type of GPU to the --gres= option (with either srun or sbatch). For a list of available GPU types, refer to the GPU Types column in the table, at the top of this page, that are listed as Public.

Command to request one p100 GPU.#
--gres=gpu:p100:1

Note
Requesting a specific type of GPU could result in longer wait times, based on GPU availability at that time.

Using CUDA#
There are several versions of CUDA Toolkits available on the HPC, including. Use the module avail command to check for the latest software versions on the cluster.
$ module avail cuda

------------------------------- /shared/centos7/modulefiles -------------------------------
cuda/10.0    cuda/10.2          cuda/11.1    cuda/11.3    cuda/11.7    cuda/12.1    cuda/9.1
cuda/10.1    cuda/11.0(default) cuda/11.2    cuda/11.4    cuda/11.8    cuda/9.0     cuda/9.2

To see details on a specific CUDA toolkit version, use module show (e.g., module show cuda/11.4).
To add CUDA to your path, use module load (e.g., module load cuda/11.4 adds CUDA 11.4).

Note
Executing nvidia-smi (i.e., NVIDIA System Management Interface) on a GPU node displays the CUDA driver information and monitor the GPU device.

How do I log onto the GPU interactively, and what are the commands to load Anaconda and CUDA ?

Using GPUs for Deep Learning. 

First, log onto gpu interactively, and load anaconda and CUDA 11.8:
srun --partition=gpu --nodes=1 --gres=gpu:v100-sxm2:1 --cpus-per-task=2 --mem=10GB --time=02:00:00 --pty /bin/bash
module load anaconda3/2022.05 cuda/11.8

Select the tab with the desire deeplearning framework.

Important
Each tab assumes you are on a GPU node before with CUDA 11.8 and anaconda modules loaded as done above.

PyTorch
The following example demonstrates how to build PyTorch inside a conda virtual environment for CUDA version 11.8.

PyTorch’s installation steps for Python 3.9 and Cuda 11.8:#
conda create --name pytorch_env python=3.10 -y
source activate pytorch_env
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia -y

Now, let us check the installation:
python -c 'import torch; print(torch.cuda.is_available())'

If CUDA is detected by PyTorch, you should see the result, True.



TensorFlow
Here are steps for installing CUDA 11.8 with the latest version of TensorFlow (TF).


For the latest installation, use the TensorFlow pip package, which includes GPU support for CUDA-enabled devices:

Tensorflow’s installation steps for Python 3.9 and Cuda 11.8:#
conda create --name TF_env python=3.9 -y
source activate TF_env
conda install -c "nvidia/label/cuda-11.8.0" cuda-toolkit -y
pip install --upgrade pip
pip install tensorflow==2.13.*

Verify the installation:
python3 -c 'import tensorflow as tf; print(tf.test.is_built_with_cuda())' # True

Note
Ignore the Warning messages that get generated after executing the above commands.

PyTorch + TensorFlow

PyTorch and Tensorflow’s installation steps for Python 3.9 and Cuda 11.8:#
conda create --name deeplearning-cuda11_8 python=3.9 -y
source activate deeplearning-cuda11_8
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia -y
conda install -c "nvidia/label/cuda-11.8.0" cuda-toolkit -y
pip install --upgrade pip
pip install tensorflow==2.13.*

Verify installation:
python -c 'import torch; print(torch.cuda.is_available())' # True
python3 -c 'import tensorflow as tf; print(tf.test.is_built_with_cuda())' # True

Tip
Install jupyterlab and few other commonly used datascience packages in the pytorch_env environment:
conda install pandas scikit-learn matplotlib seaborn jupyterlab -y

https://rc-docs.northeastern.edu/en/latest/runningjobs/recurringjobs.html

Recurring Jobs
You can use scrontab to schedule recurring jobs. Its syntax is similar to that of [crontab](https://man7.org/linux/man-pages/man5/crontab.5.html), which is a standard Unix/Linux utility for running programs at specified intervals.

Tip
scrontab vs crontab
If you are familiar with crontab, there are some important differences to note:

The scheduled times for scrontab indicate when your job is eligible to start. They do not start times like in traditional Cron jobs.
Jobs managed with scrontab won’t start if an earlier iteration of the same job is still running. Cron will happily run multiple copies of a job at the same time.
You have one scrontab file for the entire cluster, unlike crontabs, which are stored locally on each computer.

Set Up Your scrontab
Edit Your scrontab
To edit your scrontab file, run scrontab -e. If you prefer to use nano to edit files, run EDITOR=nano scrontab -e.
Lines that start with #SCRON are treated like the beginning of a new batch job and work like #SBATCH directives for batch jobs. Slurm will ignore #SBATCH directives in scripts you run as scrontab jobs. You can use the most common sbatch options just as you would when using Slurm. The first line after your SCRON directives specifies the schedule for your job and the command to run.

Note
All of your scrontab jobs will start with your home directory as the working directory. You can change this with the --chdir Slurm option.

Cron syntax
Crontab’s syntax is specified in five columns, which specify minutes, hours, days of the month, months, and days of the week. If you are new to crontab, using a helper application to generate your cron date fields may be easiest. Two popular options are crontab-generator and cronhub.io. Alternatively, use shorthand syntax such as @hourly, @daily, @weekly, @monthly, and @yearly instead of the five columns.

If you are running a script, it must be marked as executable. Jobs handled by scrontab do not run in a full login shell. Therefore, if you have customized your .bashrc file, you need to add the following line to your script to ensure that your environment is set up correctly:
source ~/.bashrc

Note that the command specified in the scrontab file is executed via bash, NOT sbatch. You can list multiple commands separated by ; and use other shell features, such as redirects. Any #SBATCH directives in executed scripts will also be ignored. To use the scrontab file, you must use #SCRON instead.

Note
Your crontab jobs will appear to have the same JobID every time they run until the next time you edit your crontab file. Only the most recent job will be logged to the default output file. If you want a deeper history, redirect the output in your scripts to filenames with more unique names, such as a date or timestamp. For example:
python my_script.py > $(date +"%Y-%m-%d")_myjob_scrontab.out

If you want to see the accounting of a job that was handled by crontab, for example, job 12345, run the following command to view the Slurm accounting:
sacct --duplicates --jobs 12345
# or with short options
sacct -Dj 12345

Recurring Job Examples

Running a Daily Simulation
This example demonstrates how to submit a 6-hour simulation that is eligible to start at 12:00 AM every day.
#SCRON --time 6:00:00
#SCRON --cpus-per-task 4
#SCRON --name "daily_sim"
#SCRON --chdir /home/netid/project
#SCRON -o my_simulations/%j-out.txt
@daily ./simulation_v2_final.sh

Running a Weekly Transfer Job
This example demonstrates how to submit a transfer script set to start every Wednesday at 8:00 PM.
#SCRON --time 1:00:00
#SCRON --partition transfer
#SCRON --chdir /home/netid/project/to_transfer
#SCRON -o transfer_log_%j.txt
0 20 * * 3 ./rclone_commands.sh

Capture output from each run in a separate file
By default, crontab overwrites the output file from the previous run when the same jobid is used. To avoid this, you can redirect the output to a file with a date stamp.
0 20 * * 3 ./commands.sh > myjob_$(date +%Y%m%d%H%M).out

https://rc-docs.northeastern.edu/en/latest/runningjobs/debuggingjobs.html

Debugging and Troubleshooting Jobs#
This page focuses on common issues encountered when running jobs, how to interpret error messages, and some practical strategies to debug these problems.

Understanding Slurm Errors#
Understanding the error messages that Slurm can generate is critical to resolving job issues.

CREATE TABLE OF COMMON ERRORS:#

Common Slurm Error Messages and Potential Solutions

sbatch: error: Batch job submission failed: Socket timed out on send/recv operation

srun: error: Unable to allocate resources: No such file or directory

Checking Job Status
How to check Job Status
This section will focus on the scontrol show jobid -dd <jobid> command to provide detailed information about the job. Example usage and output of the command can be included here.

Redirecting Standard Output and Error Streams#
Explain how to use #SBATCH --output and #SBATCH --error to redirect standard output and error to files. This helps in capturing any error messages produced by the program.

Debugging Strategies
This section will explain the importance of testing jobs on a small scale before submitting large jobs. It can discuss using a single node for testing, using the –test-only option, and examining output and error files.
#SBATCH --output=myjob.out
#SBATCH --error=myjob.err

Interactive Job Session
Explain how to start an interactive job session with the salloc command, allowing users to interact directly with their job and troubleshoot issues in real time.
salloc --nodes=1 --time=01:00:00 --account=myaccount

Using Debuggers#
Discuss using debuggers such as GDB for C/C++ and PDB for Python to step through the code and identify bugs. Include a brief example of how to use them.

Common Job Issues and Their Resolutions#
List some common issues users might encounter, like jobs getting stuck in the queue, jobs not producing the expected output, or jobs using more memory than expected. Offer solutions or workarounds for each of these issues.

https://rc-docs.northeastern.edu/en/latest/datamanagement/discovery_storage.html

Data Storage Options
How can I request data storage solutions
RC is responsible for procuring and maintaining several data storage options, including active and archive storage solutions. If you are affiliated with Northeastern, you can request one or more storage solutions to meet your needs. If you anticipate needing storage as part of a grant requirement, please schedule a storage consultation with an RC staff member to understand what storage options best meet your research needs.


Active Storage
Two main storage systems are connected to Northeastern’s HPC cluster: /home and /scratch; these options have specific quotas and limitations. The list below details the storage options available on the HPC cluster if you have an account. Every individual with an account has a /home and /scratch. While research groups can request additional storage on the /work storage system, /work storage is not currently provisioned for individuals.

Important
The /scratch space is only for temporary storage; this storage is not backed up, and there is a purge policy for data older than 28 days. Please review the /scratch policy on our Policy page.

What is the purpose of the "/home" directory in the storage system, and what are some important details about "/home" directory use, including "/home" directory quota and recommended use cases?
$HOME: /home/<username> where username is your NU login, e.g., /home/j.smith

Description: All users are automatically given a /home when their account is created. This storage is mainly intended for storing relatively small files such as script files, source code, and software installation files. While /home is permanent storage backed up and replicated, /home is not permanent storage. /home also has a small quota, so you should frequently check your space usage (use a command such as du -h /home/<yourusername> where <yourusername> is your username to see the total space usage). You should use your /scratch for running jobs and directing output files.
Quota: 75GB



What is the purpose of the "/scratch" directory in the storage system
Scratch: /scratch/<username>

Description: All users are automatically given a /scratch when their account is created. Scratch space is a shared space for all users. The total storage available is 2.5PB; however, while this is permanent, it is for temporary use only. It is not backed up. Data on /scratch should be moved to another location for permanent storage as soon as possible. You should run your jobs from and direct output to your /scratch for best performance. However, moving your files from scratch is the best practice to avoid potential data loss.
Quota: N/A


How can i request additional storage on "/work"? what are the key details about "/work" storage space?
Work: /work/<groupname>

Description: Research groups can request additional storage on /work. A PI can request this extra storage through the New Storage Space Request. This is permanent, persistent, and long-term storage for storing data actively used for research. It can be accessed by all members of the research group who have the necessary access permissions, facilitating collaboration and seamless sharing of data within the group.
Quota: Each group can request up to 35TB of free storage across all supplemental storage tiers: /work/<groupname> and /nese.
Access Request: Students with research groups can request access to the PI’s storage on /work. To expedite the request process, we recommend that you inform the group owner that they will be receiving an email requesting their permission to grant you access to /work before you submit the request.

To request access to /work, students can either create a  ServiceNow ticket with RC or email rchelp@northeastern.edu to automatically generate a ticket in ServiceNow. Please include both the storage space name and the PI’s name.
Once you have been added to the Unix group for the space on /work, please close all open connections to the HPC and log in again for the changes to reflect on your end. As you know, UNIX groups are assigned at login time, and this step ensures that your access privileges are updated accordingly. To confirm you have been added to the group, you can run the command groups.

Default Permission: By default, users are given read and write access when added to /work. However, specific permissions might be granted at the PI’s request.

Attention
The /research storage tier is no longer provided. Please contact Research Computing if you are a former user of /research and have questions or issues related to /research by submitting a ticket. Other storage options include /work, Sharepoint, and OneDrive.


What is the purpose of the "/nese" archival storage
Archival Storage

If you are not connected to the campus internet, you must be connected to the university’s VPN (i.e., GlobalProtect) before accessing the /nese system. You can find detailed information about downloading and using the GlobalProtect VPN in the FAQ: VPN and remote access.

NAME: /nese

Description: This is archival, non-permanent storage intended for researchers needing a long-term storage option for their data.
Quota: Each group can request up to 35TB of free storage across all supplemental storage tiers: /work/<groupname> and /nese.

https://rc-docs.northeastern.edu/en/latest/datamanagement/transferringdata.html

Transfer Data
The HPC has a dedicated transfer node that you must use to transfer data to and from the cluster. You cannot transfer data from any other node or the HPC to your local machine. The node name is <username>@xfer.discovery.neu.edu: where <username> is your Northeastern username to login into the transfer node.
You can also transfer files using Globus. This is highly recommended if you need to transfer large amounts of data. See Using Globus for more information.
If you are transferring data from different directories on the HPC, you need to use a compute node (see Interactive Jobs: srun Command or Batch Jobs: sbatch) with SCP, rsync, or the copy command to complete these tasks. You should use the --constraint=ib flag (see Hardware Overview) to ensure the fastest data transfer rate.

Caution
The /scratch space is for temporary file storage only. It is not backed up. If you have directed your output files to /scratch, you should transfer your data from /scratch to another location as soon as possible. See Data Storage Options for more information.


How can I use the "scp" command to transfer files or directories between my local machine and the HPC server, and what are the basic syntax and options involved in such transfers?
Transfer data via Terminal

SCP
You can use scp to transfer files/directories to and from your local machine and the HPC. As an example, you can use this command to transfer a file to your /scratch space on the HPC from your local machine:
scp <filename> <username>@xfer.discovery.neu.edu:/scratch/<username>

where <filename> is the name of the file in your current directory you want to transfer, and <username> is your Northeastern username. So that you know, this command is run on your local machine.
If you want to transfer a directory in your /scratch called test-data from the HPC to your local machine’s current working directory, an example of that command would be:
scp -r <username>@xfer.discovery.neu.edu:/scratch/<username>/test-data .

where -r flag is for the recursive transfer because it is a directory. So that you know, this command is run on your local machine.

Rsync
You can use the rsync command to transfer data to and from the HPC and local machine. You can also use rsync to transfer data from different directories on the cluster.
The syntex of rsync is
rsync [options] <source> <destination>

An example of using rsync to transfer a directory called test-data in your current working directory on your local machine to your /scratch on the HPC is
rsync -av test-data/ <username>@xfer.discovery.neu.edu:/scratch/<username>

where this command is run on your local machine in the directory that contains test-data.
Similarly, rsync can be used to copy from the current working directory on the HPC to your current working directory on your local machine:
rsync -av <username>@xfer.discovery.neu.edu:/scratch/<username>/test-data .

where this command is run on your local machine in the current directory that you want to save the directory test-data.
You can also use rsync to copy data from different directories on the HPC:
srun --partition=short --nodes=1 --ntasks=1 --time=01:05:00 --constraint=ib --pty /bin/bash
rsync -av /scratch/<username>/source_folder /home/<username>/destination_folder

sbatch
You can use a sbatch job to complete data transfers by submitting the job to the HPC queue. An example of using rsync through a sbatch script is as follows:
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks=2
#SBATCH --time=0:05:00
#SBATCH --job-name=DataTransfer
#SBATCH --mem=2G
#SBATCH --partition=short
#SBATCH --constraint=ib
#SBATCH -o %j.out
#SBATCH -e %j.err

rsync -av /scratch/<username>/source_folder /home/<username>/destination_folder

where we are transferring the data from source_folder to the destination_folder.

SSHFS
If you want to use sshfs, use it with the dedicated transfer node xfer.discovery.neu.edu. It will not work on the login or compute nodes. On a Mac, you will also have to install macFUSE and sshfs (please refer to macFUSE) to use the sshfs command.
Use this syntax to perform file transfers with sshfs:
sshfs <username>@xfer.discovery.neu.edu:</your/remote/path> <your/local/path> -<options>

For example, this will mount a directory in your /scratch named test-data to a local directory on your machine ~/mount_point:
sshfs <username>@xfer.discovery.neu.edu:/scratch/<username>/test-data ~/mount_point

You can interact with the directory from your GUI or use the terminal to perform tasks on it.

How can I use GUI applications like to transfer data between different directories on the HPC and my local machine, and what are the basic steps involved in setting up these transfers?

Transfer via GUI Application

OOD’s File Explorer
You can use OOD’s File Explorer application to transfer data from different directories on the HPC and also to transfer data to and from your local machine to the HPC. For more information to complete this please see OOD File Explorer.

MobaXterm
You can use MobaXterm to transfer data to and from the HPC. Please check out MobaXterm to download MobaXterm.

Open MobaXterm.
Click Session, then select SFTP.
In the Remote host field, type xfer.discovery.neu.edu
In the Username field, type your Northeastern username.
In the Port field, type 22.
In the Password box, type your Northeastern password and click OK. Click No if prompted to save your password.

You will now be connected to the transfer node and can transfer files through MobaXterm. Please refer to MobaXterm for further information.

FileZilla
You can use FileZilla to transfer data to and from the HPC. Please check out FileZilla to download.

Open FileZilla.
In the Host field, type sftp://xfer.discovery.neu.edu
In the Username field, type your Northeastern username.
In the Password field, type your Northeastern password.
In the Port field, type 22.

You will now be connected to the transfer node and can transfer files through FileZilla. Please refer to FileZilla for further information.

https://rc-docs.northeastern.edu/en/latest/datamanagement/globus.html

What is globus?
Globus is a data management system that you can use to transfer and share files. Northeastern has a subscription to Globus, and you can set up a Globus account with your Northeastern credentials. You can link your accounts if you have another account, either personal or through another institution.
How can i use globus?
To use Globus, you will need to set up an account, as detailed below. Then, as detailed below, you will need to install Globus Connect to create an endpoint on your local computer. After completing these two initial setup procedures, you can use the Globus web app to perform file transfers. See Using the Northeastern endpoint for a walkthrough of using the Northeastern endpoint on Globus.

How can I set up a Globus account , and what do I need to do to create a Globus endpoint on my local computer using Globus Connect?
How do I create a Globus account , and what steps are involved in the account setup process?
Globus Account Set Up
You can use the following instructions to set up an account with Globus using your Northeastern credentials.

Go to Globus.
Click Log In.
From the Use your existing organizational login, select Northeastern University, and then click Continue.
Enter your Northeastern username and password.
If you do not have a previous Globus account, click Continue. If you have a previous account, click the Link to an existing account.
Check the agreement checkbox, and then click Continue.
Click Allow to permit Globus to access your files.

You can then access the Globus File Manager app.

Tip
If you received an account identity that includes your NUID number (for example, 000123456@northeastern.edu), you can follow the “Creating and linking a new account identity” instructions below to get a different account identity if you want a more user-friendly account identity. You can then link the two accounts together.

How can I create a new identity with a different username and link it to my existing Globus account?
Creating and linking a new account identity (Optional)
If you created an account through Northeastern University’s existing organizational login and received a username that included your NUID, you can create a new identity with a different username and link the two accounts together. A username you select instead of one with your NUID can make it easier to remember your login credentials.

Go to Globus.
Click Log In.
Click Globus ID to sign in.
Click Need a Globus ID? Sign up.
Enter your Globus ID information.
Enter the verification code that Globus sends to your email.
Click Link to an existing account to link this new account with your primary account.
Select Northeastern University from the drop-down box and click Continue to be taken to the Northeastern University single sign-on page.
Enter your Northeastern username and password.

You should now see your two accounts linked in the Account section on the Globus web app.

How can I install Globus Connect Personal (GCP) on my laptop, and what steps do I need to follow to set Globus Connect Personal (GCP) up as an endpoint for file transfers?
Install Globus Connect Personal (GCP)
Use Globus Connect Personal (GCP) as an endpoint for your laptop. You first need to install GCP using the following procedure and be logged in to Globus before you can install GCP.

Go to Globus File Manager.
Enter a name for your endpoint in the Endpoint Display Name field.
Click Generate Setup Key to generate a setup key for your endpoint.
Click the Copy icon next to the generated setup key to copy the key to your clipboard. You will need this key during the installation of GCP in step 6.
Click the appropriate OS icon for your computer to download the installation file.
After downloading the installation file to your computer, double-click on the file to launch the installer.

Accept the defaults on the install wizard. After the installation, you can use your laptop as an endpoint within Globus.

Note
You cannot modify an endpoint after you have created it. If you need an endpoint with different options, you must delete and recreate it. Follow the instructions on the Globus website for deleting and recreating an endpoint.

Working with Globus
After you have an account and set up a personal endpoint using Globus Connect personal, you can perform basic file management tasks using the Globus File Manager interface, such as transferring files, renaming files, and creating new folders. You can also download and use the Globus Command Line Interface (CLI) tool. Globus also has extensive documentation and training files for you to practice with.

Using the Northeastern endpoint
To access the Northeastern endpoint on Globus, on the Globus web app, click File Manager, then in the Collection text box, type Northeastern. The endpoints owned by Northeastern University are displayed in the collection area. The general Northeastern endpoint is northeastern#discovery. Using the File Manager interface, you can easily change directories, switch the direction of transferring to and from, and specify options such as transferring only new or changed files. Below is a procedure for transferring files from Discovery to your personal computer, but with the flexibility of the File Manager interface, you can adjust the endpoints, file view, direction of the transfer, and many other options.


How do I transfer files from Discovery to my personal computer using Globus?
To transfer files from Discovery to your personal computer, do the following

Create an endpoint on your computer using the procedure above “Install Globus Connect,” if you have not done so already.
In the File Manager on the Globus web app, in the Collections textbox, type Northeastern, then in the collection list, click the northeastern#discovery endpoint.
click Transfer or Sync to in the right-pane menu.
Click in the Search text box, and then click the name of your endpoint on the Your Collections tab. You can now see the list of your files on Discovery on the left and on your personal computer on the right.
Select the file or files from the right-side list of Discovery files that you want to transfer to your personal computer.
Select the destination folder from the left-side list of the files on your computer.
(Optional) Click Transfer & Sync Options and select the transfer options you need.
Click Start.

Connecting to Google Drive
The version of Globus currently on Discovery allows you to connect to Google Drive by first setting up the connection in GCP. This will add your Google Drive to your current personal endpoint.
Just so you know, you will first need a personal endpoint, as outlined in the procedure above. This procedure is slightly different from using the Google Drive Connector with
Globus version 5.5. You will need your Google Drive downloaded to your local computer.

How can I add Google Drive to my Globus Connect Personal (GCP) endpoint?
To add Google Drive to your endpoint, do the following

Open the GCP app. Right-click the G icon in your taskbar on Windows and select Options. Click the G icon in the menu bar on Mac and select Preferences.
On the Access tab, click the + button to open the Choose a directory dialog box.
Navigate to your Google Drive on your computer and click Choose.
Click the Shareable checkbox to make this a shareable folder in Globus File Manager, and then click Save.

You can now go to Globus File Manager and see that your Google Drive is available as a folder on your endpoint.

Command Line Interface (CLI)#
The Globus Command Line Interface (CLI) tool allows you to access Globus from the command line. It is a stand-alone app that requires a separate download
and installation. Please refer to the Globus CLI documentation for working with this app.

Globus documentation and test files#
Globus provides detailed instructions on using Globus and has test files for you to practice with. These are free for you to access and use. We would like to encourage you to use the test files to become familiar with the Globus interface. You can access the Globus documentation and training files on the Globus How To website.



https://rc-docs.northeastern.edu/en/latest/datamanagement/securityandcompliance.html

Security and Compliance
Security and compliance in data management are essential to protect sensitive information and meet regulatory requirements. This page provides guidelines and best practices for ensuring data security and compliance on HPC systems.

Data Classification
Understanding the sensitivity of your data is the first step in protecting it.

Identify Sensitive Data: Recognize what data needs special protection.
Classify Accordingly: Categorize data based on sensitivity.

Access Control
Controlling who can access data is fundamental to security.

Implement Role-Based Access Control (RBAC): Grant permissions based on roles.
Use Strong Authentication Methods: Consider multi-factor authentication.
Regularly Review Access Rights: Ensure only authorized individuals have access.

Encryption
Encrypting data helps protect it from unauthorized access.

Encrypt Data at Rest: Use file-level or disk encryption.
Encrypt Data in Transit: Secure data when transferring between systems.

Auditing and Monitoring
Track those who access data and when it is crucial for security.

Enable Logging: Log all access and modifications to sensitive data.
Monitor Regularly: Set up automated monitoring and alerts.

Compliance Standards
Adhere to legal and organizational compliance standards.

Understand Applicable Regulations: GDPR, HIPAA, or other relevant laws.
Follow Organizational Policies: Adhere to your institution’s policies.

Incident Response
Prepare for and respond to any security incidents.

Have an Incident Response Plan: Outline steps to take if a breach occurs.
Regularly Review and Update the Plan: Keep it current with changing risks.

Tips for Secure Data Management

Use Secure Connections: SSH or VPN for remote access.
Avoid Storing Sensitive Data on Shared File Systems: Unless adequately secured.
Educate Yourself and Your Team: Stay informed about best practices and threats.

Further Resources

NIST Cybersecurity Framework
ISO/IEC 27001: Information Security Management

Data security and compliance are shared responsibilities. Always consult your organization’s security and legal teams to ensure you meet all applicable requirements and follow best practices. Please feel free to contact the HPC support team if you have specific questions or need assistance.




https://rc-docs.northeastern.edu/en/latest/software/systemwide/modules.html

How do I manage application environments on high-performance computing systems?
Using Module

Note
Some modules conflict, resulting in the software behaving differently than expected. Also, if there are multiple software versions and you load more than one version of the software, only the latest version will be used. Use module list to view the modules loaded into your path.

The ‘modules’ tool is widely used for managing application environments on High-Performance Computing (HPC) systems. This page will provide an overview of ‘modules’, instructions on using them, best practices, use cases, and more.
The module system on the cluster includes many commonly used scientific software packages that you can load into your path when you need them and unload when you no longer need them. In essence, ‘modules’ handle environment variables like PATH and LD_LIBRARY_PATH to avoid conflicts between software applications.
Use the module avail command to show a list of the most currently available software on the cluster.

Tip
The which <target> prints the path of executable <target> in your path (e.g., which python prints the python that will execute if called).

Module commands#
The following are common module commands helpful in interacting with software packages on the cluster.

Here is the conversion of the table into a descriptive paragraph format:

1. The `module avail` command is utilized to display a comprehensive list of all available modules on the cluster, providing users with an overview of the software packages that can be accessed.

2. When it comes to understanding which modules are currently active, the `module list` command comes into play. It shows a list of all the modules that have been loaded at that moment.

3. To delve deeper into the specifics of a software package, the `module show <module>` command is used. This command allows users to view detailed information about the target module, including its functions and configurations.

4. The `module load <module>` command serves an essential function by enabling users to load a specific module. This command is key for accessing and utilizing the functionalities of a particular software package on the cluster.

5. Conversely, the `module unload <module>` command is designed to unload a specific module. This is particularly useful for managing resources or changing the software environment by removing unwanted or unnecessary modules.

6. For a more comprehensive approach, the `module purge` command is available. This command unloads all currently loaded modules, essentially resetting the module environment to its default state.

7. Finally, the `module swap <module1> <module2>` command offers a convenient way to replace one module with another. It is particularly handy for switching between different versions of software packages or different sets of functionalities.



Caution
module purge unloads all modules from your environment, including the default module discovery/2019-02-21. This module sets the HTTP proxy needed to access the internet from nodes. If you accidentally purge this module, it automatically reloads by logging out and then back in. You can also load it manually module load.


What should I do before loading a module, and how can I check for dependencies or required commands using the "module show <name of module>" command?
Module show example
Before loading a module, type module show <name of module> to see if there are any dependencies or commands that you need to execute
before loading the module. Sometimes, a module might depend on loading other modules to work as expected. While modules are convenient for loading software on the cluster, scientific software can come with many packages and dependencies. In addition to the module, you will need to look over other ways to load the cluster’s software.



Here is an example of using module show to show details for the Amber software package.
$ module show amber

Command-line output.#
/shared/centos7/modulefiles/amber/18-mpi:

module-whatis     loads the modules environment for Amber 18 MPI parallel executable
                  on CPU nodes.

Please load the following modules:
module load openmpi/3.1.2
module load amber/18-mpi
module load python/2.7.15

setenv            AMBER_HOME /shared/centos7/amber/amber18-cpu
prepend-path      PYTHONPATH /shared/centos7/amber/amber18-cpu/lib/python2.7/site-packages
prepend-path      PATH /shared/centos7/amber/amber18-cpu/bin
prepend-path      LD_LIBRARY_PATH /shared/centos7/amber/amber18-cpu/lib
prepend-path      C_INCLUDE_PATH /shared/centos7/amber/amber18-cpu/include
prepend-path      CPLUS_INCLUDE_PATH /shared/centos7/amber/amber18-cpu/include

Module load and unload example#
The software module stata/15 was loaded and unloaded in the following code snippet. After each, module list displays loaded modules showing whether or not STATA was loaded.

Loading Stata version 15.#
$ module load stata/15
$ module list
Currently Loaded Modulefiles:
1) discovery/2019-02-21     2) stata/15

Unloading Stata version 15.#
$ module unload stata/15
$ module list
Currently Loaded Modulefiles:
1) discovery/2019-02-21

Launching applications via X11 Forwarding#
If you are attempting to open a GUI-based software application that  uses X11 forwarding to display, such as MATLAB or Maestro, and you get an error such as Error: unable to open display localhost:19.0, this is most likely due to an issue with passwordless SSH. See Connecting To Cluster for tips and troubleshooting information opening applications that use X11 forwarding.

Advanced Module Usage#
Explain how to use modules in job scripts or interactive sessions, using module save and module restore to manage module collections and other advanced topics.

Creating Module Files#



Best Practices Using Modules#

Only load the modules you need: Unnecessary modules can cause conflicts.
Know module hierarchies: Some modules might only become available after loading another module.
Always load a specific module version: This avoids problems if the default version changes.

Common Issues and Troubleshooting Modules#
Cover common issues users might face while using ‘modules’, like conflicts, missing modules, or unexpected behavior, and provide troubleshooting tips.

Use-Cases for Modules#
Provide several examples of how to use ‘modules’ for various use cases. This could include:

Loading modules for a specific software stack for a project.
Swapping compiler modules to test code compatibility.
Using module collections to switch between different project environments easily.

https://rc-docs.northeastern.edu/en/latest/software/systemwide/mpi.html


Using MPI
Messaging Passing Interface (MPI) is a standardized and portable message-passing system designed to function on a wide variety of parallel computing architectures. It provides a library of functions that enable a program to distribute a computational task across multiple nodes in a cluster.
There are multiple implementations of MPI including OpenMPI (Open Source Message Passing Interface), MPICH, and Intel MPI. OpenMPI is a widely used MPI implementation in the HPC community and the one we will be working with through our documentation.


What are the prerequisites for getting started with MPI on a Slurm-based HPC cluster?  What prerequisites should I have before starting with MPI on a Slurm-based high-performance computing cluster?
Getting Started with MPI
To get started with MPI on a Slurm-based HPC cluster, you should have:

Basic knowledge of Linux/Unix commands
Familiarity with a programming language supported by MPI (e.g., C, C++, FORTRAN) if you are developing a program
Understand how to load MPI module on the HPC
Understand how to compile your source code and run the binaries (compiled languages) or to run the interpreted language with MPI

MPI libraries on Discovery
There are many versions of OpenMPI, MVAPICH, and MPICH that are available on the HPC as modules compiled with different compilers and additional libraries and features. To see them, use the module avail openmpi, module avail mpich, and module avail mvapich respectively.
Use the module show command to view information about the compilers you need to use with these libraries and if they support InfiniBand (IB) or not. For example, module show openmpi/4.1.0-zen2-gcc10.1.

Output for module show openmpi/4.1.0-zen2-gcc10.1

/shared/centos7/modulefiles/openmpi/4.1.0-zen2-gcc10.1:

module-whatis	 Loads the executables, libraries and headers for OpenMPI v. 4.1.1. Built using Intel 2021 compilers on AMD EPYC architecture (zen2).

Please note - this MPI module supports communication through the HDR200 InfiniBand network by using the Mellanox (OFED 5.3) UCX (1.10.1) framework with cross platform unified API. To make sure InfiniBand is being used, make sure to compile and run your applications using this module only on AMD EPYC architectures (zen2).

To allocate the zen2 arch compute node, add the following flag to your SLURM command: --constraint=zen2
For more details:
https://rc-docs.northeastern.edu/en/latest/hardware/hardware_overview.html

To use the module, type:
module load gcc/10.1.0
module load openmpi/4.1.0-zen2-gcc10.1

conflict	 openmpi
prepend-path	 PATH /shared/centos7/openmpi/4.1.0-zen2-gcc10.1/bin
prepend-path	 MANPATH /shared/centos7/openmpi/4.1.0-zen2-gcc10.1/share/man
prepend-path	 LD_LIBRARY_PATH /shared/centos7/openmpi/4.1.0-zen2-gcc10.1/lib
prepend-path	 CPATH /shared/centos7/openmpi/4.1.0-zen2-gcc10.1/include
prepend-path	 LIBRARY_PATH /shared/centos7/openmpi/4.1.0-zen2-gcc10.1/lib
setenv		 OMPI_MCA_btl ^vader,tcp,openib,uct

Running a MPI Program#
The following is a basic slurm script for running an MPI program with annotations:
#!/bin/bash
#SBATCH --job-name=test_job         # Set the job name
#SBATCH --output=res_%j.out         # Set the output file name (%j expands to jobId)
#SBATCH --ntasks=4                  # Request 4 tasks
#SBATCH --time=01:00:00             # Request 1 hour runtime
#SBATCH --mem-per-cpu=2000          # Request 2000MB memory per CPU

module load openmpi/4.0.5           # Load the necessary module(s)
mpirun -n 4 ./your_program          # Run your MPI executable

Note
For MPI tasks, --ntasks=X is used, where X requests the number of cpu cores for tasks.

This script specifies that it needs 4 tasks (i.e., CPU cores), a maximum of 10 minutes of runtime, and 2000MB of memory per CPU. It then loads the OpenMPI module and runs the MPI program using mpirun.

Tip
Best practice for writing your sbatch script is including the versions of the modules you are loading to ensure you always have your expected environment on the HPC.

OpenMPI Tuning for Performance Optimization#
OpenMPI provides a variety of environment variables that can be used to optimize the runtime characteristics of your MPI program for maximum performance. For instance, you can specify which network interfaces to use by setting the OMPI_MCA_btl variable:
export OMPI_MCA_btl=self,vader,tcp

Tip
You can also include or exclude certain network interfaces by setting the OMPI_MCA_btl_tcp_if_include or OMPI_MCA_btl_tcp_if_exclude variables.

Also you can check if certain MPI modules already have certain OMPI_MCA_btl set by using the module show command and looking for the setenv options listed.
In addition, OpenMPI lets you control the placement of processes on nodes, which can be critical for performance. The --map-by and --bind-to options dictate how processes are mapped to hardware resources and how they are bound to those resources, respectively.
Remember, optimizing for performance often requires a thorough understanding of your application, your hardware, and MPI.

Troubleshooting and Debugging MPI Programs#
Debugging MPI programs can be challenging due to their parallel nature. Fortunately, OpenMPI provides several tools and techniques to help with this.
One useful feature is verbose error reporting. To enable this, set the OMPI_MCA_mpi_abort_print_stack to 1:
export OMPI_MCA_mpi_abort_print_stack=1

If you have a parallel debugger such as TotalView or DDT, you can use it with OpenMPI using the mpiexec command with the -tv or -debug options, respectively.
Finally, remember to check your slurm job output files for any error messages or abnormal output. Sometimes, the issue may be with how you are running your job rather than with your MPI program itself.

Benchmarking OpenMPI Performance#
Benchmarking is a method used to measure the performance of a system or one of its components under different conditions. For MPI, benchmarks can be used to measure its communication and computation efficiency on different high-performance computing (HPC) systems. By comparing these benchmarks, you can identify potential bottlenecks and areas for improvement to optimize the performance of MPI.

Tools for Benchmarking#
There are several tools available for benchmarking MPI, including the following:

HPC Challenge (HPCC): This benchmark suite measures a range of metrics, including latency and bandwidth, as well as floating-point computation performance.
Intel MPI Benchmarks (IMB): A suite of benchmarks provided by Intel specifically for MPI. It includes a set of MPI-1 and MPI-2 function benchmarks and measures point-to-point communication, MPI data types, collective communication, and more.
OSU Micro-Benchmarks (OSU-MB): A lightweight set of benchmarks designed to measure latency, bandwidth, and other performance metrics for various MPI functions.

To use these tools, you generally need to download and compile them, and then run them using a slurm job script.

Developing with MPI#

Hello world program#
The fundamental concept in MPI is the communicator, which defines a group of processes that can send messages to each other. By default, all processes belong to the MPI_COMM_WORLD communicator. Here is a simple C++ program that using MPI:
#include <mpi.h>
#include <stdio.h>

int main(int argc, char** argv) {
    MPI_Init(NULL, NULL);

    // Get the number of processes
    int world_size;
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);

    // Get the rank of the process
    int world_rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);

    printf("Hello world from processor %d out of %d processors\n", world_rank, world_size);

    MPI_Finalize();
}

In this code, MPI_Init initializes the MPI environment, MPI_Comm_size gets the number of processes, MPI_Comm_rank gets the rank (ID) of the process, and MPI_Finalize ends the MPI environment. In the C/C++ language, the #include <mpi.h> header file needs to be added to compile MPI code.
To understand how to run an MPI program, let us write a simple program that prints a "Hello, World!" message from each process.
First, create a file called hello_world.c in your preferred text editor and add the following code:
#include <mpi.h>
#include <stdio.h>

int main(int argc, char** argv) {
    MPI_Init(NULL, NULL);
    int world_rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);
    int world_size;
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);
    printf("Hello, World! I am process %d out of %d\n", world_rank, world_size);
    MPI_Finalize();
    return 0;
}

This program initializes the MPI environment, gets the rank of the process, gets the total number of processes, and then prints a message. Finally, it finalizes the MPI environment.
Next, compile the program using the mpicc command, which is a wrapper for the C compiler that includes the OpenMPI libraries:
mpicc hello_world.c -o hello_world

Where -o is the output flag, naming the executable hello_world. If omitted (i.e., mpicc helloworld_c would generate a compiled executable named a.out by default).
Finally, create a slurm job script to run the program:
#!/bin/bash
#SBATCH --job-name=hello_world
#SBATCH --output=result.txt
#SBATCH --ntasks=4
#SBATCH --time=10:00
#SBATCH --mem-per-cpu=2000

module load openmpi/4.0.5
mpirun -n 4 ./hello_world

Submit this script to slurm with the sbatch command:
sbatch job_script.sh

You should see output in the result.txt file that shows "Hello, World!" messages from each process.

MPI Communication: Send and Receive Operations#
MPI allows processes to communicate by sending and receiving messages. These messages can contain any type of data. Here is a simple example of using MPI_Send and MPI_Recv to send a number from one process to another:
#include <mpi.h>
#include <stdio.h>

int main(int argc, char** argv) {
    MPI_Init(NULL, NULL);

    int world_rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);

    int number;
    if (world_rank == 0) {
        number = -1;
        MPI_Send(&number, 1, MPI_INT, 1, 0, MPI_COMM_WORLD);
    } else if (world_rank == 1) {
        MPI_Recv(&number, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
        printf("Process 1 received number %d from process 0\n", number);
    }

    MPI_Finalize();
}

MPI Monte Carlo#
A key aspect of using OpenMPI is the ability to implement parallel algorithms, which can significantly speed up computation. Here is an example of a parallel version of the Monte Carlo method for estimating the number \(\pi\):
#include <mpi.h>
#include <stdio.h>
#include <stdlib.h>
#include <time.h>

int main(int argc, char** argv) {
    MPI_Init(NULL, NULL);

    int world_rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);
    srand(time(NULL) * world_rank); // Ensure random numbers on all processes

    int local_count = 0;
    int global_count = 0;
    int flip = 1 << 24;
    double x, y, z;

    // Calculate hits within circle locally
    for (int i = 0; i < flip; i++) {
        x = (double)rand() / (double)RAND_MAX;
        y = (double)rand() / (double)RAND_MAX;
        z = sqrt((x*x) + (y*y));
        if (z <= 1.0) {
            local_count++;
        }
    }

    // Combine all local sums into the global sum
    MPI_Reduce(&local_count, &global_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);

    // Process 0 calculates pi and prints the result
    if (world_rank == 0) {
        double pi = ((double)global_count / (double)(flip * world_rank)) * 4.0;
        printf("The estimated value of pi is %f\n", pi);
    }

    MPI_Finalize();
}

In this code, each process performs its own Monte Carlo simulation and then combines its results with those from other processes using the MPI_Reduce function.

Using OpenMPI with Python’s mpi4py#
mpi4py is a Python package that provides bindings to the MPI standard. It allows Python programs to take advantage of the distributed memory model and scale across multiple nodes of a high performance computing cluster, just like MPI.
The mpi4py package has been designed to be as close as possible to the MPI standard, providing Python developers with a familiar and straightforward interface to MPI.
In this program, process 0 sends the number -1 to process 1, which receives it and prints it.
To install mpi4py inside of a conda environment:
srun -n 4 --pty /bin/bash
module load anaconda3/2022.05
mkdir -p /path/to/mpi4py_env
conda create --prefix=/path/to/mpi4py_env -y
source activate /path/to/mpi4py_env
conda install -c conda-forge mpi4py

In your preferred text editor, write a file hello.py that contains the following:
from mpi4py import MPI

comm = MPI.COMM_WORLD
rank = comm.Get_rank()

if rank == 0:
    print('Hello from the master process')
else:
    print(f'Hello from process {rank}')

This program gets the communicator for the current process, obtains the rank of the process, and then prints a message. If the rank is 0, the process is the master, otherwise, it is a worker.
You can run this program using the mpirun command:
srun -n 4 --pty /bin/bash
mpirun -np 4 python hello_world.py

This will run the program on 4 processes.
Just like in MPI, mpi4py allows you to perform point-to-point communication using the send and recv methods, and collective communication using methods like bcast (broadcast), gather, and reduce.
Here is an example of point-to-point communication:
from mpi4py import MPI

comm = MPI.COMM_WORLD
rank = comm.Get_rank()

if rank == 0:
    data = {'a': 1, 'b': 2, 'c': 3}
    comm.send(data, dest=1)
else:
    data = comm.recv(source=0)
    print(f'Received data {data} at process {rank}')

In this program, the master process sends a dictionary to a specific process and that process receives the dictionary.

Note
Anything greater than rank 2 will make this program hang.

Writing Efficient MPI Code#
Efficiency and scalability are crucial when writing MPI code. Here are some tips to follow:

Overlap Computation and Communication: Whenever possible, organize your code so that computation can occur while communication is ongoing. This will reduce the waiting time for communication to complete.
Minimize Communication: Communication is often the bottleneck in parallel programs. Therefore, design your algorithms to minimize the amount of data that needs to be sent between processes.
Use Collective Operations: MPI provides collective operations like MPI_Bcast and MPI_Reduce. These operations are often optimized for the underlying hardware and should be used whenever possible.
Use Non-Blocking Operations: MPI also provides non-blocking versions of its send and receive functions. These functions (MPI_Isend and MPI_Irecv) return immediately, allowing the program to continue executing while communication is happening in the background.

Getting Help with MPI#
For assistance with getting started with using MPI or troubleshooting using MPI libraries on Discovery, reach out to us at rchelp@northeastern.edu or schedule a consultation with one of our team members.

https://rc-docs.northeastern.edu/en/latest/software/systemwide/r.html

Using R#
R is available as a modules and it is also an interactive app on Open OnDemand (OOD). You can also use R with Anaconda.



How do i use R on open ondemand ood? What are the available flavors of R in Open OnDemand?
Using R on Open OnDemand
The Open OnDemand application offers several different versions of R accessed through an interactive session with RStudio server in a Rocker Container. Each version of R is available as a different flavor whereby different packages are pre-installed. We host three flavors whose package-lists build on one another in the following order: RStudio < Tidyverse < Geospatial.
In addition to different R packages you will find additional package requirements (e.g, compilers) also present in the three flavors in increasing order.

Important
If you have tried to install a package in the RStudio or Tidyverse flavors and recieve an error message saying a necessary compiler is missing (e.g., glibc, CMAKE, zlib) or other “compilation failed” message. Please try to install the package again in the Geospatial flavor. If this still returns an error reach out to rchelp@northeastern.edu

How do I create a Packrat environment on Discovery for managing R packages? 
How can I set up a Packrat environment for R packages on Discovery?
Creating a Packrat Environment
If you work with R packages, using a Packrat environment can be a helpful way to access packages across different sessions in the Open OnDemand app, between the Open OnDemand app and the command line, or between the different R flavors. Use the procedure below to create a Packrat environment on Discovery.
After you create a new directory for your R project, you can then use Packrat to store your package dependencies inside it.
We recommend making your Packrat directory in /work (preferred) or /home

Connect to Discovery via ssh.
Type module load R/4.2.1.
Create a new directory for your R project by typing, mkdir /work/<groupname>/<username>/<directoryname> where <groupname> is your group name, <username> is your username, and <directoryname> is the name of the directory you want to create for your R project. For example, /work/coolsciencegroup/j.smith/packrat_r.
Create a new directory for your R project by typing, mkdir <directoryname>
Open the R interface and install Packrat:

   install.packages("packrat") # install in a local directory, as you cannot install as root

Initialize the environment where R packages will be written to:

   packrat::init("/work/<groupname>/<yourusername>/<directoryname>")

You can then install R packages that you need. For example, to install a package called rtracklayer, type the following:

if (!requireNamespace("BiocManager", quietly = TRUE))
install.packages("BiocManager")
BiocManager::install("rtracklayer")

How do I set up a Packrat environment in RStudio within the Open OnDemand app. what are the steps involved in creating and managing Packrat environment for R packages?
When using RStudio in the OOD App:
The instructions below can be applied on any RStudio “flavor” available (i.e., RStudio, Geospatial, and Tidyverse). Once a Packrat snapshot is created it can easily be transferred between flavors and even machines (e.g., personal laptop, Discovery).

Launch an RStudio instance on the OOD. Specify the flavor and other parameters as usual.
In the RStudio console type: install.packages("packrat").

Note
This will install by default in $HOME/R/x86_64-pc-linux-gnu-library/<version>/ as long as you don’t have previous environments or those have been turned off (see below). For Packrat installation, it is best to specify a “project folder” in your $HOME, /scratch or /work directory (if you do not have /work please see Data Storage Options). The location /tmp/Rtmp8CbQCA/downloaded_packages would not work because /tmp corresponds to the compute node that you were on while running the R session. Optimally, you would like to have the Packrat location in a persistent place so that all packages and libraries are available to you at all times regardless of the compute node you are on.

Create a Packrat project directory at the desired location by selecting “New Folder” in the “Files” tab in the lower right hand side of the RStudio screen. Alternatively, use mkdir in the terminal tab on the lower left-hand side of the RStudio screen. For example: mkdir projectfolder.
In the RStudio console, initialize the Packrat. If your current directory is the project folder (i.e., getwd() == Packrat project folder) you can omit the path here.

packrat::init("<path-to-project-folder>")

You can now record all the currently installed packages to your Packrat with the snapshot command. This may take some time if you have installed a lot of packages.

packrat::snapshot()

And now you can check on the status of your Packrat with:

packrat::status()

Now turn Packrat on. Packrat will now stay on for all your RStudio sessions and across the RStudio flavors (RStudio, geospatial, and tidyverse).

packrat::on()

You can now install packages as normal. You should see the installation location for your Packrat project folder. For example: “Installing package into /work/groupname/username/packrat_R/”

install.packages("viridis")

Packrat Tips

How can I check the status of my Packrat environment ?
At any time you can check the status of your Packrat with packrat::status().

How can I enable and disable Packrat ?
Packrat can be toggled on and off with packrat::on() and packrat::off(), respectively.

How can i disconnect packrat?
To disconnect Packrat and allow for package installation outside your packrat project folder: packrat::disable(project = NULL, restart = TRUE), where project refers to the current Packrat project folder, and restart = TRUE will restart the R session.

How can i re-initialize packrat?
To re-initialize Packrat run: packrat::init("<path-to-packrat-project-folder>"). This will automatically restart your R session.
A package can be removed from Packrat via: remove.packages("viridis), but will remain in your Packrat snapshot and can be restored with: Packrat::restore().

How can i list any unused packages in packrat?
The function packrat::clean(dry.run=T) will list any unused packages that were installed in your snapshot. You can remove them with: packrat::clean().

Note
For most cases, having a single Packrat directory is sufficient, unless you notice specific package conflicts or need different versions of the same package. A single Packrat directory also saves from having to install the same dependencies multiple times in different locations.

If the installation location is not setting to your project folder you may need to turn off these environments. In some cases, these folders could also be present in your /work/groupname/<project-name> directory.
mv ~/.rstudio ~/.rstudio-off
mv ~/.local ~/.local-off
mv ~/ondemand ~/ondemand.off
mv ~/.Rprofile ~/.Rprofile.off
mv ~/.Rhistory ~/.Rhistory.off

https://rc-docs.northeastern.edu/en/latest/software/systemwide/matlab.html

Using MATLAB



MATLAB is available on the cluster as a module and as an interactive app on Open OnDemand. You can also download MATLAB for your computer through the Northeastern portal on the MATLAB website. Note that the procedures detailed below pertain specifically to using MATLAB on the cluster, and not to using MATLAB on your computer.

How do I install a MATLAB toolbox on the cluster?  what are the steps involved in downloading, setting up, and using the toolbox within MATLAB?
Installing MATLAB toolboxes
Use the following procedure if you need to install a MATLAB toolbox:

Download the toolbox from its source website.
Connect to the cluster.
Create a directory in your /home directory. We recommend creating a directory called matlab by typing:
mkdir /home/<username>/matlab  #where <username> is your username

Go to the directory you just created by typing:
cd /home/<username>/matlab

Unzip the toolbox file by typing:
unzip <toolboxname>

Load MATLAB by typing:
module load matlab

Start MATLAB by typing:
matlab

Add the toolbox to your PATH by typing:
addpath('/home/<username>/matlab/<toolbox>') #where <toolbox> is the name of the toolbox you just unzipped

If this is a toolbox you want to use more than once, you should save it to your path by typing:
savepath()

You can now use the toolbox within MATLAB. When you are done, type quit.

How can I set up and use MATLAB Parallel Server on the cluster?
Using MATLAB Parallel Server
The cluster has MATLAB Parallel Server installed. This section details an example of how you can set up and use the MATLAB Parallel Computing Toolbox. This walkthrough uses MATLAB 2020a launched as an interactive app on the Open OnDemand web portal. There are several parts to this walkthrough. We suggest that you read it through completely before starting. The parameters presented represent only one scenario.
This walkthrough will use Open OnDemand, the web portal on the cluster, to launch MATLAB. You will then create a
cluster profile. This allows you to define cluster properties that will be applied to your jobs. Supported
functions are batch, parpool, and parcluster. The Parallel Computing Toolbox comes with a cluster profile
called local, which you will change in the walkthrough below.

Note
This walkthrough details submitting jobs through cluster’s Open OnDemand web portal. Some parameters will vary if you are using MATLAB from the command line. This walkthrough does not apply
to other versions of MATLAB.


How do I create and configure a Cluster Profile for MATLAB Parallel Server on the cluster using Open OnDemand? 

Before starting, create a folder in your /scratch/<username> directory. This folder is where you will save your job data.

Go to your /scratch directory: cd /scratch/<username> where <username> is your NU username
Make a new folder. We suggest calling it matlab-metadata: mkdir matlab-metadata

To start MATLAB and add a Cluster Profile, do the following:

Go to http://ood.discovery.neu.edu. If prompted, sign in with your cluster username and password.
Click Interactive Apps, and select MATLAB.1. Select MATLAB version 2020a, and keep the default time of one hour and default memory of 2GB. Click Launch.
If necessary, adjust the Compression and Image Quality, and then click Launch MATLAB.
On the MATLAB Home tab, in the Environment section, select Parallel, then click Create and Manage Clusters. This opens the Cluster Profile Manager window.
On the Cluster Profile Manager window, select Add Cluster Profile, then click Slurm. If prompted, click OK on the notice about needing a Parallel Server.
Double-click the new profile name in the Cluster Profile column, and type a name such as TestProfile. Press Enter to save the change.
Select Edit in the Manage Profile section. This lets you edit the options on the Properties tab. For this walkthrough, make the following edits:

In the Folder where job data is stored on the client option, type /scratch/<yourusername>/matlab-metadata (this is the directory that you created in the first procedure above).
In the Number of workers available to cluster option, type a number between between 1 and 10. This field is the number of MPI processes you intend to run. This corresponds to the --ntasks Slurm option. The maximum is 128 per job; however, for this task, we recommend keeping it lower and use threading inside the nodes. The number you set here will be the default maximum for the job. You can set it for less than or equal to this number in the MATLAB Command Window when submitting your job.
In the Number of computational threads to use on each worker option, type a number between 1 and 10. This field represents the number of threads that each worker will possess. This corresponds to cpus-per-task in Slurm. Do not exceed the number of available cores on the node.

When you have finished editing your properties, click Done.
(Optional) If you want to validate your setup, click the Validation tab (next to the Properties tab). Ensure all the stages are checked, then click the Validate button at the bottom of the page.

This will check the properties of your profile. You might need to wait a minute or two for this to complete.

Caution
Do not click the green Validate button. This will attempt validation using the maximum number of workers, which can cause the validation to hang or fail. If you accidentally click the green Validate button, click Stop to end the validation process.

(OPTIONAL) In the Cluster Profile column, right-click on the TestProfile name and select Set as Default. This sets your profile to be the default.
Now that you have set up your profile, you can use the default cluster profile you just created (TestProfile) with the following commands:
#with parpool
parallel.defaultClusterProfile(‘TestProfile’)
parpool

#with parcluster
c = parcluster(‘TestProfile’)

Using parcluster example
How can I submit batch jobs to the cluster for scaling calculations on an integer factorization problem?
This section provides instructions for submitting batch jobs to the cluster for scaling calculations on an integer factorization sample problem. The complexity of this problem increases with the magnitude of the number, making it computationally intensive. To perform these calculations, we will use the myParallelAlgorithmFcn.m MATLAB function. Please note that this section assumes you have already configured a MATLAB Cluster Profile per the procedure above.
There are benchmarking scripts and examples available in **/shared/centos7/matlab/R2020a/examples/parallel/main/** on the cluster.
To make the scripts and examples available, add the path to this folder to the list of available paths by doing one of the following:

On the MATLAB Home tab, in the Environment section, click Set Path and add the path to the script.
Alternatively, provide the script’s full path in the MATLAB command line.

The contents of myParallelAlgorithmFcn are as follows:
function [numWorkers,time] = myParallelAlgorithmFcn ()

complexities =  [2^18 2^20 2^21 2^22];
numWorkers = [1 2 4 6 16 32 64];

time = zeros(numel(numWorkers),numel(complexities));

% To obtain predictable sequences of composite numbers, fix the seed
% of the random number generator.
rng(0,'twister');

for c = 1:numel(complexities)

   primeNumbers = primes(complexities(c));
   compositeNumbers =    primeNumbers.*primeNumbers(randperm(numel(primeNumbers)));
   factors = zeros(numel(primeNumbers),2);

   for w = 1:numel(numWorkers)
       tic;
       parfor (idx = 1:numel(compositeNumbers), numWorkers(w))
          factors(idx,:) = factor(compositeNumbers(idx));
       end
       time(w,c) = toc;
   end
end

Submit Batch Job
To submit myParallelAlgorithmFcn as a batch job, in the MATLAB Command Window, type:
totalNumberOfWorkers = 65;
cluster = parcluster('TestProfile');
job = batch(cluster,'myParallelAlgorithmFcn',2,'Pool',totalNumberOfWorkers-1,'CurrentFolder','.');

This specifies the totalNumberOfWorkers as 65, where 64 workers will be used to run parfor in parallel (so the pool is set to 64), and the additional worker will run the main process.
To monitor the job after submitting it, click on Parallel, then Monitor Jobs to open the Job Monitor. Here, you can view job information, such as the job state (i.e., running, failed, finished, etc.), and fetch outputs by right-clicking on the job line.
You can close MATLAB after submitting the job to the scheduler. The job monitor tool will continue to track the jobs.
If you want to block MATLAB until the jobs are finished, type Wait(job).
Once the jobs are complete, you can transfer the function outputs using the fetchOutputs command.
outputs = fetchOutputs(job);
numWorkers = outputs{1};
time = outputs{2};

You can plot the performance (speedup) by typing:
figure
speedup = time(1,:)./time;
plot(numWorkers,speedup);
legend('Problem complexity 1','Problem complexity 2','Problem complexity 3','Problem complexity 4','Location','northwest');
title('Speedup vs complexity');
xlabel('Number of workers');
xticks(numWorkers(2:end));
ylabel('Speedup');



https://rc-docs.northeastern.edu/en/latest/software/packagemanagers/conda.html



Conda
What is Conda? What is  Miniconda? What is  Anaconda?
Conda is an open-source environment and package manager. Miniconda is a free installer for Conda and Python and comes with a few other packages. Anaconda is also a package manager that has a much larger number of packages pre-installed.

Managing Conda Environments#

Creating Environments

Note
We recommend avoiding building Conda environments in your /home, for its space quota. Instead, Use /work, which can be requested by PIs for groups in need of space /work.



Installing local virtual environment using Conda is recommended on the cluster. You can have multiple environments with different packages for each, which allows project’s environments to be independent of others. You only have to load the anaconda3 module.
From the login node, log-in to a compute node.

Request one node on the short partition with 1 CPU core. Then, load the anaconda3/2022.05 module.#
1srun --partition=short --nodes=1 --cpus-per-task=1 --pty /bin/bash
2module load anaconda3/2022.05

To create a new Conda environment where <environment-name> is the path and name. You can see a list of your existing environments with conda env list.
conda create --prefix=/<path>/<environment-name> python=3.11 anaconda

Follow the prompts to complete the Conda install, then activate the environment.
source activate /<path>/<environment-name>

Your command line prompt will then include the path and name of environment.
(/<path>/<environment-name>) [<username>@c2001 dirname]$

Tip
The conda config --set env_prompt '({name}) ' command modifies your .condarc to show only the environment, which displays as follows:
(<environment-name>) [<username>@c2000 dirname]$

With your Conda environment activated you can install a specific package with
conda install [packagename]

To deactivate the current active Conda environment
conda deactivate

To delete a Conda environment and all of its related packages, run:
conda remove -n yourenvironmentname --all

Listing Environments#
You can view the environments you’ve created in your home directory by using the following command
conda env list

# conda environments:
#
MlGenomics               $HOME/.conda/envs/MlGenomics
base                     $HOME/miniconda3

To list the software packages within a specific environment, use
conda list --name env_name

If you’ve created an environment in a different location, you can still list its packages using:
conda list --prefix /path/to/env

Exporting Environment#
For ensuring reproducibility, it’s recommended to export a list of all packages and versions in an environment to an environment file. This file can then be used to recreate the exact environment on another system or by another user. It also serves as a record of the software environment used for your analysis.

Removing Environments#
When you need to remove an environment located in your home directory, execute:
conda env remove --name env_name

For environments located elsewhere, you can remove them using:
rm -rf /path/to/env

Clean Conda Environment#
To remove packages that are no longer used by any environment and any downloaded tarballs stored in the conda package cache, run:
conda clean --all

By following these guidelines, you can efficiently manage your Conda environments and packages, ensuring reproducibility and a clean system.

Using Miniconda#
This procedure assumes that you have not installed Miniconda. If you need to update Miniconda, do not follow the installation procedure. Use conda update. This procedure uses the Miniconda3 version with Python version 3.8 in step 2, although there are other versions you can install (e.g., 3.9 or 3.11).

Installing Miniconda#

Attention
Make sure to log on to a compute node.
srun --partition=short --nodes=1 --cpus-per-task=1 --pty /bin/bash

Download Miniconda, check the hash key, and install as follows:
wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
sha256sum Miniconda3-latest-Linux-x86_64.sh
bash Miniconda3-latest-Linux-x86_64.sh -b -p <dir>

Where <dir> is the full path to your desired installation directory (e.g., /work/mygroup/miniconda3).
Activate the base Miniconda environment
source <dir>/bin/activate

You can now create a new environment with this command where we are using python version 3.8:
conda create --name my-py38env python=3.8

Type y if asked to proceed with the installation.
Now you can activate your new environment
conda activate my-py38env

To deactivate the environment, type conda deactivate. You can type this command again to deactivate the base Miniconda environment.

Conda Best Practices#



Your ~/.conda may get very large if you install multiple packages and create many virtual Conda environments. Make sure to clean the Conda cache and clean unused packages with: conda clean --all.
Clean unused Conda environments by first listing the environments with: conda env list , and then removing unused ones: conda env remove --name <environment-name>.
You can build Conda environments in different locations to save space on your home directory (see Data Storage Options). You can use the --prefix flag when building your environment. For example: conda create myenv --prefix=/work/<mygroup>/<mydirectory>.
Another recommended step is to update your Conda version (possible only when using Miniconda): conda update conda -y

https://rc-docs.northeastern.edu/en/latest/software/packagemanagers/spack.html

Spack#
Research Computing recommends using Spack to conveniently install software packages locally to your path. Please refer to the Spack documentation for the latest information about the packages that Spack contains. To use Spack, you first need to copy it to your /home directory or a /work directory, then you need to add it to your local environment.

Note
Spack software installations are part of your research and should preferably be stored in your PI’s /work directory.

Install Spack
These instructions will demonstrate how to install Spack in your /home (non-shared) or /work (shared) directory and then how to add Spack to your local environment while on a compute node, so you have access to the Spack commands (steps 4-5).

Non-shared
Copy Spack’s Git repository to ‘$HOME’
git clone -c feature.manyFiles=true https://github.com/spack/spack.git

Shared
Copy Spack’s Git repository to /work and modify directory permissions to give write access to the members of your PI’s /work.
cd /work/<PI-Project-Dir>
git clone -c feature.manyFiles=true https://github.com/spack/spack.git
chmod -R 775 spack/

how do i Install a software using Spack?
Install a software using Spack

Request a compute node interactively: srun -p short --pty -N 1 -n 28 /bin/bash. While building the software Spack will attempt to run make in parallel. Hence, you need to request a compute node with multiple cores. This srun request is for 28 cores on one node (-N 1 -n 28).
Any module that is required for your software installation needs to be in your $PATH prior to adding Spack to your local environment. For example, to use a newer version of python for compatibility with Spack, type: module load python/3.8.1.
Add Spack to your local environment, so you can use the Spack commands. If Spack has been installed on $HOME:
For Spack on $HOME
export SPACK_ROOT=/home/<yourusername>/spack
. $SPACK_ROOT/share/spack/setup-env.sh

For Spack on /work/<PI-Project-Dir>
export SPACK_ROOT=/work/<PI-Project-Dir>/spack
. $SPACK_ROOT/share/spack/setup-env.sh

After you have the Spack commands in your environment, type spack help to ensure Spack is loaded in your environment and to see the commands you can use with Spack. You can also type spack list to see all the software that you can install with Spack, but note this command can take a few moments to populate the list.
To check your spack version: spack --version .
To see information about a specific software package, including options and dependencies: spack info <software name>. Make sure to note the options and/or dependencies that you want to add or not add before installing the software.
To install a software package plus any dependencies or options:
spack install <software name> +<any dependencies or options>;
you can specify -<any dependencies or options>. You can also list
+ or - different options and dependencies within the same line. Do
not put a space between each option/dependency that you list.
To view information about your installed software packages: spack find <software package name> or spack info <software package name> .
To Install a specific version of the software: spack install <softwarename@version>.

When you have installed a software package, you can add it to the module system by executing this command:
. $SPACK_ROOT/share/spack/setup-env.sh

Example: Installing LAMMPS#
This section details how to install the LAMMPS application with the
KOKKOS and User-reaxc packages using Spack. This example assumes that
you do not have any previous versions of LAMMPS installed. If you have
any previous versions of LAMMPS, you must uninstall them before using
this procedure. To see if you have any previous versions of LAMMPS,
type spack find lammps. If you do have a previous version, you will
need to uninstall LAMMPS by typing spack uninstall --dependents lammps. Then, you can follow the instructions below. Note that the
installation can take about two hours to complete. As part of the
procedure, we recommend that you initiate a tmux session so that
you can have the installation running as a separate process if you
need to do other work on Discovery. If you decide to use tmux, make
note of the compute node number (compute node numbers start with c or
d with four numbers, such as c0123) to make it easier to check on the
progress of the installation.
If LAMMPS has a dependency on a specific gcc compiler, then do the following before starting the installation procedure. This will update the compilers.yaml file located in $HOME/.spack/linux.

cd $HOME/.spack/linux/
Open compilers.yaml and copy-paste a compiler entry at the end of the file.
Edit ‘spec’ and ‘path’ to indicate the version of the GCC compiler that is required for installation.
For example:
     spec: gcc@=8.1.0
 	paths:
 	  cc: /shared/centos7/gcc/8.1.0/bin/gcc
  	  cxx: /shared/centos7/gcc/8.1.0/bin/g++
 	  f77: /shared/centos7/gcc/8.1.0/bin/gfortran
   	  fc: /shared/centos7/gcc/8.1.0/bin/gfortran

The compilers.yaml file should now have the desired gcc version as its latest compiler entry.
Assuming that Spack has already been installed at a desired location. For installing gpu-supported LAMMPS, request a GPU node for 8 hours:
srun --partition=gpu --nodes=1 --ntasks=14 --pty --gres=gpu:1 --mem=16GB --time=08:00:00 /bin/bash

Load compatible CUDA, GCC, and Python modules and activate Spack from the installed location.
 module load cuda/10.2 gcc/8.1.0 python/3.8.1
 export SPACK_ROOT=/work/<PI-Project-Dir>/spack
 . $SPACK_ROOT/share/spack/setup-env.sh

(Optional) Initiate a tmux session:

Start a tmux session: tmux.
List tmux sessions: tmux ls
Detach from tmux session: Ctrl+b d
Attach to tmux session: tmux attach-session -t 0
Exit a tmux session: Ctrl+d

Type:
spack install lammps +asphere +body +class2 +colloid +compress +coreshell +cuda \
cuda_arch=70 +cuda_mps +dipole +granular +kokkos +kspace +manybody +mc +misc +molecule \
+mpiio +peri +python +qeq +replica +rigid +shock +snap +spin +srd +user-reaxc +user-misc

Type spack find LAMMPS to view your installed software package.
Type spack load lammps.

https://rc-docs.northeastern.edu/en/latest/software/packagemanagers/../fromsource/index.html

From Source#

System-Wide Software#

Make

CMake

Installing Your Own Module#
In addition to the pre-installed software on our HPC cluster, users may need to install their software. This section outlines the general steps for doing so.

Check If the Software Is Already Installed

Before installing new software, check if it is already installed by using the module avail command:
module avail

This command lists all the available modules (i.e., software) on the system.

Choose the Appropriate Location for Installation

Users should install their software in their home directory or a designated project directory to avoid permission issues. For example:
cd /home/<username>/software

Replace <username> with your username.

Download the Software

Download the software package using wget or curl, for example:
wget http://example.com/software.tar.gz

Ensure to replace http://example.com/software.tar.gz with the actual URL of the software package.

Extract the Software

Most software packages are distributed as compressed files that must be extracted.
tar -xvzf software.tar.gz

Replace software.tar.gz with the actual filename of the software package.

Install the Software

Installation steps can vary widely between different programs. Many use the configure, make, make install steps, like this:
cd software_directory   # Replace with actual directory
./configure --prefix=/home/<username>/software
make
make install

Replace <username> with your username and software_directory with the actual directory of the software.

Note
Always read the software’s installation instructions, as steps may vary.

Test the Software

After installation, it is important to test the software to ensure it works correctly. Most software packages include a set of test cases for this purpose. Refer to the software’s documentation for how to run these tests.

A workflow of software installation (replace with our own).#

Make the Software Available via Modules

For ease of use, consider creating a module file for the software. Here is an example of a basic module file for software installed in /home/<username>/software:
#%Module
set root /home/<username>/software
prepend-path PATH $root/bin
prepend-path LD_LIBRARY_PATH $root/lib

Replace <username> with your username.

Common Software Installation Commands#

Command
Description

module avail
List all available modules

cd
Change the current directory

wget or curl
Download files

tar
Extract files from a tarball

./configure
Configure the software for the system

make
Compile the software

make install
Install the software

Following these steps, users should be able to install most software packages on their own. However, always refer to the specific software’s documentation, as steps can vary. If you run into issues or need help, please contact support.

https://rc-docs.northeastern.edu/en/latest/software/fromsource/makefile.html

Make#

Important
Be sure to refer to the installation instructions provided with software being installed. If the software requires additional dependencies not installed on the system, they might need to installed and added to your PATH similarly.

If you want to use make to add software locally to your path, you must first download the software package from its source (e.g., its webpage or GitHub) and unpack it or unzip it if need be. Then, you must set the installation path to a directory with write access on the cluster, such as your home directory or your /work.

Note
You can use ./configure to specify the installation path (e.g., ./configure --prefix=${HOME}/software).

After setting the installation path, compile the code via make and then install the software using make install.

Makefile Example: Installing FFTW Library#
Even without root access, you can install software system-wide by installing it in your home directory. Let us continue with the FFTW library as an example.

Download the FFTW tarball. Here, we download version 3.3.9:
cd ~
wget http://www.fftw.org/fftw-3.3.9.tar.gz

Extract the tarball:
tar xzf fftw-3.3.9.tar.gz

Move into the directory:
cd fftw-3.3.9

Configure the build process, specifying the prefix as a location in your home directory. This location is where the library will be installed. Note that the specified directory should be in your PATH to ensure system-wide accessibility.
./configure --prefix=$HOME/fftw

Compile the software:
make -j 8

Instead of the default make install, which typically requires root access for system-wide installation, you can make install with the prefix configuration to install the software in your home directory.
make install

The FFTW library should now be installed in the fftw directory in your home directory.

Important
Include the location of the software in your PATH to access directly. This can be done by adding the following line to your ~/.bashrc:
export PATH=$HOME/fftw/bin:$PATH

This puts the program in your path so that the system can find the FFTW library binaries when called.

Note
You need to source your profile or restart your shell for these changes to take effect, which is done as follows:
source ~/.bashrc

Makefile Example: Installing LAMMPS#



Note
There are no configure options used and the information is stored within the makefiles mylammps/MAKE in the make.serial and make.mpi files.

The following instructions to build LAMMPS using make.
1. To allocate an interactive job on compute node type:
   ::::{code-block} bash
   srun -N 1 -n 28 --constraint=ib --pty /bin/bash

Load the following modules required for building LAMMPS:
module load openmpi/4.0.5
module load python/3.6.6
module load gcc/9.2.0

Change the directory to the src directory using the command cd /path/to/mylammps/src
Use the following command to build serial version or the MPI version of LAMMPS depending on the requirement. This will generate lmp_serial binary for a serial build and lmp_mpi for an MPI build.
make serial
make mpi

Now you can start running the program, using ./lmp_serial or mpirun -n 1 ./lmp_mpi -h.

https://rc-docs.northeastern.edu/en/latest/software/fromsource/cmake.html

CMake#

CMake Example: Serial LAMMPS Build#

Note
This is a minimal example using the command line version of CMake to build LAMMPS with no add-on packages enabled and no customization.

To allocate an interactive job on compute node type:
srun -p short --pty -N 1 -n 28 /bin/bash

For LAMMPS, you will need to create and use the build directory with the following command:
mkdir build
cd build/

Load the CMake module and use CMake to generate a build environment in a new directory.
module load cmake/3.23.2
cmake ../cmake

Next, you will work on the compilation and linking of all objects, libraries, and executables using the selected build tool.
cmake --build .
make install

The cmake --build command will launch the compilation, which, if successful, will ultimately generate an executable lmpinside thebuildfolder. Now you can start running LAMMPS using ./lmp.

CMake Example: Parallel LAMMPS Build#



The following instructions to build LAMMPS using cmake.

Running LAMMPS in parallel is possible using a domain decomposition parallelization. In order to build the MPI version, first allocate an interactive job on compute node by typing:
srun -N 1 -n 28 --constraint=ib --pty /bin/bash

Load the required modules required for building LAMMPS:
module load openmpi/4.0.5
module load cmake/3.23.2

For LAMMPS, you will need to create and use the build directory with the following command:
mkdir build
cd build/

In the build directory, run the following commands with DBUILD_MPI=yes  to build the MPI version :
cmake ../cmake -DBUILD_MPI=yes
cmake --build .
make install

The instructions above will create a lmp inside the build directory.

Note
When compiled with MPI, the binaries expect mpirun or mpiexec with the number of tasks to run.

Now, you can start running LAMMPS using mpirun -n 1 ./lmp -h.



https://rc-docs.northeastern.edu/en/latest/slurmguide/introductiontoslurm.html

Introduction to Slurm
What is the primary purpose of using Slurm in High-Performance Computing environments?
Slurm (Simple Linux Utility for Resource Management) is an open-source, highly configurable, fault-tolerant, and adaptable workload manager. It is extensively used across High-Performance Computing (HPC) environments.
Slurm is designed to accommodate the complex needs of large-scale computational workloads. It can efficiently distribute and manage tasks across clusters comprising thousands of nodes, offering seamless control over resources, scheduling, and job queuing.
It is the software on the HPC that provides functionalities such as Slurm Array Jobs and Dependencies, Job Management, view Account information, and check the Cluster and Node States: sinfo.

How does Slurm manage the execution and scheduling of complex workflows in HPC systems?
Slurm on HPC
HPC systems are designed to perform complex, computationally intensive tasks. For example, users can specify complex workflows of jobs where specific jobs depend on others, and Slurm will manage the scheduling and execution of these workflows. Efficiently managing these tasks and resources in such an environment is a daunting challenge. That is where Slurm comes into play.
Slurm allows users to submit their computational tasks as jobs to be scheduled on the cluster’s compute nodes. Its role-based access control ensures proper resource allocation and job execution, preventing resource conflicts.
Slurm is crucial in research environments, where it ensures fair usage of resources among a multitude of users, helps optimize the workload for the available resources, and provides precise job accounting and statistics.

Section Objective:
To understand the Slurm workload manage, which will allow you to properly leverage the HPC. It starts with the basics - the resources that Slurm manager. Then, useful Slurm features (e.g., job submission, monitoring, canceling, etc.) are mentioned with code examples. We discuss jobs that are both interactive (i.e., Interactive Jobs: srun Command) and batch (i.e., Batch Jobs: sbatch), along with the slurm array variants (i.e., Slurm Array Jobs and Dependencies).



Who Should Use This Guide?
This guide is for HPC users: researchers intending to use Slurm-based clusters for their computation tasks, system administrators managing HPC environments, and even seasoned HPC users looking to brush up on their knowledge. It progresses from fundamental to advanced topics, making it a valuable resource for a broad audience.

Slurm: Basic Concepts#
Before we delve into using Slurm, it is essential to grasp some fundamental concepts related to its operation.

What are nodes on slurm? 
Nodes
In the context of Slurm, a ‘node’ refers to a server within the HPC cluster. Each node possesses its resources, such as CPUs, memory, storage, and potentially GPUs. Slurm manages these nodes and allocates resources to the tasks.

What are partitions on slurm? What factors typically influence how partitions are defined in an HPC system?
Partition(s)
A ‘partition’ is a grouping of nodes. You can think of partitions as virtual clusters within your HPC system. They allow administrators to segregate the compute environment based on factors like job sizes, hardware type, or resource allocation policies.




Account information
When running a job with either srun or sbatch, if you have more than one account associated with your username, we recommend you use the --account= flag and specify the account that corresponds to the respective project.
To find out what account(s) your username is associated with, use the following command:
sacctmgr show associations user=<yourusername>

After you have determined what accounts your username is associated with, if you have more than one account association, you can use the account= flag with your usual srun or sbatch commands.

Jobs, Job Steps, and Job Arrays#
A job in Slurm is a user-defined computational task that is submitted to the cluster for execution. Each job has one or more job steps, sub-tasks that are part of a larger job and can be executed in parallel or sequentially.
Job arrays are a series of similar jobs that differ only by the array index. They are especially useful when you want to execute the same application with different inputs.

Tasks#
Tasks are the individual processes that run within a job step. They could be single-threaded or multi-threaded and can run on one or more nodes.

Slurm References#

SchedMD. (2023). Slurm Workload Manager. https://slurm.schedmd.com
SchedMD. (2023). Slurm Quick Start User Guide. https://slurm.schedmd.com/quickstart.html
IBM. (2023). High Performance Computing For Dummies, IBM Limited Edition. https://www.ibm.com/downloads/cas/WQDZWBYJ
Thank you for following along with this guide, and we wish you success in your HPC journey!

https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmcommands.html

Slurm Commands#

### Slurm Commands

Slurm is a widely used workload manager, designed for the efficient management of jobs on large-scale compute clusters. Here's a reference to various commands and options within Slurm:

**Job Submission:**

- `salloc`: This is used to obtain a job allocation.
- `sbatch`: Allows users to submit a batch script for execution at a later time.
- `srun`: This command both obtains a job allocation (when required) and executes an application.
  
With these commands, there are several options and parameters:
- Memory requirements: `--mem=<MB>` for memory required per node and `--mem-per-cpu=<MB>` for memory per allocated CPU.
- Node specifics: `-N<minnodes[-maxnodes]>` to determine node count for the job, `-n<count>` to set the number of tasks to launch, and `--nodelist=<names>` to include specific host names in job allocation.
- Output control: `--output=<name>` specifies the file in which to store job output.
- Job settings: `--partition=<names>` to choose the partition or queue in which to run the job, and `--qos=<name>` for the quality of service.
- Timing and command wrapping: `--time=<time>` to set a wall clock time limit, and `--wrap=<command_string>` (specifically for sbatch) to wrap the specified command in a simple "sh" shell.

**Accounting:**

- `sacctmgr`: Helps in viewing and modifying account information. 

Some of the options associated with this command include: 
  - `--array=<indexes>`: Job array specification for the sbatch command.
  - Job settings: `--account=<name>`, `--begin=<time>`, `--clusters=<name>`, `--constraint=<features>`, `--cpus-per-task=<count>`, among others.

**Job Management:**

- `sbcast`: Transfers a file to a job's compute nodes using the syntax `sbcast [options] SOURCE DESTINATION`. 
- `scancel`: Sends signals to jobs, job arrays, and/or job steps. 

Various options include `--allusers`, `--endtime=<time>`, `--format=<spec>`, and others.

**Viewing Job and Node Information:**

- `squeue`: Offers a view of information about jobs. 
- `sinfo`: Provides information about nodes and partitions.

**Configuration and State Management:**

- `scontrol`: This is utilized to view and modify configuration and state.

Several environment variables like `SLURM-CLUSTER-NAME`, `SLURM-JOB-ID`, and `SLURM-JOB-NAME` provide crucial information about job execution.

**Daemons:**

- `slurmctld`: Executes on the cluster's "head" node to manage workload.
- `slurmd`: Operates on each compute node to manage resources locally.
- `slurmdbd`: Manages a database of resources limits, licenses, and archives accounting records.

Lastly, `sview` offers a graphical user interface version for viewing Slurm details.

*Note: Copyright 2017 SchedMD LLC. All rights reserved. More details can be found at [SchedMD's website](http://www.schedmd.com).*



https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmrunningjobs.html

Slurm Running Jobs
You have two options when running tasks, run interactively via Interactive Jobs: srun Command or by batch via Batch Jobs: sbatch.
For parallel tasks, one can treat each task as a separate job and run them independently. The other option is to allocate resources for all the jobs simultaneously, allowing them to overlap (share CPUs, RAM, etc.). This is done with the --overlap flag: the assumption must be that not all tasks require all resources simultaneously, creating a more natural working environment, and resources are not wasted on idle time.

Note
While the sbatch and srun commands request resource allocation if none exists, using salloc allows us to separate the allocation and submission processes.

Some things Slurm assumes, like there is no overlap between different CPUs by default: tasks do not share CPUs with others running parallel. If overlap is needed, use the following Slurm flag:
--overlap

Also, set the environment variable SLURM_OVERLAP=1 via
export SLURM_OVERLAP=1

Important
Run export SLURM_OVERLAP=1 prior to logging onto a compute node when using MPI interactively.

What is the purpose of using the `srun` command in Slurm? how can you start an interactive job session ?
Interactive Jobs: srun Command
The srun command is used to submit an interactive job which runs a single task directly via the shell. This method is useful when you want to test a short computation or run an interactive session like a shell, Python, or an R terminal.

Syntax: srun#
srun [options] [command]

Options and Usage: srun#

n, --ntasks=<number>: specify the number of tasks
N, --nodes=<minnodes[-maxnodes]>: specify the number of nodes
J, --job-name=<jobname>: specify a name for the job

srun -N 1 -n 1 --pty bash

This command starts an interactive bash shell on one node with one task.

Examples using srun#
The user needs to review the Hardware Overview and Partitions to be familiar with the available hardware and partition limits on Discovery. This way, user can tailor the request to fit both the needs of the job and the limits of partitions. For example, if the user specifies --partition=short and --time=01:00:00, it will result in an error because the time specified exceeds the limit for that partition.


How can you  move to a compute node after logging into an HPC system?
This simple srun example is to move to a compute node after you first log into the HPC:
srun --pty /bin/bash

How do you request a compute node with specific resources using srun? 
To request one node and one task for 30 minutes with X11 forwarding on the short partition, type:
srun --partition=short --nodes=1 --ntasks=1 --x11 --mem=10G --time=00:30:00 --pty /bin/bash

To request one node, with 10 tasks and 2 CPUs per task (a total of 20 CPUs), 1 GB of memory, for one hour on the express partition, type:
srun --partition=short --nodes 1 --ntasks 10 --cpus-per-task 2 --pty --mem=1G --time=01:00:00 /bin/bash

To request two nodes, each with 10 tasks per node and 2 CPUs per task (a total of 40 CPUs), 1 GB of memory, for one hour on the express partition, type:
srun --partition=short --nodes=2 --ntasks 10 --cpus-per-task 2 --pty --mem=1G --time=01:00:00 /bin/bash

To allocate a GPU node, you should specify the gpu partition and use the –gres option:
srun --partition=gpu --nodes=1 --ntasks=1 --gres=gpu:1 --mem=1Gb --time=01:00:00 --pty /bin/bash


How do I submit a batch job? give example of using the `sbatch` command in Slurm. what are some examples of SBATCH directives for a single-node job? How do i submit a job to run later?
Batch Jobs: sbatch
The sbatch command is used to submit a job script for later execution. The script includes the SBATCH directives that control the job parameters like the number of nodes, CPUs per task, job name, etc.

Syntax: sbatch#
sbatch [options]  <script_file>

Options and Usage: sbatch#

n, --ntasks= <number> : specify the number of tasks
N, --nodes=<minnodes[-maxnodes]> : specify the number of nodes
J, --job-name=<jobname> : specify a name for the job

#!/bin/bash
#SBATCH -J MyJob               # Job name
#SBATCH -N 2                   # Number of nodes
#SBATCH -n 16                  # Number of tasks
#SBATCH -o output_%j.txt       # Standard output file
#SBATCH -e error_%j.txt        # Standard error file

# Your program/command here
srun ./my_program

To submit this job script, save it as my_job.sh and run:
sbatch my_job.sh

Examples using sbatch#

Single node#
Run a job on one node for four hours on the short partition:
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --time=4:00:00
#SBATCH --job-name=MyJobName
#SBATCH --partition=short

# <commands to execute>

https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmmonitoringandmanaging.html

Monitoring and Managing Jobs
The monitoring and management section aims to give you a solid understanding of how to manage and control your jobs effectively. Always ensure to monitor your jobs regularly and adjust parameters as needed to achieve the best performance.

How can you  view information about all partitions in a cluster?
Cluster and Node States: sinfo
Below are some more examples of using sinfo and scontrol to provide information about the state of the cluster and specific nodes.

Using sinfo#
The sinfo command will show information about all partitions in the cluster, including the partition name, available nodes, and status. By default, sinfo reports:
| PARTITION | The list of the cluster’s partitions; a set of compute nodes grouped logically |
| --- | --- |
| AVAIL | The active state of the partition (up, down, idle) |
| TIMELIMIT | The maximum job execution wall-time per partition |
| NODES | The total number of nodes per partition |
| STATE | See STATE table below |
| NODELIST(REASON) | The list of nodes per partition |

Examples using sinfo#
View information about all partitions:
sinfo -a

Or, a specific partition, which gives all the nodes and the states the nodes are in at the current time:
sinfo -p gpu


How do you use the `sinfo` command in Slurm to view information about a specific (e.g., short, gpu, long) partition?
View information about a specific partition (e.g., short, gpu, long):
sinfo -p <partition_name>

Or, only view nodes in a certain state:
sinfo -p <partition> -t <state>

You can use the --Format flag to get more information or a specific format for the output:
sinfo -p <partition> -t idle --Format=gres,nodes

Below command will show detailed information about all nodes in the cluster, including the node name, state, CPU architecture, memory, and available features:
sinfo -N -l

View what features a node has:
sinfo -n <node> --Format=nodes,nodelist,statecompact,features

View what nodes are in what state in a partition using statecompact:
sinfo -p <partition> --Format=time,nodes,statecompact,features,memory,cpus,nodelist

The scontrol command is a command-line utility that allows users to view and control Slurm jobs and job-related resources. It provides a way to check the status of jobs, modify job properties, and perform other job-related tasks.
You can monitor your jobs by using the Slurm scontrol command. Type scontrol show jobid -d <JOBID>, where JOBID is the number of your job. In the figure at the top of the page, you can see that when you submit your srun command, Slurm displays the unique ID number of your job (job 12962519). This is the number you use with scontrol to monitor your job.

Using scontrol#
Some of the tasks that can done using scontrol include:

How can i View job status and properties
Viewing job status and properties: scontrol can display detailed information about a job, including its status, node allocation, and other properties.
How can i Modify job properties
Modifying job properties: scontrol allows users to modify job properties such as the job name, the number of nodes, the time limit, and other parameters.
How can i Manage job dependencies
Managing job dependencies: scontrol provides a way to specify job dependencies and view the dependencies between jobs.
How can i Suspend and resume jobs
Suspending and resuming jobs: scontrol can stop and resume running jobs, allowing users to temporarily halt jobs or continue them as needed.
How can i Cancel jobs
Canceling jobs: scontrol can cancel jobs that are running or queued, allowing users to stop jobs that are no longer needed.

Overall, scontrol is a powerful tool for managing Slurm jobs and job-related resources. Its command-line interface allows users to perform a wide range of tasks, from checking the status of jobs to modifying job properties and managing dependencies.

Controlling jobs: scontrol#
Place a hold on a pending job, i.e., prevent specified job from starting. <job_list> is either a space separate list of job IDs or job names.
scontrol hold <jobid>

Release a held job, i.e., permit specified job to start (see hold).
scontrol release <jobid>

Re-queue a completed, failed, or canceled job
scontrol requeue <jobid>

For more information on the commands listed above, along with a complete list of scontrol commands run below:
scontrol --help

Syntax: scontrol#
scontrol [command] [options]

Example: scontrol#
scontrol show jobid -d <JOBID>

Options and Usage: scontrol#

update: used to modify job or system configuration
hold jobid=<job_id>: hold a specific job
release jobid=<job_id>: release a specific job
requeue jobid=<job_id>: requeue a specific job

Examples using scontrol#
View information about a specific node:
scontrol show node -d <node_name>

For information on all reservations, this command will show information about a specific node in the cluster, including the node name, state, number of CPUs, and amount of memory:
scontrol show reservations

View information about a specific job. This command will show information about a specific job, including the job ID, state, username, and partition name:
scontrol show job <job_id>

To view information about a specific reservation (e.g., found via scontrol show res listed above), and print information about a specific reservation in the cluster, including the reservation name, start time, end time, and nodes included in the reservation:
scontrol show reservation <reservation_name>

Job Management#
Managing jobs in a Slurm-based HPC environment involves monitoring running jobs, modifying job parameters, and canceling jobs when necessary. This section will cover the commands and techniques you can use for these tasks.

Monitoring Jobs#
The squeue command allows you to monitor the state of jobs in the queue. It provides information such as the job ID, the partition it is running on, the job name, and more.

Syntax: squeue#
squeue [options]

Options and Usage#

j, --jobs=<job_id>: display information about specific job(s)
u, --user=<user_name>: display jobs for a specific user
l, --long: display more information (long format)

Code Example of Job Monitoring#
To monitor all jobs of a specific user, use the following command:
squeue -u <username>

To monitor a specific job, use:
squeue -j <job_id>

Canceling Jobs: scancel#
The scancel command is used to cancel a running or pending job. Once canceled, a job cannot be resumed.

Syntax: scancel#
scancel [options] [job_id]

Options and Usage: scancel#

u, --user=<user_name>: cancel all jobs of a specific user
-name=<job_name>: cancel all jobs with a specific name

Examples using scancel#
To cancel a specific job, use:
scancel <job_id>

To cancel all jobs of a specific user:
scancel -u <username>

To cancel all jobs with a specific name:
scancel --name=<job_name>

https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmscripts.html

Slurm Job Scripts#

sbatch Job Script Examples

How can job arrays be used in Slurm to efficiently manage and run large numbers of similar jobs, and what are their advantages?

Single node with additional memory
The default memory per allocated core is 1.95GB. If calculations attempt to access more memory than allocated, Slurm automatically terminates the job. Request a specific amount of memory in the job script if calculations require more than the default. The example script below requests 100GB of memory (--mem=100G). Use one capital letter to abbreviate the unit of memory (i.e., kilo K, mega M, giga G, and tera T) with the --mem= option, as that is what Slurm expects to see:
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --time=4:00:00
#SBATCH --job-name=MyJobName
#SBATCH --mem=100G
#SBATCH --partition=short

# <commands to execute>

Single with exclusive use of a node#
If you need exclusive use of a node, such as when you have a job that has high I/O requirements, you can use the exclusive flag. The example script below specifies the exclusive use of one node in the short partition for four hours:
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --time=4:00:00
#SBATCH --job-name=MyJobName
#SBATCH --exclusive
#SBATCH --partition=short

# <commands to execute>

https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmarray.html

Slurm Array Jobs and Dependencies#
<> (job-arrays)=

Slurm Job Arrays#
Job arrays are a convenient way to submit and manage large numbers of similar jobs quickly. They can process millions of tasks in milliseconds, provided they are within size limits. Job arrays are particularly useful when running similar jobs, such as performing the same analysis with different inputs or parameters.
Using job arrays can save time and reduce the amount of manual work required. Instead of submitting each job individually, you can submit a single job array and let Slurm handle the scheduling of individual jobs. This approach is beneficial if you have limited time or resources, as it allows you to use the cluster’s computing power more efficiently by running multiple jobs in parallel.
There are several ways to define job arrays, such as specifying the range of indices or providing a list of indices in a file. Slurm also offers various features to manage and track job arrays, such as options to simultaneously suspend, resume, or cancel all jobs in the array.

Syntax: Job Arrays#
The most basic configuration for a job array is as follows:
#!/bin/bash
#SBATCH --partition=short
#SBATCH --job-name=jarray-example
#SBATCH --output=out/array_%A_%a.out
#SBATCH --error=err/array_%A_%a.err
#SBATCH --array=1-6

This command runs the same script six times using Slurm job arrays. Each job array has two additional environment variable sets. SLURM_ARRAY_JOB_ID (%A) is set to the first job ID of the array, and SLURM_ARRAY_TASK_ID (%a) is set to the job array index value.

Note
Both the SLURM_ARRAY_JOB_ID (%A) and SLURM_ARRAY_TASK_ID (%a) are referenced when naming outputs so file do not overwrite when a “task” (i.e., one of the executions of the script through the job array) finishes.

Tip
Generally, we want to pass the former as an argument for our script. If you are using R, you can retrieve the former using task_id <- Sys.getenv("SLURM_ARRAY_TASK_ID"). If you are using job arrays with Python, you can obtain the task ID using the following:
import sys
taskId = sys.getenv('SLURM_ARRAY_TASK_ID')

When submitting an array and setting its size with many dimensions, please use the % symbol to indicate how many tasks run simultaneously. For example, the following code specifies an array of 600 jobs, with 20 running at a time:
#!/bin/bash
#SBATCH --partition=short
#SBATCH --job-name=jarray-example
#SBATCH --output=out/array_%A_%a.out
#SBATCH --error=err/array_%A_%a.err
#SBATCH --array=1-600%20

Whenever you specify the memory, number of nodes, number of CPUs, or other specifications, they will be applied to each task. Therefore, if we set the header of our submission file as follows:
#!/bin/bash
#SBATCH --partition=short
#SBATCH --job-name=jarray-example
#SBATCH --output=out/array_%A_%a.out
#SBATCH --error=err/array_%A_%a.err
#SBATCH --array=1-600%20
#SBATCH --mem=128G
#SBATCH --nodes=2

Slurm will submit 20 jobs simultaneously. Each job, represented by a task ID, will use two nodes with 128GB of RAM each. In most cases, setting up a single task is sufficient.
Lastly, we usually use job arrays for embarrassingly parallel jobs. If your case is such that the job executed at each job ID does not use any multi-threading libraries, you can use the following header to avoid wasting resources:
#!/bin/bash
#SBATCH --partition=short
#SBATCH --job-name=jarray-example
#SBATCH --output=out/array_%A_%a.out
#SBATCH --error=err/array_%A_%a.err
#SBATCH --array=1-600%50  # 50 is the maximum number
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=1
#SBATCH --mem-per-cpu=2G ## debug prior to know how much RAM

Warning
50 is the maximum number of jobs allowed to be run at once per user-account.

The above examples apply for interactive mode, as well. For instance:
sbatch --array=<indexes> [options] script_file

Indexes can be listed as 1-5 (i.e., one to five), =1,2,3,5,8,13 (i.e., each index listed), or 1-200%5 (i.e., produce a 200 task job array with only 5 tasks active at any given time). The symbol used is the % sign, which tasks to be submitted at once (again, cannot be set larger than 50).

Use-cases: Job Arrays#
Job arrays can be used in situations where you have to process multiple data files using the same procedure or program. Instead of creating multiple scripts or running the same script multiple times, you can create a job array, and Slurm will handle the parallel execution for you.

Example using Job Array Flag#
In the following script, the $SLURM_ARRAY_TASK_ID variable is used to differentiate between array tasks.
#!/bin/bash
#SBATCH -J MyArrayJob           # Job name
#SBATCH -N 1                    # Number of nodes
#SBATCH -n 1                    # Number of tasks
#SBATCH -o output_%A_%a.txt     # Standard output file (%A for array job ID, %a for array index)
#SBATCH -e error_%A_%a.txt      # Standard error file

# Your program/command here
srun ./my_program input_$SLURM_ARRAY_TASK_ID

To submit this job array, save it as my_array_job.sh and run:
sbatch --array=1-50 my_array_job.sh

This command will submit 50 jobs, running my_program with input_1 through input_100.

https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmbestpractices.html

Slurm Best Practices
Slurm is the nerve center of many High-Performance Computing (HPC) clusters. Understanding it well is not optional; it’s a requirement for effective computational research or tasks. This guide outlines how to navigate Slurm as a user.

Job Submission

Interactive and Batch Jobs

Batch Jobs: Use sbatch for long-running jobs. Create an sbatch script specifying resources and runtime parameters.
Interactive Jobs: For testing or debugging, use srun or salloc.

Sbatch Scripts#
#!/bin/bash
#SBATCH --job-name=example_job
#SBATCH --nodes=1
#SBATCH --cpus-per-task=4
#SBATCH --mem=8G
#SBATCH --time=2-00:00:00

# commands to execute

The above script is a template. Modify the directives according to your needs.

Optimizing Resource Usage

Specify Exact Requirements

Use --cpus-per-task to specify the exact number of CPU cores your job requires.
Allocate only the memory you need using --mem.
Make sure to check the amount of resources you are allowed to use for a given partition.

Monitoring Jobs#

Job Status#

squeue: Shows the status of your submitted jobs.
sacct: Provides account information after job completion, good for diagnosing problems.

Real-Time Monitoring

sstat: This is your real-time dashboard for jobs, showing you CPU, memory, and IO metrics.

Troubleshooting#

Job Priority#

sprio: Use this to understand the priority of your pending jobs and why they might be queued.

Logs#

Each Slurm job generates logs. Know how to access and interpret them.

Data Management#

Input and Output#

Use --input and --output directives in your sbatch script to handle data.

Staging#

Utilize tools like rsync to move data to and from the cluster efficiently. Be mindful of disk quotas.


https://rc-docs.northeastern.edu/en/latest/classroom/class_use.html

Classroom HPC: FAQ#
There are several use cases for teaching and learning with the cluster in a classroom. The cluster offers a command line and web interface, allowing access and instruction-level flexibility. Using the cluster gives you, and your students access to many popular scientific applications and allow you and your students to install other packages as needed. The easy-to-use Open OnDemand web portal also offers a built-in visual file explorer for viewing and transferring.
The following Frequently Asked Questions section should help answer most of your questions about using the cluster for classroom use. You can also contact us at rchelp@northeastern.edu or submit a ServiceNow ticket for further information.

How can I use Discovery with my class?#
There are several ways you can use the cluster in your classroom. Your class can use the cluster to access specific software packages and working environments and learn how to utilize high-performance computing (HPC) resources for large and complex data processing, such as machine learning; AI and molecular simulations; and more.

How do I get my class access to the cluster?#
You fill out an HPC Classroom Use Request ticket. The form asks you to submit a list of your student’s names and emails. We will create accounts on the cluster for them. Follow the steps in the article How to Download a List of Student Email Addresses from Canvas to get a list of your students’ emails. If your class roster is incomplete, you can attach a preliminary list to the ticket and follow up with us on the same ticket with an updated roster when the add/drop period is over.

Is there any training on the cluster for my class?#
Yes, we currently provide online training for your class on using the cluster. In the past, we have also given in-person presentations during a class period on the Boston campus. We provide training each semester. We can customize the training to focus on the resources you will be using with them for the class. Email us at rchelp@northeastern.edu and provide details about your class and what training you would like us to provide.

Do my students have to learn Linux to work with the cluster?#
Depending on your class assignments, many students can work with the Open OnDemand web portal, which does not require any knowledge of Linux to use. In cases where you want them to work on the command line, they should have a basic understanding of Linux commands. Please see the question “Is there any training on the cluster for my class” above if you would like us to provide your class with an introductory training course before using the cluster.

What software is available to use with my class on the cluster?#
Many software packages are available, including popular software apps such as Jupyter Notebook, RStudio, and MATLAB. If you have an account on the cluster, you can see the list of available software by using the module avail command. See Using Module for more information. Students have access to all the modules on the cluster. They can also use the interactive apps available on Open OnDemand. See Introduction to OOD for more information. We can also install additional modules and libraries for your class as needed.

My class needs access to a specific software application that I do not see installed on the cluster or Open OnDemand (OOD). What should I do?#
If your class requires software not currently installed on the cluster or OOD, follow the procedure below to request that software be installed on the cluster. You must be a professor or instructor to initiate this request. Students in your class should refrain from making this request. If your students need a specific software application, you must complete the form for them. This is to ensure that we only get one request for the software. Students in one class often make multiple requests for the same software, so having all requests go through the professor or instructor reduces this overlap.
To request additional software (instructors only):

Go to HPC Cluster Software Request. If prompted, sign in to ServiceNow with your Northeastern username and password to access the form.
In the Sponsor’s Name field, enter your name.
Make sure to follow the instructions on the form regarding either providing the URL of the open-source software library or uploading the installation package in your home directory if it requires you to register it first. The request will only be completed with this information.
Select the acknowledgment checkbox, and click Submit.

Note
Software requests can take up to 24 hours to verify; additional time is needed to complete the installation. We might not be able to install every software application requested. If so, we will notify you and provide alternative software to meet your needs.

Tip
You and your students can install software locally to your PATH on the cluster, which may be a better option in some cases, such as installing multiple conda environments. See Software Overview for more information.

I just need my class to access Open OnDemand. How do I request that?#
Open OnDemand is a web portal that lets you access the resources on the cluster through an easy-to-navigate web browser interface. As outlined above, you request course access like you would access the cluster. Please specify that you’d like your course to be on Open OnDemand for your students, in addition to the information we request above.

I’d like my class to use specific resources on the cluster. Can you create a reservation on the cluster for my class?#
We often create a reservation on the cluster for your class for specific hardware resources. For example, if your course assignments require GPUs or nodes with a large amount of memory, we can create a reservation on specific nodes that only your class can access for the course duration. However, the cluster is a shared resource, so we ask that you test out your assignments on the cluster so that you are requesting an appropriate amount of resources for your class. We can always increase your reservation if a class needs more resources due to higher-than-expected use. But, if a reservation is not being used to capacity, we will ask you to review the need for the requested resources and adjust the reservation accordingly. We understand that sometimes it takes time to determine precisely your resource needs. Still, we do need to keep a reservation to a reasonable limit to keep the shared resources available to all users.

How long do my students have access to the cluster?#
Students will have access to the cluster for the whole class duration. They must request an individual account if they want to continue accessing the cluster after that period.

How do I get an account on the cluster?#
If you are a professor or instructor at Northeastern, you can request an account on the cluster. See Sponsor Approval Process for more information.

How do my students get help with the cluster?#
You and your students can submit a Get Assistance with Research Computing ticket or email rchelp@northeastern.edu.

https://rc-docs.northeastern.edu/en/latest/classroom/cps_ood.html

CPS Class Instructions#

Important
The following instructions will only work for CPS classes.

These instructions describe the process of opening a CPS JupyterLab environment on the Open OnDemand (OOD) on the cluster web portal and accessing class /courses folders.

Note
Due to problems with launching OOD on Safari, we recommend using Google Chrome, Mozilla Firefox or Microsoft Edge browsers instead for best experience.

Open JupyterLab For Classes#

Important
The class instructor needs to fill in the: [Discovery Classroom Use Request] You will only be able to find your class resources if a request was already made.

In a web browser, go to http://ood.discovery.neu.edu. Login with your NU credentials.
Under the Courses menu, select your Class Name (For example: ALY6080 JupyterLab).
Select the default options and click Launch. Wait until the session is successfully created and ready to be launched (turns green).
For more control of the session, modify Time for the session time (in hours), Memory to get more memory in GB, and the Working Directory where JupyterLab will launch.

Note
If Working Directory is left blank, the session will launch in the main class folder (in this example /courses/ALY6080.202335/data). Alternatively, start the session directly from your personal working directory by entering: /courses/ALY6080.202335/students/[username], where [username] is your username on Discovery. The instructions below assume the field is left blank.
courses/
└── ALY6080.202335
    ├── data
    ├── staff
    │   ├── doug
    │   ├── evelyn
    │   └── frank
    └── students
        │── alice
        │── bab
        └── cindy

The above file tree represents the folder structure for the ALY6080.202335 course.

Click Connect to Jupyter to open JupyterLab. This will open a JupyterLab interface in another tab.

Caution
Select Cancel when prompted with the Build Recommended option. The package jupyterlab-dash does not require a build, and will not work when build is enabled.

How do i Access Class Directories
Below are the steps to Access Class Directories

After you are connected to a CPS JupyterLab session on OOD, you can access any shared class directories and your private class directory.
You can navigate between the class folders using the left menu. The files in the shared class directory are read ony and the students do not have permissions to edit or remove the files. Your instructor may use the shared class directory to share files related to the classwork
Navigate to your personal class directory under: /courses/[coursename.termcode]/students/<your username>. Now you can create and edit Jupyter notebook files here.
Open a new Python notebook session from the Launcher menu by clicking the Python 3 (ipykernel).
A new file will be created inside your directory called Untitled.ipynb. You can rename it by right-clicking on it and using the rename option.
This Python notebook has ready-to-use Python packages needed for your class.

What i am getting "Permission Denied" errors?
Permission Denied errors:
Do not attempt to create, edit or write files that are outside your personal student directory. Most “Permission Denied” errors are due to directories or files having read-only access permissions.



https://rc-docs.northeastern.edu/en/latest/best-practices/homequota.html

Home Directory Storage Quota
There are strict quotas for each home directory (i.e., /home/<username>), and staying within the quota is vital for preventing issues on the HPC. This page provides some best practices for keeping within the quota. For more information about data storage on the HPC, see Data Storage Options.

Important
All commands on this page should be run from a compute node because they are CPU-intensive. You can find more information on getting a job on a compute node from Interactive Jobs: srun Command.

How do i check size of file, directory, and hidden directory in my /home/<username> space?
Analyze Disk Usage
From a compute node, run the following command from your /home/<username> directory:
du -shc .[^.]* ~/*

This command will output the size of each file, directory, and hidden directory in your /home/<username> space, with the total of your /home directory being the last line of the output. After identifying the large files and directories, you can move them to the appropriate location (e.g., /work for research) or back up and delete them if they are no longer required. An example output would look like:
[<username>@<host> directory]$  du -shc .[^.]* ~/*
39M     .git
106M    discovery-examples
41K     README.md
3.3M    software-installation
147M    total


What is /work and /scratch directory used for? 
Utilize /work and /scratch
Use /work for long-term storage. 
PIs can request a folder in /work via New Storage Space Request and additional storage via Storage Space Extension Request. Utilize /scratch/<username> for temporary or intermediate files. Then, move files from /scratch to /work for persistent storage (i.e., the recommended workflow).

Note
Please be mindful of the /scratch purge policy, which can be found on the Research Computing Policy Page. See Data Storage Options for information on /work and /scratch.

Cleaning Directories

Conda#
How can i reduce the storage size of the environments ?
Note
Conda environments are part of your research and should be stored in your PI’s /work directory.

Here are some suggestions to reduce the storage size of the environments for those using the /home/<username>/.conda directory.
Remove unused packages and clear caches of Conda by loading an Anaconda module and running the following:
source activate <your environment>
conda clean --all

This will only delete unused packages in your ~/.conda/pkgs directory.
To remove any unused conda environments, run:
conda env list
conda env remove --name <your environment>

Singularity#
If you have pulled any containers to the HPC using Singularity, you can clean your container cache in your /home/<username> directory by running the following command from a compute node:
module load singularity/3.5.3
singularity cache clean all

To avoid your ~/.singularity directory filling up, you can set a temporary directory for when you pull a container to store the cache in that location; an example of this procedure (where <project> is your PI’s /work directory) is the following:
mkdir /work/<project>/singularity_tmp
export SINGULARITY_TMPDIR=/work/<project>/singularity_tmp

Then, pull the container using Singularity as usual.

Cache#
The ~/.cache directory can become large with the general use of HPC and Open OnDemand. Make sure you are not running any processes or jobs at the time by running the following:
squeue -u <username>

which prints a table with JOBID, PARTITION, NAME, USER ST, TIME, NODES, and NODELIST (REASON), which is empty when no jobs are running (i.e., it is safe to remove ~/.cache when no jobs are running).

Storing research environments#

Conda environments#
Use conda environments for Python on HPC. To create an environment in /work, use the --prefix flag as follows: (where <project> is your PI’s /work directory and <my conda env> is an empty directory to store your Conda environment):
conda create --prefix=/work/<project>/<my conda env>

Utilize the same conda environment to save storage space and time (i.e., avoid duplicate conda environments). Hence, shared environments can be easily done for a project accessing the same /work directory.
More information about creating custom Conda environments.

Singularity containers#
Containers pulled, built, and maintained for research work should be stored in your PI’s /work directory, not your /home/<username> directory.

https://rc-docs.northeastern.edu/en/latest/best-practices/checkpointing.html

Checkpointing Jobs
The complexity of HPC systems can introduce unpredictable hardware or software component behavior, leading to job failures. Applying fault tolerance techniques to your HPC workflows can make your jobs more resilient to crashes, partition time limits, and hardware failures.

how do i  handle unexpected interruptions during job execution?
The Checkpointing technique
Checkpointing is a fault tolerance technique based on the Backward Error Recovery (BER) technique, designed to overcome “fail-stop” failures (interruptions during the execution of a job).

To implement checkpointing:

Use data redundancy to create checkpoint files, saving all necessary calculation state data. Checkpoint files are generally created at constant intervals during the run.
If a failure occurs, start from an error-free state, check for consistency, and restore the algorithm to the previous error-free state.

Checkpointing allows you to:

Create resilient workflows in the event of faults.
Overcome most scheduler resource time limitations.
Implement an early error detection approach by inspecting intermediate results.

What are the different types of checkpointing available on the Discovery HPC cluster?
Checkpointing types
Checkpointing can be implemented at different levels of your workflow.

Application-level checkpointing is recommended for most Discovery users. You can use the checkpointing tool that is already available in your software application. For example, most software designed for HPC has a checkpointing option, and information on proper usage is often available in the software user manual.
User-level checkpointing is good if you develop your code or know the application code well enough to integrate checkpointing techniques effectively. We recommend this approach for some Discovery users with advanced proficiency and familiarity with checkpointing mechanisms.
System-level checkpointing is done on the system side, where the user saves the state of the entire process. This option is less efficient than User-level or Application-level checkpointing as it introduces a lot of redundancy.
Model-level checkpointing is suitable for saving a model’s internal state (its weights, current learning rate, etc.) so that the framework can resume the training from this point whenever desired. This is often the intent of users doing machine learning on Discovery.

Which checkpoint type should you use?
There are several checkpointing options, depending on your software’s needs.

If your software already includes built-in checkpointing, this is often the preferred option, as it is the most optimized and efficient way to checkpoint.
Application-level checkpointing is the easiest to use, as it exists within your application. It does not require significant script changes and saves only the relevant data for your specific application.
User-level checkpointing is recommended if you are writing your code. You can use DMTCP or implement checkpointing.
ML Model-level checkpointing is specific to model training and deployment, as detailed in the ML Model-level_ section.

Note
Some packages can be used to implement checkpointing if you are developing in Python, Matlab, or R. Some examples include Python PyTorch checkpointing, TensorFlow checkpointing, MATLAB checkpointing, and R checkpointing. Additionally, many Computational Chemistry and Molecular Dynamics software packages have built-in checkpointing options (e.g., GROMACS and LAMMPS).

Implementing checkpointing can be achieved by the following:

Some save-and-load mechanisms of your calculation state.
The use of Slurm Job Arrays.

Note
To overcome partition time limits, replace your single long job with multiple shorter jobs. Then, use job arrays to set each job to run one after the other. If checkpointing is used, each job will write a checkpoint file. The following job will use the latest checkpoint file to continue from the latest state of the calculation.




Application-level checkpointing
How can I implement application-level checkpointing in a GROMACS job using Slurm job arrays and the partition's time limits?
GROMACS checkpointing example
The following example shows how to implement a 120-hour GROMACS job using multiple shorter jobs on the short partition. We use Slurm job arrays and the GROMACS built-in checkpointing option to implement checkpointing.



The following script submit_mdrun_array.sh creates a Slurm job array of 10 individual array jobs:
#!/bin/bash
#SBATCH --partition=short
#SBATCH --constraint=cascadelake
#SBATCH --nodes=1
#SBATCH --time=12:00:00
#SBATCH --job-name=myrun
#SBATCH --ntasks=56
#SBATCH --array=1-10%1  # execute 10 array jobs, 1 at a time
#SBATCH --output=myrun-%A_%a.out
#SBATCH --error=myrun-%A_%a.err

module load cuda/10.2
module load gcc/7.3.0
module load openmpi/4.0.5-skylake-gcc7.3
module load gromacs/2020.3-gpu-mpi
source /shared/centos7/gromacs/2020.3-gcc7.3/bin/GMXRC.bash

srun --mpi=pmi2 -n $SLURM_NTASKS gmx_mpi mdrun -ntomp 1 -s myrun.tpr -v -dlb yes -cpi state

The script above sets the checkpoint flag -cpi state preceding the filename to dump checkpoints. This directs mdrun to the checkpoint in state.cpt when loading the state. The Slurm option --array=1-10%1 creates 10 Slurm array tasks and runs one task job serially for 12 hours. The variable %A denotes the main job ID, while %a denotes the task ID (i.e., spanning 1-10).
To submit this array job to the scheduler, use the following command:
sbatch submit_mdrun_array.bash

DMTCP checkpoint example#
Distributed MultiThreaded checkpointing (DMTCP) is a tool checkpoint without changing code. It works with most Linux applications, such as Python, Matlab, R, GUI, and MPI.
The program runs in the background of your program, without significant performance loss and saves the process states into checkpoint files. DMTCP is available on the cluster
module avail dmtcp
module show dmtcp
module load dmtcp/2.6.0

Since DMTCP runs in the background, it requires some changes to your shell script. See examples of checkpointing with DMTCP, which use DMTCP with a simple C++ program (scripts modified from RSE-Cambridge).

Application-level checkpointing tips#
What data should you save?

Non-temporary application data
Any application data that has been modified since the last checkpoint
Delete no longer useful checkpoints; keep only the most recent checkpoint file.

How frequently do checkpoints occur?

Too often will slow your calculation; maybe I/O is heavy and memory intensive.
Too infrequently leads to large or long rollback times.
Consider how long it takes to reach a checkpoint and restart your calculation.
In most cases, every 10–15 minutes is okay.

How can I manage long-running machine learning training jobs on the Discovery HPC cluster to ensure progress is not lost in case of interruptions? 
What is model-level checkpointing in machine learning, and why is it important for long-running training jobs?
ML Model-level Checkpointing
Model-level checkpointing is a technique used to periodically save the state of a machine learning (ML) model during training, enabling the training process to be resumed from the saved checkpoint in case of interruptions or premature termination. The saved state typically includes the model’s parameters, optimizer state, and essential training information (e.g., the epoch number and loss value). Model checkpoints are especially critical for long-running training jobs.

Why is Checkpointing Important in Deep Learning?
Why is checkpointing a critical practice in deep learning, and how does it benefit the training process?
Checkpointing is crucial in deep learning because the training process can be time-consuming and require significant computational resources. Additionally, the training process may be interrupted due to hardware or software issues. Checkpoints solve this problem by saving the model’s current state to resume where it left off.
Moreover, checkpointing also saves the best-performing model, which can be loaded for evaluation. For instance, the model’s performance can vary based on the initialization and optimization algorithms, so checkpointing provides a way to select the best model based on a performance metric.
In summary, checkpointing is essential in deep learning as it provides a way to save progress, resume training, and select the best-performing model.

how do i perform checkpoint in tensorflow?
TensorFlow checkpoint example
The following example demonstrates implementing a longer ML job using the tf.keras checkpointing API and multiple shorter Slurm job arrays on the GPU partition.
The following example of the submit_tf_array.bash script:
#!/bin/bash
#SBATCH --job-name=myrun
#SBATCH --time=00:10:00
#SBATCH --partition=gpu
#SBATCH --nodes=1
#SBATCH --gres=gpu:1
#SBATCH --mem=10Gb
#SBATCH --output=%A-%a.out
#SBATCH --error=%A-%a.err
#SBATCH --array=1-10%1  #execute 10 array jobs, 1 at a time.

module load miniconda3/2020-09
source activate tf_gpu

# Define the number of steps based on the job id:
numOfSteps=$(( 500 * SLURM_ARRAY_TASK_ID ))

# Run python and save outputs to a log file corresponding to the current job task ID
python train_with_checkpoints.py $numOfSteps &> log.$SLURM_ARRAY_TASK_ID

The following checkpoint example is given in the program train_with_checkpoints.py:
checkpoint_path = "training_2/{epoch:d}.ckpt"
checkpoint_dir = os.path.dirname(checkpoint_path)
cp_callback = tf.keras.callbacks.ModelCheckpoint(
   filepath=checkpoint_path,
   verbose=1,
   save_weights_only=True,
   period=5)

See scripts, which are modified from TensorFlow Save and load models.
The Slurm option --array=1-10%1 will create 10 Slurm array tasks and run one task at a time. Note that the saved variable %A denotes the main job ID, while variable %a denotes the task ID (spanning values 1-10). Note that the output/error files are also unique to prevent different jobs from writing to the same files. The Shell variable SLURM_ARRAY_TASK_ID holds the unique task ID value and can be used within the Slurm Shell script to point to different files or variables.
To submit this job to the scheduler, use the command:
sbatch submit_tf_array.bash

PyTorch checkpoint example#



Setting up a Sample Project#
Next, we will set up a sample project to demonstrate checkpointing in PyTorch. We will create a simple neural network in PyTorch and train it on a sample dataset. The following code creates a simple neural network in PyTorch:
import torch
import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 128)
        self.fc2 = nn.Linear(128, 10)
    def forward(self, x):
        x = x.view(-1, 28 * 28)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

Once the neural network is defined, we can load a sample dataset and train the model. In this example, we will use the MNIST dataset in the torchvision library. The following code loads the MNIST dataset and trains the model:
import torchvision
import torchvision.transforms as transforms

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)
net = Net()
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

for epoch in range(2):
    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        inputs, labels = data
        optimizer.zero_grad()
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
    print('Epoch %d loss: %.3f' % (epoch + 1, running_loss / len(trainloader)))

With this simple project setup, we can demonstrate checkpointing in PyTorch.

What is the model state in PyTorch?#
In PyTorch, the model state refers to the values of all the parameters, weights, and biases that define the model’s behavior. This state is stored in the model’s state_dict, a dictionary-like object that maps each layer to its parameter tensors. Additionally, the state_dict contains information about the model’s architecture and the values of the parameters learned during training.
The state_dict can be saved to a file using PyTorch’s torch.save function and loaded back into memory using torch.load. This allows for checkpointing, which saves the state of a model periodically during training so that it can be recovered in the case of a crash or interruption.
PyTorch also provides the ability to save and load the entire model, including the model’s architecture and optimizer state, using the torch.save and torch.load functions. This allows for complete checkpointing of a model’s state.
It is important to note that the state_dict only contains information about the model’s parameters, not the optimizer or other training-related information. So, to save the optimizer state, you also need to save it separately and load it back when loading the model.
In conclusion, the model state in PyTorch represents the values of all the parameters, weights, and biases that define the model’s behavior. The state is stored in the state_dict and can be saved and loaded for checkpointing purposes. By saving the model state periodically during training, you can ensure that your progress is recovered during a crash or interruption.

Saving a PyTorch Model#

Saving a Model’s state_dict#
To save the learnable parameters of a PyTorch model and the optimizer, we must save the model’s state_dict. We can use the torch.save function to pass the state_dict and file names as arguments. The following code demonstrates how to save the state_dict of the model:
checkpoint_path = 'checkpoint.pth'
state = {'epoch': epoch + 1,
         'state_dict': net.state_dict(),
         'optimizer': optimizer.state_dict()}
torch.save(state, checkpoint_path)

Saving the Entire Model#
In addition to saving the state_dict, we can also save the entire model, which includes the architecture, the learnable parameters, and the optimizer state. We can use the torch.save() function to save the entire model by passing the model and a filename as arguments. The following code demonstrates how to save the entire model:
model_path = 'model.pth'
torch.save(net, model_path)

Note
When saving the entire model, the custom classes used to define the model must be importable, either as a part of the project or in a separate file that can be imported.

We have seen how to save the state_dict and the entire model in PyTorch. The following section will show how to load a saved model.

Loading a PyTorch Model#

Loading a PyTorch state_dict#
We can use the torch.load function to load the state_dict of a saved model by passing the file name as an argument. After loading the state_dict, we can set the state_dict of the model using the model.load_state_dict method. The following code demonstrates how to load the state_dict of a saved model:
codecheckpoint = torch.load(checkpoint_path)
net.load_state_dict(checkpoint['state_dict'])
optimizer.load_state_dict(checkpoint['optimizer'])
epoch = checkpoint['epoch']

Loading the Entire PyTorch Model#
To load the entire model, use the torch.load function and pass the file name as an argument. The loaded model can then be assigned to a variable, as shown in the following code:
loaded_model = torch.load(model_path)

In PyTorch, we can load a saved model’s state_dict or the entire model. This section will cover how to resume training from a checkpoint.

Resuming Training from a Checkpoint#
To resume training from a checkpoint, we must load the model’s state_dict and the optimizer’s state (see Loading a PyTorch Model). After loading these, we can continue the training process from where it was left off. The following code demonstrates how to resume training from a checkpoint:
checkpoint = torch.load(checkpoint_path)
net.load_state_dict(checkpoint['state_dict'])
optimizer.load_state_dict(checkpoint['optimizer'])
epoch = checkpoint['epoch'] # Continue training
for epoch inrange(epoch, num_epochs): # Train the model
    # Save the checkpoint
    state = {'epoch': epoch + 1,
             'state_dict': net.state_dict(),
             'optimizer': optimizer.state_dict()
             }
torch.save(state, checkpoint_path)

This code loads the state_dict and the optimizer’s state from the checkpoint file and resumes training from the epoch value saved in the checkpoint file. After each epoch, the model’s state_dict and the optimizer’s state are saved in the checkpoint file, as described in Section III. This shows how to checkpoint PyTorch models, save and load the state_dict, save and load the entire model, and resume training from a checkpoint.

Model-level tips and tricks

Save only the model’s State_dict
What should be included in a checkpoint file to efficiently resume training a model, and what should be avoided to reduce the file size?
Save only the model’s state_dict and optimizer state, allowing us to save only the information needed to resume training. Doing so reduces the checkpoint file’s size and makes it easier to load the model. So, please don’t forget to avoid unnecessary information in the checkpoint file, such as irrelevant metadata or tensors we can define during training.

Save regularly
To prevent losing progress in case of a crash or interruption, save the checkpoint file regularly (i.e., after each epoch).

Save to multiple locations
What is a recommended practice for saving checkpoint files during a model training process to minimize data loss in case of interruptions?
Save the checkpoint file to multiple locations, such as a local drive and the cloud, to ensure that the checkpoint is recovered in case of failure.

Why is it important to use the latest version of PyTorch and related libraries when saving and managing checkpoints in model training?
Use the latest versions of libraries
Using the latest version of PyTorch and other relevant libraries is vital; changes in these libraries may cause compatibility issues with older checkpoints. With these best practices, you can ensure that your PyTorch models are saved efficiently and effectively and that your progress is not lost in the event of a crash or interruption.

Naming conventions#
Use a consistent naming convention for checkpoint files; include information such as the date, time, and epoch number in the file name to make tracking multiple checkpoint files easier and ensure you choose the proper checkpoint to load.

Checkpoint Validation#
Validate the checkpoint after loading it to ensure that the model’s state_dict and the optimizer’s state are correctly loaded by making a prediction using the loaded model and checking that the results are as expected.

Periodic clean-up
Periodically, remove old checkpoint files to avoid filling up storage. This can be done by keeping only the latest
checkpoint or keeping only checkpoint files from the last few epochs.

Document checkpoints#
Document the purpose of each checkpoint and what it contains: the model architecture, the training data, the hyperparameters, and the performance metrics. This will help keep track of the progress and make it easier to compare different checkpoints. With these additional best practices, you can ensure that your checkpointing process is efficient, effective, and well-organized.

https://rc-docs.northeastern.edu/en/latest/best-practices/optimizingperformance.html

Optimizing Job Performance
Optimizing job performance is essential to maximizing efficiency and minimizing computational time on HPC systems. This page provides an overview and best practices on how to achieve this.

Understanding the System Architecture#
Understanding the underlying hardware, including CPUs, GPUs, memory hierarchy, and network topology, will help you make informed decisions about structuring your computational tasks.



Choosing the Right Resources
Selecting the right resources, such as the correct queue, number of CPUs, amount of memory, or using GPUs, is essential for performance optimization.

Number of CPUs: Determine the appropriate number based on your task’s parallel nature.
Memory: Monitor and evaluate the memory requirements.
GPUs: Utilize GPUs if your computations are suitable for GPU acceleration.

Code Optimization#
Code optimization involves:

Algorithmic Efficiency: Choose the most efficient algorithms.
Compiler Options: Utilize compiler optimization flags.
Data Structure Choices: Choose the most appropriate data structures.

Parallelization#
Understanding and employing parallelization techniques can significantly reduce computational time. This includes:

Thread-level parallelization: Using tools like OpenMP.
Process-level parallelization: Using MPI for distributed computing.

Profiling Tools#
Leveraging profiling tools to identify bottlenecks and areas for improvement in your code is crucial.

Common Profiling Tools in HPC#

Tool
Purpose

gprof
GNU Performance Profiling Tool

Intel VTune
Performance and Thread profiler

NVIDIA Nsight
GPU performance analysis and debugging

Best Optimization Practices

Always Profile First: Use profiling tools to find bottlenecks before optimizing any code.
Regular Monitoring: Monitor the job’s performance and make necessary adjustments.
Use Existing Libraries: Often, existing libraries are optimized and can save you time.
Documentation and Version Control: Keep your code well-documented and use version control for reputability.

Further Reading#

Intel’s Optimization Guide
NVIDIA’s GPU Optimization Guide

Note
This is a general guide and may not cover all specific cases. Always consult with the HPC support team or refer to the specific documentation related to your hardware and software for tailored optimization strategies.

Please contact our support team if you need personalized assistance optimizing your job’s performance.

https://rc-docs.northeastern.edu/en/latest/best-practices/software.html

Best SW Practices#
Good software practices are vital for successful scientific computing. This page outlines the best methods to follow when developing and running software on HPC systems.

Coding Standards#
Adhering to coding standards ensures your code is readable, maintainable, and less prone to errors.

Follow a Style Guide: Include a style guide relevant to your programming language.
Use Meaningful Names: Name variables and functions descriptively.
Comment Wisely: Write comments that explain the ‘why,’ not the ‘what.’

Version Control#
Utilizing a version control system like Git helps track changes, allows collaboration with others, and is essential for reputability.

Commit Regularly: Make frequent, small commits.
Use Meaningful Commit Messages: Describe what and why changes were made.
Keep a Changelog: Document significant changes in a human-readable file.

Documentation#
Proper documentation helps others (and you in the future) understand your code.

Write inline comments: Explain complex or non-intuitive parts of the code.
Maintain a README: Include setup, usage, and other essential information.
Use automated documentation tools: Tools like Doxygen or Sphinx can help.

Testing#
Implementing automated tests ensures that your code behaves as expected.

Write Unit Tests on individual code units for correctness.
Perform Integration Tests between different code parts.
Automate testing using Continuous Integration tools to run tests automatically.

Dependency Management#
Managing dependencies effectively avoids conflicts and ensures reputability.

Use Dependency Management tools like Conda or virtualenv.
Specify exact versions: Note the specific versions of dependencies.

Performance Considerations#
Consider performance throughout the development process.

**Profile often: Identify bottlenecks and areas for optimization.
**Consider parallelization: Utilize parallel computing resources.

Security#
Please follow the best practices to keep your code and data secure.

Keep Sensitive Information Private: Use environment variables for secrets.
Stay Updated: Keep dependencies updated to avoid known vulnerabilities.

Collaboration#
Effective collaboration fosters quality and productivity.

Communicate Openly: Keep lines of communication open with your team.
Use Collaboration Platforms: GitHub or GitLab facilitate collaboration.

These best practices will help develop high-quality, maintainable, and performing software suitable for HPC and other computational environments. Please always seek feedback from peers, and don’t hesitate to consult the HPC support team for specific guidance.

https://rc-docs.northeastern.edu/en/latest/tutorialsandtraining/../faq.html


What hardware is in the cluster?

The cluster compute and storage capabilities are changing on a regular basis.


What is a cluster?

A cluster is a network of interconnected computers designed to work together as a unified and robust system. These computers, also known as nodes, collaborate to collectively handle complex computational tasks more efficiently than a single machine could achieve. By distributing workloads across multiple nodes, a cluster harnesses parallel computing capabilities, enabling researchers and professionals to solve intricate problems, analyze large datasets, and perform simulations with incredible speed and precision. Clusters are widely used in high-performance computing (HPC) environments, scientific research, data analysis, and other fields that demand significant computing resources.

What is the policy on fair use of resources?

The fair use policy for computing resources aims to provide equitable access and distribution within the cluster. This policy ensures that all users have an equal opportunity to utilize the cluster resources based on their needs and priorities.
Fair use policies typically utilize a scheduler that allocates resources to jobs based on job priority, resource requirements, and historical resource usage. This approach prevents any single user or job type from monopolizing the resources and ensures a balanced and efficient allocation of resources across different users and projects.
This policy promotes fairness and efficient resource utilization and contributes to an inclusive and productive computing environment for all users.





How do I get an account for the HPC cluster?

See Request An Account, and complete the ServiceNow RC Access Request form with your details and submit it. Your request will be processed within a few business days.

How can I reset my password?

Please use our website’s ‘Account Management’ tool for password reset. Follow the instructions to reset your password.



What happens to my account when I leave NU?

ITS controls access to the University’s computing resources, so when you or your students leave, you/they may lose access to many of these resources. Sponsored accounts allow people who work or volunteer at NU, but who are not paid by NU, to access the University’s computing resources. Researchers with sponsored accounts cannot request RC services, but they are allowed to use the systems we manage as members of a MyGroups (requires VPN connection) group controlled by a NU Principal Investigator (PI). Details on sponsored accounts are posted on the ITS sponsored accounts page.

How do I get access?

The HPC resources are available to Northeastern University research faculty, and classes that require high-performance computing for their coursework, and students. For detailed information about reporting requirements and any access restrictions, please refer to our Getting Access page. If you have questions about our access policy or procedures, please contact RC.

What is my cluster username?

Your cluster username is the same as your NU username. If you do not have a cluster account, your NU username is usually your last name, period, and then the first letter of your first name plus your last name (for staff, first letter of first name, period, then followed by lastname). If you’re unsure, please contact RC.
Your cluster username is the same as your NU username. Your NU username is the same as your email address without @northeastern.edu. If you’re unsure, please contact RC.




How do I gain access to the cluster?

A faculty or research staff member must sponsor your HPC account request. Faculty and staff can self sponsor. Full details can be found here.



How do I log on to the cluster?

Use an SSH client and connect to login.discovery.neu.edu. Instructions for using ssh and other login tools, as well as recommended clients for different operating systems, as described in Shell Environment on the Cluster. You can also access the cluster through our Web-based interface Open OnDemand (OOD).


How do I reset my password?

Access to the HPC cluster requires a valid Northeastern University email account. Your HPC password is the same as your Northeastern University account password so in order to reset it, reset your Northeastern University account password.



How can I view .pdf or .csv files on the cluster?

You can view .pdf and .csv files utilizing the File Explorer application on Open OnDemand and the Open OnDemand Desktop Application.



How do I request all CPUs on a node with more than one GPU?

You may wish to request a single GPU on a node and all of the node’s CPUs. However, the GPUs are bound to specific CPUs so the job will only run on the CPUs associated with the GPU you’re running on. Specifying the –exclusive flag in your job script or requesting all of the node’s CPUs will not change this. If you would like to use all cores on a node with one of the GPUs, you must specify this in your Slurm script: #SBATCH –gres-flags=disable-binding
Refer to the Slurm documentation for further information.

How can I get information on the HPC such as how busy it is?

From a login node, you can run sinfo -p short to get the state of the HPC.




How can I transfer data to/from the HPC cluster?

Data transfer can be done using various methods like scp, rsync, or Globus. Refer to the ‘Transferring Data’ section in ‘Data Management’ for detailed instructions.



What Linux commands can I use to transfer files to/from the cluster?

Smaller files can be transferred to/from the cluster using scp, sftp, and rsync as well as standard FTP tools utilizing xfer.discovery.neu.edu.
Larger files should be moved using Globus.




How do I submit jobs?

You submit jobs by writing a Slurm script and submitting it with the sbatch command. Please see our Slurm documentation page.



How do I submit an interactive job?

For an interactive job on the command line, submit an srun job from the login node. 
If you wish to run a program that requires a graphical user interface or generates other graphics for display, such as a plot or chemical model, use one of the Open OnDemand interactive apps. Several are available, but if you one you wish to use isn’t in the list, submit an OOD Desktop job. You can also use X11 forwarding with srun.



What partitions can I use?

All account holders have access to the short, debug, express, and gpu partitions. 

How do I choose which partition to use?

Partitions are set up to emphasize run time limits and different resource limits and specialty hardware including GPUs. 

How do I check the status of my jobs?

Run the command squeue -u $USER from the login node.
If reporting a problem to us about a particular job, please let us know the JobID for the job that you are having a problem with. You can also run squeue -u $USER to obtain the JobID.

Why is my job not starting?

Several things can cause jobs to wait in the queue. If you request a resource combination we do not have available at the moment the job will be marked pending (PD). You may also have run a large number of jobs in the recent past and the “fair share” algorithm is allowing other users higher priority. Finally, the queue you requested may simply be very busy. If your job is pending there will be another field with the reason; if it is Resources that means that the resource you requested isn’t available, either because it is busy or because you requested a nonexistent resource. If the reason is “Priority” it means that a job with higher priority than yours is running. Your job will rise in priority as it waits, so it will start eventually. To request an estimate from the queueing system of your start time, run squeue -u $USER --start.

How can I check when my job will start?

Run
squeue -j <jobid> --start

Slurm will provide an estimate of the day and time your job will start.

Why was my job killed?

Usually this is because you inadvertently submitted the job to run in a location that the compute nodes can’t access or is temporarily unavailable. If your jobs exit immediately this is usually why. Other common reasons include using too much memory, too many cores, or running past a job’s timelimit.
You can run sacct
If it’s still not clear why your job was killed, please contact us and send us the output from sacct.

How can I submit a job to the HPC cluster?

Jobs can be submitted with sbatch and srun to the HPC.



How can I check the status of my job?

You can check the status of your job using the squeue -u $USER command, which will display your current jobs in the queue.



What should I do if my job fails?

Check the output and error files for any error messages if your job fails. These files are usually located in your job’s working directory. If you cannot resolve the issue yourself, contact Research Computing with the details of the error message.

When will my job start?

You can list information on your job’s start time using the squeuegit  command:
squeue -u $USER --start

Note that Slurm’s estimated start time can be a bit inaccurate. This is because Slurm calculates this estimation off the jobs that are currently running or queued in the system. Any job that is submitted after yours with a higher priority may delay your job. Alternatively, if jobs complete in less time than they’ve requested, more jobs can start sooner than anticipated.
For more information on the squeue command, take a look at our Job Management

How do I check the efficiency of my completed jobs?

Run the command seff on the Slurm job ID:
[user@login-00 ~] seff 38391902
Job ID: 38391902
Cluster: discovery
User/Group: user/users
State: COMPLETED (exit code 0)
Cores: 1
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 00:03:25 core-walltime
Job Wall-clock time: 00:03:25
Memory Utilized: 652.00 KB
Memory Efficiency: 0.03% of 1.95 GB

What is a batch system? / What is a job scheduler?

The purpose of a batch system is to execute a series of tasks in a computer program without user intervention (non-interactive jobs). The operation of each program is defined by a set or batch of inputs, submitted by users to the system as “job scripts.”
When job scripts are submitted to the system, the job scheduler determines how each job is queued. If there are no queued jobs of higher priority, the submitted job will run once the necessary compute resources become available.

What are the differences between batch jobs, interactive jobs, and GUI jobs?

A batch job is submitted to the batch system via a job script passed to the sbatch command. Once queued, a batch job will run on resources chosen by the scheduler. When a batch job runs, a user cannot interact with it.

View a walk-through of an example batch job script
An interactive job is any process that is run at the command line prompt, generally used for developing code or testing job scripts. Interactive jobs should only be run in an interactive session, which are requested through the srun command. As soon as the necessary compute resources are available, the job scheduler will start the interactive session.
View the Interactive Development & Testing documentation page
A GUI job uses the cluster compute resources to run an application, but displays the application’s graphical user interface (GUI) to the local client computer. GUI sessions are also managed by the job scheduler, but require additional software to be installed on the client side computer or ran through Open OnDemand.

How do I submit a job to the batch system?

The primary job submission mechanism is via the sbatch command via the Linux command line interface
$ sbatch <your_job_script>

where <your_job_script> is a file containing the commands that the batch system will execute on your behalf.



How do I run applications that use multiple processors (i.e. parallel computing)?

Parallel computing refers to the use of multiple processors to run multiple computational tasks simultaneously. Communications between tasks use one of the following interfaces, depending on the task:

OpenMP – used for communication between tasks running concurrently on the same node with access to shared memory
MPI (eg OpenMPI) – used for communication between tasks which use distributed memory
Hybrid – a combination of both OpenMP and MPI interfaces

You must properly configure your job script in order to run an application that uses multiple processors. View sample SLURM scripts for each case below:

Why do I get the error ‘slurmstepd: Exceeded job memory limit at some point’?

Sometimes, SLURM will log the error slurmstepd: Exceeded job memory limit at some point. This appears to be due to memory used for cache and page files triggering the warning. The process that enforces the job memory limits does not kill the job, but the warning is logged. The warning can be safely ignored. If your job truly does exceed the memory request, the error message will look like:
slurmstepd: Job 5019 exceeded memory limit (1292 > 1024), being killed
slurmstepd: Exceeded job memory limit
slurmstepd: *** JOB 5019 ON dev1 CANCELLED AT 2016-05-16T15:33:27 ***

How do I check the my job status?

You can use the following command to check the status of the jobs you’ve submitted:
$ squeue -u <user_name>

To check the status of jobs running under a particular group, modify the command with the -A flag:
$ squeue -A <group_name>

To also return QoS information for jobs under a particular group, use the following command:
$ squeue -O jobarrayid,qos,name,username,timelimit,numcpus,reasonlist -A <group_name>

How do I delete a job from the batch system?

You can use the command
$ scancel <job_id>

to delete jobs from the queue. You can only delete jobs that you submitted.

What are the wall time limits for a partition?

You can check the wall time limits, denoted as TIMELIMIT, for a partition with the following command replacing $PARTITION with the partition of interest
$ sinfo -p $PARTITION

How can I check how busy a partition is?

Use the following command to view how busy the partition is
$ sinfo -p $PARTITION

and view the STATE column to see how many nodes are ‘idle’ (no jobs running on them).

How can I use GPUs for my job?

To use GPUs for your job, you need to request them in your job script. You can find more information in the ‘Working with GPUs’ section.



How do I login to the compute node my job is running on?

You will only be able to login to compute nodes that your jobs are running on. If the job is running on one node use:
srun --jobid=jobid --pty /bin/bash

where you can find your jobid by running
squeue -u $USER

If the job is running on more than one node, specify the node you want to login to:
srun --jobid=jobid --nodelist=node_name -N1 --pty /bin/bash

If your job is allocated all of the resources on the node, you will need to include the –overlap option.



What are the data storage options?

Several data storage options are available depending on your needs, such as /home, project directories (/work/$PROJECT), and temporary storage (/scratch). Details can be found in the ‘Data Management’ section.



What types of storage are available and how should each type be used?

Can my /work storage quota be increased?

You can request a quota increase by submitting a Storage Space Extension Request

Why can’t I run jobs in my home directory?

/home/$USER directories are intended for relatively small amounts of human-readable data such as text files, shell scripts, and source code. All research work should be stored at /work/$PROJECT for long term storage and /scratch/$USER should be used for temporary job storage during run time. Be mindful of /scratch/$USER purge policy.



Why should I use /scratch storage?

Scratch storage is fast and provides a large quantity of free space for temporary jobs files. However, there are limits on the number of files and the amount of space you may use and there is a time based purge policy that is implimented. Please review our scratch filesystem policy for details.



How do I check my /home/$USER disk usage?

Run du -shc .[^.]* * /home/$USER which will output:
[<username>@<host> ~]$  du -shc .[^.]* * /home/$USER/
39M     .git
106M    discovery-examples
41K     README.md
3.3M    software-installation
147M    total

How long can I store files in /scratch?

/scratch is designed to serve as fast, temporary storage for running jobs, and is not long-term storage. For this reason, files are periodically marked for deletion from all /scratch directories. Please review the /scratch filesystem policy for more details. Store longer-term files in your /work/$PROJECT directory.





What software applications are available?

The full list of applications installed on the cluster is available by using the module avail command from the terminal on the HPC.



May I submit an installation request for an application?

You can submit a Software Installation Request ticket to the Research Computing team.
You can also try and install your program locally with package mangers or compiling from source by following our documentation pages.



Can I install my software on the HPC cluster?

Yes, you can. Please follow the guidelines in the Package Managers and From Source sections. If you encounter any issues, contact Research Computing.

How do I use research software that’s already installed?

We use the modules system for managing software environments. Learn more about how to use modules from Using Module.

Can I run this Docker container on the cluster?

We do not run Docker on the cluster due to security issues. Instead, we use Singularity. Singularity can run Docker images directly, or you can convert a Docker image to a Singularity image (.sif). To import existing Docker images, use the singularity pull command.
module load singularity
singularity pull docker://<container repository URL path>

Can I run my application/container on a GPU?

Please check the documentation for your application/container before running on a GPU. The developer of the program/container will outline GPU support and commands for the program.



Classroom (course specific)#

How can I get my class access to the HPC?

Please submit a Classroom Access Request ticket in order for a classroom to get a access.




How do I develop and test software?

Use compute nodes for software development and testing. Connect to the cluster and use the following command:
$ srun --pty /bin/bash

The srun command can be modified to request additional time, processors, and memory. Please refer to Interactive Jobs: srun Command to see more configuration and options for this command.
We use modules to update our \(PATH and \)LD_LIBRARY_PATH environment variables. To use any available software package that is not part of the default environment, including compilers, you must load the associated modules. To see what is available on the HPC, please use module avail

What compilers are available?

We have the GNU and Intel compilers available on the HPC. Please use module avail gcc or module avail intel to see the versions we have on the HPC.



Why does my Open OnDemand desktop or app show it’s starting but then it immediately ends?

There are two common reasons why you might not be able to launch an Open OnDemand session including interactive desktops and apps like JupyterLab Notebook and RStudio.

You are over quota in your /home/$USER directory. See Home Directory Storage Quota
You have a conda environment loading in your .bashrc environment file or are loading a Python module in your .bashrc file that is interfering with the Open OnDemand desktop. See Shell Environment on the Cluster



How do I run MATLAB programs?

You may use the interactive MATLAB interpreter on the compute nodes after requesting a srun session and module load matlab/<version>.



Common Errors and Issues#

Why can’t I log in?

This is a very generic question that is difficult for us to answer. Research Computing supports many services. If you were to ask this question in a help ticket we would respond with: What are you trying to login to? Are you getting any error messages?

Why does my application keep getting killed on the login nodes?

Login nodes are not meant for long running processes and for CPU intensive tasks. If you are running applications on the HPC interactively, please use srun to get time on a compute node.


How can I see what the file permissions are?

The getfacl command is an easy way to see the permissions of a file or directory. It will display the file/directory name, owner of the file/directory, group name that owns the file/directory, and the detailed permissions of the file/directory. 

Why am I getting a QOSMaxSubmitJobPerUserLimit error when I try to submit a job?

You may see this error message when you are submitting more jobs than permitted to run on the partition at a given time. These limits are discussed in Partitions.
sbatch: error: QOSMaxSubmitJobPerUserLimit
sbatch: error: Batch job submission failed: Job violates accounting/QOS policy (job submit limit, user's size and/or time limits)

You will get this error if you have reached the partition or per user limits as described here. For example, if you have 1000 jobs in the general-compute partition and try to submit another one, you will get this error. If you’ve already launched one viz desktop, you’ve reached your limit. Wait for some of your jobs to finish and submit more at that time.

Why does my SSH session automatically disconnect?

SSH connections will time out either due to inactivity or network disruptions. If your sessions are disconnecting due to inactivity, one thing you can do to keep the SSH connection open is to have ssh send a periodic keep alive packet to the server, so it will not timeout. Add the -o ServerAliveInterval=600 option to your ssh login command. SSH can be sensitive to any disruptions in the network which can be common with Wi-Fi networks. Sometimes the ‘keep alive’ setting prevents this. Other times, it may be that you have a setting on your Wi-Fi or ethernet adapter that tells the operating system it can put the device to sleep after a period of inactivity. This is especially common on Windows. Check your network adapters for ‘Power Settings’ and uncheck any options that tell the system it can disable the device to save power. This will vary by operating system so we recommend you conduct an internet search for the appropriate instructions.

Why am I seeing WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED when I log in?

Some users logging in through ssh may encounter this error message. If you receive this message, please see our instructions on how to clear this error.
When I try to log in with ssh, nothing happens when I type my password!
When you type your password, the ssh program does not echo your typing or move your cursor. This is normal behavior.

When running Firefox on the cluster, I get : “Firefox is already running, but is not responding. To open a new window, you must first close the existing Firefox process, or restart your system.” What can I do?

From your home directory on the cluster, run the commands:
rm -rf ~/.mozilla/firefox/*.default/.parentlock
rm -rf ~/.mozilla/firefox/*.default/lock


How do I acknowledge the use of Research Computing resources?

Please acknowledge resources provided by Northeastern University’s Research Computing in publications as follows:
Support provided by Research Computer at Northeastern University [1]. And cite as (using the appropriate citation format):
[1] Research Computing, Northeastern University, https://rc.northeastern.edu/.

I think I have a cluster issue, but I’m not sure about it. What should I do?

You can always open a support request when you have questions even if you are not sure whether there is an issue.


What if my question does not appear here?

Take a look at our documentation (this). If your answer is not there, submit a documentation request on GitHub or contact us via rchelp@northeastern.edu.


