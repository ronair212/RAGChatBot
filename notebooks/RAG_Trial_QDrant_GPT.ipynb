{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation using ChromaDB and GPT\n",
    "\n",
    "## Overview\n",
    "\n",
    "This code snippet explores the implementation of a conversational chatbot using various APIs and the `langchain` Python library. Specifically, it initiates API interactions with `Cohere` and `OpenAI`, utilizes web scraping for document loading, applies token-based text splitting, and leverages embedding models for semantic understanding. Additionally, it applies a memory buffer to retain chat history and structures conversational chains for query and answer generation. \n",
    "\n",
    "### Setting Up API Keys\n",
    "\n",
    "```python\n",
    "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OPENAI API Key:\")\n",
    "```\n",
    "\n",
    "The above code securely retrieves API keys for Cohere and OpenAI, prompting the user to input them without displaying them on-screen.\n",
    "\n",
    "### Loading and Pre-processing Data\n",
    "\n",
    "```python\n",
    "urls = [...]\n",
    "loader = WebBaseLoader(urls)\n",
    "data = loader.load()\n",
    "```\n",
    "\n",
    "A predefined set of URLs is loaded and parsed using `WebBaseLoader`, which essentially fetches documents/web pages for further interaction.\n",
    "\n",
    "### Embeddings and Document Representation\n",
    "\n",
    "```python\n",
    "embeddings = CohereEmbeddings(model='...', cohere_api_key=..(\"COHERE_API_KEY\"))\n",
    "db = Chroma.from_documents(docs, embeddings)\n",
    "```\n",
    "This segment prepares document embeddings using the `CohereEmbeddings` and stores them in a format suitable for quick retrieval and similarity search using `Chroma`.\n",
    "\n",
    "### Retrievers and Compressors\n",
    "\n",
    "```python\n",
    "from langchain.retrievers import ContextualCompressionRetriever, VectorStoreRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "\n",
    "#... [various retriever and compressor setup]\n",
    "```\n",
    "Retrievers and compressors manage the extraction and summarization of information from the loaded documents.\n",
    "\n",
    "### Conversational Interface\n",
    "\n",
    "```python\n",
    "memory = ConversationTokenBufferMemory(llm=llm, memory_key=\"chat_history\", return_messages=True, input_key='question', max_token_limit=1000)\n",
    "```\n",
    "The conversation’s state is managed through memory objects. This setup helps retain and reference past interactions to manage coherent dialogue.\n",
    "\n",
    "### Question and Answer Generation\n",
    "\n",
    "```python\n",
    "#... [prompt template set up]\n",
    "\n",
    "question_generator = LLMChain(llm=llm, prompt=chat_prompt_for_ques, verbose=True)\n",
    "answer_chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\", verbose=True, prompt=chat_prompt)\n",
    "```\n",
    "Using predefined prompt templates, the system generates questions and answers. These questions derive from user inputs and historical chat data, while answers reference and cite embedded documents.\n",
    "\n",
    "### Conversational Chain and User Interaction\n",
    "\n",
    "```python\n",
    "chain = ConversationalRetrievalChain(\n",
    "            retriever=reranker,\n",
    "            question_generator=question_generator,\n",
    "            combine_docs_chain=answer_chain,\n",
    "            verbose=True,\n",
    "            memory=memory,\n",
    "            rephrase_question=False\n",
    ")\n",
    "\n",
    "query = \"What is the Scheduling Policies for HPC cluster?\"\n",
    "result = chain({\"question\": query})\n",
    "\n",
    "print(\"Question from user : \" , query ,\"\\n\")\n",
    "print(\"Reply from ChatBot : \" , result['answer'])\n",
    "```\n",
    "This conversational chain consolidates the previously defined components to facilitate user interaction. It retrieves and generates responses dynamically, considering prior conversational history and the loaded document base. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MNSwbz8svgCi",
    "outputId": "edbdd442-5caa-4c86-b902-da4a5f72f106"
   },
   "outputs": [],
   "source": [
    "#!pip install pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MuNwBfb6BngZ",
    "outputId": "30409fdb-2773-411e-dcfc-170ab7fc63f7"
   },
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nwJCPFhtCBmi",
    "outputId": "aa6b6914-32a8-4f4d-e68c-84e5077d0fad"
   },
   "outputs": [],
   "source": [
    "#!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vwqNT7T0CF-O",
    "outputId": "9fc123df-4163-4f34-f81f-610513bc1fe5"
   },
   "outputs": [],
   "source": [
    "#!pip install cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QQl_Bfl8CLL1",
    "outputId": "cc07de5a-2e7c-4b28-a66d-5906fea3a052"
   },
   "outputs": [],
   "source": [
    "#!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3oMYj3f-vmG7"
   },
   "outputs": [],
   "source": [
    "#import pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "njgYMfbQvnjn"
   },
   "outputs": [],
   "source": [
    "#pinecone.init(api_key=\"b360318b-4fc8-4580-bf6c-d88959179985\",\n",
    "#              environment=\"us-west1-gcp-free\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "drKj9gcYvx36"
   },
   "outputs": [],
   "source": [
    "#pinecone.whoami()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "B979WTowvyGz"
   },
   "outputs": [],
   "source": [
    "#pinecone.list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "71U2rh7xwQIy"
   },
   "outputs": [],
   "source": [
    "#pinecone.list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Cohere API Key: ········································\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nCohereAPIError: You are using a Trial key, which is limited to 2 API calls / minute.\\nYou can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at\\n'https://dashboard.cohere.ai/api-keys'. \\nContact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "CohereAPIError: You are using a Trial key, which is limited to 2 API calls / minute.\n",
    "You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at\n",
    "'https://dashboard.cohere.ai/api-keys'. \n",
    "Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OPENAI API Key: ···················································\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OPENAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mq7rbJww6Cjo"
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "\n",
    "urls = [\"https://rc-docs.northeastern.edu/en/latest/welcome/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/welcome/welcome.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/welcome/services.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/welcome/gettinghelp.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/welcome/introtocluster.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/welcome/casestudiesandtestimonials.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/get_access.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/accountmanager.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/connectingtocluster/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/connectingtocluster/mac.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/connectingtocluster/windows.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/first_steps/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/first_steps/passwordlessssh.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/first_steps/shellenvironment.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/first_steps/usingbash.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/hardware/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/hardware/hardware_overview.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/hardware/partitions.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/introduction.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/accessingood.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/desktopood.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/fileexplore.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/jupyterlab.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/understandingqueuing.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/workingwithgpus.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/recurringjobs.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/debuggingjobs.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/discovery_storage.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/transferringdata.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/globus.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/databackup.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/securityandcompliance.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/modules.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/mpi.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/r.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/matlab.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/packagemanagers/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/packagemanagers/conda.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/packagemanagers/spack.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/fromsource/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/fromsource/makefile.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/fromsource/cmake.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/introductiontoslurm.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmcommands.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmrunningjobs.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmmonitoringandmanaging.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmscripts.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmarray.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmbestpractices.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/classroom/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/classroom/class_use.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/classroom/cps_ood.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/classroom/classroomexamples.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/best-practices/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/best-practices/homequota.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/best-practices/checkpointing.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/best-practices/optimizingperformance.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/best-practices/software.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/tutorialsandtraining/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/tutorialsandtraining/canvasandgithub.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/faq.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/glossary.html\",\n",
    "]\n",
    "loader = WebBaseLoader(urls)\n",
    "data = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6twYZJo26GmB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "encoding_name = tiktoken.get_encoding(\"cl100k_base\")\n",
    "#tiktoken package to count the total tokens in our corpus\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "9b3T5m1YBVrf",
    "outputId": "4f06dc21-756e-4dd9-f462-c503f4dd675b",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\n\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    chunk_size = 700,\\n    chunk_overlap  = 70,\\n    length_function = len,\\n    add_start_index = True,\\n)\\ndocs = text_splitter.create_documents([data])\\n\\nfor idx, text in enumerate(docs):\\n    docs[idx].metadata[\\'source\\'] = \"RCDocs\"\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "text_splitter = TokenTextSplitter(chunk_size=500, chunk_overlap=25)\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "'''\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 700,\n",
    "    chunk_overlap  = 70,\n",
    "    length_function = len,\n",
    "    add_start_index = True,\n",
    ")\n",
    "docs = text_splitter.create_documents([data])\n",
    "\n",
    "for idx, text in enumerate(docs):\n",
    "    docs[idx].metadata['source'] = \"RCDocs\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_DfcNy9sT3jj",
    "outputId": "36618e1f-e92c-4ed3-f051-980b780bc57b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain.schema.document.Document"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PILXy2pdVC4v",
    "outputId": "99fdfd5c-a9f0-422a-dded-c4c33e642735"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='\\n\\n\\n\\n\\n\\n\\nResearch Computing - RC RTD\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nContents\\n\\n\\n\\n\\n\\nMenu\\n\\n\\n\\n\\n\\n\\n\\nExpand\\n\\n\\n\\n\\n\\nLight mode\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDark mode\\n\\n\\n\\n\\n\\n\\nAuto light/dark mode\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHide navigation sidebar\\n\\n\\nHide table of contents sidebar\\n\\n\\n\\n\\n\\nToggle site navigation sidebar\\n\\n\\n\\n\\nRC RTD\\n\\n\\n\\n\\nToggle Light / Dark / Auto color theme\\n\\n\\n\\n\\n\\n\\nToggle table of contents sidebar\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nResearch ComputingToggle child pages in navigation\\nWelcome\\nServices We Provide\\nGetting Help\\nIntroduction to HPC and Slurm\\nCase Studies and User Testimonials\\n\\n\\n\\n\\nGetting StartedToggle child pages in navigation\\nGetting Access\\nAccount Manager\\nConnecting To ClusterToggle child pages in navigation\\nMac\\nWindows\\n\\n\\n\\n\\nFirst StepsToggle child pages in navigation\\nPasswordless SSH\\nShell Environment on the Cluster\\nCluster via Command-Line\\n\\n\\n\\nUser Guides\\n\\nHardwareToggle child pages in navigation\\nOverview\\nPartitions\\n\\n\\nOpen OnDemand (OOD)Toggle child pages in navigation\\nIntroduction to OOD\\nAccessing Open OnDemand\\nInteractive Open OnDemand ApplicationsToggle child pages in navigation\\nDesktop App\\nOOD File Explorer\\nJupyterLab\\nStata\\n\\n\\n\\n\\nRunning JobsToggle child pages in navigation\\nUnderstanding the Queuing System\\nJob Scheduling Policies and Priorities\\nInteractive and Batch Mode\\nWorking with GPUs\\nRecurring Jobs\\nDebugging and Troubleshooting Jobs\\n\\n\\nData ManagementToggle child pages in navigation\\nData Storage Options\\nTransfer Data\\nUsing Globus\\nData Backup and Restore\\nSecurity and Compliance\\n\\n\\nSoftwareToggle child pages in navigation\\nSystem WideToggle child pages in navigation\\nModules\\nMPI\\nR\\nMatlab\\n\\n\\nPackage ManagersToggle child pages in navigation\\nConda\\nSpack\\n\\n\\nFrom SourceToggle child pages in navigation\\nMake\\nCMake\\n\\n\\n\\n\\nSlurmToggle child pages in navigation\\nIntroduction to Slurm\\nSlurm Commands\\nSlurm Running Jobs\\nMonitoring and Managing Jobs\\nSlurm Job Scripts\\nSlurm Array Jobs and Dependencies\\nSlurm Best Practices\\n\\n\\nHPC for the ClassroomToggle child pages', metadata={'source': 'https://rc-docs.northeastern.edu/en/latest/welcome/index.html', 'title': 'Research Computing - RC RTD', 'language': 'en'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "kh1JnImJ6Mt2"
   },
   "outputs": [],
   "source": [
    "#from langchain.vectorstores import Pinecone\n",
    "#import pinecone\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "#query_result = embeddings.embed_query(text)\n",
    "\n",
    "#doc_result = embeddings.embed_documents([text])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "yZiQ1hc_6PYl"
   },
   "outputs": [],
   "source": [
    "##uncomment for cohere embeddings\n",
    "#from langchain.embeddings import CohereEmbeddings\n",
    "#embeddings = CohereEmbeddings(model='embed-english-light-v2.0',cohere_api_key=os.environ.get(\"COHERE_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bumy27EP6Q7F",
    "outputId": "19048b6b-2076-4af7-b351-78053cf34a9c"
   },
   "outputs": [],
   "source": [
    "#pinecone.init(\n",
    "#\tapi_key='b360318b-4fc8-4580-bf6c-d88959179985',\n",
    "#\tenvironment='us-west1-gcp-free'\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "8U4W4qHt_m79"
   },
   "outputs": [],
   "source": [
    "#pinecone.delete_index(\"chatbot1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "XYqGH05t_nXW"
   },
   "outputs": [],
   "source": [
    "#pinecone.create_index(\"chatbot1\", dimension=1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "yHrQSGHpWSMT"
   },
   "outputs": [],
   "source": [
    "#index = pinecone.Index('chatbot1')\n",
    "\n",
    "#index_name = \"chatbot1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "tLU7yDoS6Stw"
   },
   "outputs": [],
   "source": [
    "#docsearch = Pinecone.from_documents(docs, embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "5oEHdLS1mfma"
   },
   "outputs": [],
   "source": [
    "#from langchain.vectorstores import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#You can configure Chroma to save and load from your local machine. \\n#Data will be persisted automatically and loaded on start (if it exists).\\n\\n# save to disk\\ndb2 = Chroma.from_documents(docs, embedding_function, persist_directory=\"./chroma_db\")\\ndocs = db2.similarity_search(query)\\n\\n# load from disk\\ndb3 = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding_function)\\ndocs = db3.similarity_search(query)\\nprint(docs[0].page_content)\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#You can configure Chroma to save and load from your local machine. \n",
    "#Data will be persisted automatically and loaded on start (if it exists).\n",
    "\n",
    "# save to disk\n",
    "db2 = Chroma.from_documents(docs, embedding_function, persist_directory=\"./chroma_db\")\n",
    "docs = db2.similarity_search(query)\n",
    "\n",
    "# load from disk\n",
    "db3 = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding_function)\n",
    "docs = db3.similarity_search(query)\n",
    "print(docs[0].page_content)\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "39b7bDIInas5"
   },
   "outputs": [],
   "source": [
    "#Chromadb = Chroma.from_documents(docs, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#In-memory\\u200b\\n#For some testing scenarios and quick experiments, you may prefer to keep all the data in memory only, so it gets lost when the client is destroyed - usually at the end of your script/notebook.\\n\\nqdrant = Qdrant.from_documents(\\n    docs,\\n    embeddings,\\n    location=\":memory:\",  # Local mode with in-memory storage only\\n    collection_name=\"RC_documents\",\\n)\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#In-memory​\n",
    "#For some testing scenarios and quick experiments, you may prefer to keep all the data in memory only, so it gets lost when the client is destroyed - usually at the end of your script/notebook.\n",
    "\n",
    "qdrant = Qdrant.from_documents(\n",
    "    docs,\n",
    "    embeddings,\n",
    "    location=\":memory:\",  # Local mode with in-memory storage only\n",
    "    collection_name=\"RC_documents\",\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Local mode, without using the Qdrant server, may also store your vectors on disk so they're persisted between runs.\n",
    "from langchain.vectorstores import Qdrant\n",
    "\n",
    "Qdrantdb = Qdrant.from_documents(\n",
    "    docs,\n",
    "    embeddings,\n",
    "    path=\"/work/rc/projects/chatbot/chatbotrc/notebooks/RAG/tmp/local_qdrant\",\n",
    "    collection_name=\"RC_documents\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#On-premise server deployment\\u200b\\n#No matter if you choose to launch Qdrant locally with a Docker container, or select a Kubernetes deployment with the official Helm chart, the way you\\'re going to connect to such an instance will be identical. You\\'ll need to provide a URL pointing to the service.\\n\\nurl = \"<---qdrant url here --->\"\\nqdrant = Qdrant.from_documents(\\n    docs,\\n    embeddings,\\n    url=url,\\n    prefer_grpc=True,\\n    collection_name=\"my_documents\",\\n)\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#On-premise server deployment​\n",
    "#No matter if you choose to launch Qdrant locally with a Docker container, or select a Kubernetes deployment with the official Helm chart, the way you're going to connect to such an instance will be identical. You'll need to provide a URL pointing to the service.\n",
    "\n",
    "url = \"<---qdrant url here --->\"\n",
    "qdrant = Qdrant.from_documents(\n",
    "    docs,\n",
    "    embeddings,\n",
    "    url=url,\n",
    "    prefer_grpc=True,\n",
    "    collection_name=\"my_documents\",\n",
    ")\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "CF8AHNQm6Ucf"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "#from langchain.vectorstores import Pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "aeEbBgCS6dHH"
   },
   "outputs": [],
   "source": [
    "# load index\n",
    "#docsearch = Pinecone.from_existing_index(index_name, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "zp19GFSp6ed2",
    "outputId": "b8cf41d0-4243-43bc-a35d-08a2eb34aba8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nVectorStoreRetriever\\n\\nReturn VectorStoreRetriever initialized from this VectorStore.\\n\\nArgs:\\n    search_type (Optional[str]): Defines the type of search that\\n        the Retriever should perform.\\nCan be \"similarity\" (default), \"mmr\", or\\n\"similarity_score_threshold\".\\n    search_kwargs (Optional[Dict]): Keyword arguments to pass to the\\n        search function. Can include things like:\\n            k: Amount of documents to return (Default: 4)\\n            score_threshold: Minimum relevance threshold\\n                for similarity_score_threshold\\n            fetch_k: Amount of documents to pass to MMR algorithm (Default: 20)\\n            lambda_mult: Diversity of results returned by MMR;\\n                1 for minimum diversity and 0 for maximum. (Default: 0.5)\\n            filter: Filter by document metadata\\n\\nReturns:\\n    VectorStoreRetriever: Retriever class for VectorStore.\\n\\nExamples:\\n\\n# Retrieve more documents with higher diversity\\n# Useful if your dataset has many similar documents\\ndocsearch.as_retriever(\\n    search_type=\"mmr\",\\n    search_kwargs={\\'k\\': 6, \\'lambda_mult\\': 0.25}\\n)\\n\\n# Fetch more documents for the MMR algorithm to consider\\n# But only return the top 5\\ndocsearch.as_retriever(\\n    search_type=\"mmr\",\\n    search_kwargs={\\'k\\': 5, \\'fetch_k\\': 50}\\n)\\n\\n# Only retrieve documents that have a relevance score\\n# Above a certain threshold\\ndocsearch.as_retriever(\\n    search_type=\"similarity_score_threshold\",\\n    search_kwargs={\\'score_threshold\\': 0.8}\\n)\\n\\n# Only get the single most similar document from the dataset\\ndocsearch.as_retriever(search_kwargs={\\'k\\': 1})\\n\\n# Use a filter to only retrieve documents from a specific paper\\ndocsearch.as_retriever(\\n    search_kwargs={\\'filter\\': {\\'paper_title\\':\\'GPT-4 Technical Report\\'}}\\n)\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize base retriever\n",
    "#retriever = docsearch.as_retriever(search_kwargs={\"k\": 4})\n",
    "#retriever = Chroma.as_retriever(search_kwargs={\"k\": 4})\n",
    "#retriever = Chroma.as_retriever(\n",
    "#    search_type=\"mmr\",\n",
    "#    search_kwargs={'k': 4, 'fetch_k': 50} )\n",
    "\n",
    "from langchain.schema.vectorstore import VectorStoreRetriever\n",
    "retriever = VectorStoreRetriever(vectorstore=Qdrantdb, search_type=\"mmr\", search_kwargs={'k': 4, 'fetch_k': 10},)\n",
    "\n",
    "'''\n",
    "\n",
    "VectorStoreRetriever\n",
    "\n",
    "Return VectorStoreRetriever initialized from this VectorStore.\n",
    "\n",
    "Args:\n",
    "    search_type (Optional[str]): Defines the type of search that\n",
    "        the Retriever should perform.\n",
    "Can be \"similarity\" (default), \"mmr\", or\n",
    "\"similarity_score_threshold\".\n",
    "    search_kwargs (Optional[Dict]): Keyword arguments to pass to the\n",
    "        search function. Can include things like:\n",
    "            k: Amount of documents to return (Default: 4)\n",
    "            score_threshold: Minimum relevance threshold\n",
    "                for similarity_score_threshold\n",
    "            fetch_k: Amount of documents to pass to MMR algorithm (Default: 20)\n",
    "            lambda_mult: Diversity of results returned by MMR;\n",
    "                1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
    "            filter: Filter by document metadata\n",
    "\n",
    "Returns:\n",
    "    VectorStoreRetriever: Retriever class for VectorStore.\n",
    "\n",
    "Examples:\n",
    "\n",
    "# Retrieve more documents with higher diversity\n",
    "# Useful if your dataset has many similar documents\n",
    "docsearch.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={'k': 6, 'lambda_mult': 0.25}\n",
    ")\n",
    "\n",
    "# Fetch more documents for the MMR algorithm to consider\n",
    "# But only return the top 5\n",
    "docsearch.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={'k': 5, 'fetch_k': 50}\n",
    ")\n",
    "\n",
    "# Only retrieve documents that have a relevance score\n",
    "# Above a certain threshold\n",
    "docsearch.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={'score_threshold': 0.8}\n",
    ")\n",
    "\n",
    "# Only get the single most similar document from the dataset\n",
    "docsearch.as_retriever(search_kwargs={'k': 1})\n",
    "\n",
    "# Use a filter to only retrieve documents from a specific paper\n",
    "docsearch.as_retriever(\n",
    "    search_kwargs={'filter': {'paper_title':'GPT-4 Technical Report'}}\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "LcR414tm7UAd"
   },
   "outputs": [],
   "source": [
    "compressor = CohereRerank() #LLMChainExtractor,LLMChainFilter,EmbeddingsFilter\n",
    "# will iterate over the initially returned documents and extract from each only the content that is relevant to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "dmf4wxNb7Xr0"
   },
   "outputs": [],
   "source": [
    "# Set up cohere's reranker\n",
    "''' instead of immediately returning retrieved documents as-is, \n",
    "you can compress them using the context of the given query, so that only the relevant information is returned. '''\n",
    "reranker = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "E6L9I9sU7Yy7"
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "EQuyfiGR7aO5"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "lUJilunXIox7"
   },
   "outputs": [],
   "source": [
    "#from langchain.callbacks import ContextCallbackHandler\n",
    "#from langchain.callbacks import FlyteCallbackHandler\n",
    "from langchain.callbacks import StdOutCallbackHandler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "M18gm_YBHKE8"
   },
   "outputs": [],
   "source": [
    "#context_callback = ContextCallbackHandler(token=\"T1gM1n4RzGWLFSsJnQ5ziLUW\")\n",
    "#context_callback = FlyteCallbackHandler()\n",
    "context_callback = StdOutCallbackHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "-BFTmJkK-na4"
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.7, verbose=True, openai_api_key = os.environ.get(\"OPENAI_API_KEY\"), streaming=True, callbacks=[context_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "4_aSlIy6F1e8"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "ConversationTokenBufferMemory keeps a buffer of recent interactions in memory,\n",
    "and uses token length rather than number of interactions to determine when to flush interactions.\n",
    "'''\n",
    "memory = ConversationTokenBufferMemory(llm=llm,memory_key=\"chat_history\", return_messages=True,input_key='question',max_token_limit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "BAbvsetCTEIL"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "CONDENSE_QUESTION_PROMPT = '''\n",
    "Below is a summary of the conversation so far, and a new question asked by the user that needs to be answered by searching in a knowledge base.\n",
    "Generate a search query based on the conversation and the new question.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Search query:\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "PromptTemplates = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"question\"],\n",
    "    template=\"\"\"\n",
    "Below is a summary of the conversation so far, and a new question asked by the user that needs to be answered by searching in a knowledge base.\n",
    "Generate a search query based on the conversation and the new question.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Search query:\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "system_message_prompt = SystemMessagePromptTemplate(prompt=PromptTemplates)\n",
    "\n",
    "chat_prompt_for_ques = ChatPromptTemplate.from_messages(\n",
    "    [system_message_prompt])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "wJiRbW27TaZp"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "WiKNdmX3S1Zj"
   },
   "outputs": [],
   "source": [
    "question_generator = LLMChain(llm=llm, prompt=chat_prompt_for_ques, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "CzcMctULXMY9"
   },
   "outputs": [],
   "source": [
    "Answer_Generator_Prompt= '''\n",
    "<Instructions>\n",
    "Important:\n",
    "Answer with the facts listed in the list of sources below. If there isn't enough information below, say you don't know.\n",
    "If asking a clarifying question to the user would help, ask the question.\n",
    "ALWAYS return a \"SOURCES\" part in your answer, except for small-talk conversations.\n",
    "\n",
    "Question: {question}\n",
    "Sources:\n",
    "---------------------\n",
    "    {summaries}\n",
    "---------------------\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "C2yZpFpRVJPD"
   },
   "outputs": [],
   "source": [
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "\n",
    "chat_prompt = PromptTemplate(template=Answer_Generator_Prompt, input_variables=[\"question\", \"summaries\",\"chat_history\"])\n",
    "\n",
    "answer_chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\", verbose=True,prompt=chat_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "WgdFwotlZtyJ"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "\n",
    "chain = ConversationalRetrievalChain(\n",
    "            retriever=reranker,\n",
    "            question_generator=question_generator,\n",
    "            combine_docs_chain=answer_chain,\n",
    "            verbose=True,\n",
    "            memory=memory,\n",
    "            rephrase_question=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ClKEHA56ZxqI",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "d6d1b3c3-fa3d-4e9b-aef7-1894e9d2f630",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "<Instructions>\n",
      "Important:\n",
      "Answer with the facts listed in the list of sources below. If there isn't enough information below, say you don't know.\n",
      "If asking a clarifying question to the user would help, ask the question.\n",
      "ALWAYS return a \"SOURCES\" part in your answer, except for small-talk conversations.\n",
      "\n",
      "Question: What is the Scheduling Policies for HPC cluster?\n",
      "Sources:\n",
      "---------------------\n",
      "    Content:  resources to become available.\n",
      "\n",
      "Resource Reservation#The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.\n",
      "\n",
      "Scheduling Policy#A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their priority, resource requirements, and other factors.\n",
      "\n",
      "Scratch Space#Temporary storage that allows users to store intermediate data during job execution. Data in scratch space is not preserved between jobs.\n",
      "\n",
      "Storage Cluster#A set of networked storage devices used to provide centralized and scalable storage solutions for the HPC environment.\n",
      "\n",
      "Scheduler#A program that manages the cluster’s resources and allocates them to jobs based on priority, requested resources, and fair use policies.\n",
      "\n",
      "Singularity#A containerization platform commonly used in HPC environments. It allows users to create and run containers focusing on security and compatibility, making it suitable for running scientific applications.\n",
      "\n",
      "Slurm#An open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small HPC clusters.\n",
      "\n",
      "Task#A unit of work within a job that can be executed independently. A job can consist of multiple tasks.\n",
      "\n",
      "VPN#A technology that creates a secure and encrypted connection over a public network, such as the Internet. It often provides remote access to HPC clusters, ensuring data privacy and security during remote cluster interactions.\n",
      "\n",
      "\n",
      "\n",
      "This glossary is not exhaustive. If you come across a term not listed here, please check the specific section of the documentation or ask in our User Community and Forums.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Next\n",
      "\n",
      "Change Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Previous\n",
      "\n",
      "Frequently Asked Questions (FAQs)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                Copyright © 2023, RC\n",
      "            \n",
      "            Made with \n",
      "            Furo\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/glossary.html\n",
      "\n",
      "Content:  larger resource requirements may be assigned higher priority, as they require more significant resources to execute efficiently.\n",
      "\n",
      "\n",
      "Walltime Limit#\n",
      "Jobs with shorter estimated execution times may receive higher priority, ensuring they are executed promptly and freeing up resources for other jobs.\n",
      "\n",
      "\n",
      "\n",
      "Balancing Policies#\n",
      "\n",
      "Backfilling#\n",
      "This policy allows smaller jobs to “backfill” into available resources ahead of larger jobs, optimizing resource utilization and reducing wait times.\n",
      "\n",
      "\n",
      "Preemption#\n",
      "Higher-priority jobs can preempt lower-priority ones, temporarily pausing the lower-priority job’s execution to make resources available for the higher-priority job.\n",
      "\n",
      "\n",
      "\n",
      "Best Practices#\n",
      "\n",
      "Set Realistic Priorities: Assign accurate priorities to your jobs to reflect their importance and resource requirements.\n",
      "Use Resource Quotas: Be mindful of the resources you request to prevent over- or underutilization.\n",
      "Leverage Backfilling: Submit smaller, shorter jobs that can backfill into available resources while waiting for larger jobs to start.\n",
      "\n",
      "Understanding these scheduling policies and priorities empowers you to make informed decisions when submitting jobs, ensuring that your computational tasks are executed efficiently and promptly. If you need further guidance on selecting the right scheduling policy for your job or optimizing your resource usage, our support team is available at rchelp@northeastern.edu or consult our Frequently Asked Questions (FAQs).\n",
      "Optimize your job execution by maximizing our cluster’s scheduling capabilities. Happy computing!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Next\n",
      "\n",
      "Interactive and Batch Mode\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Previous\n",
      "\n",
      "Understanding the Queuing System\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                Copyright © 2023, RC\n",
      "            \n",
      "            Made with \n",
      "            Furo\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            On this page\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Job Scheduling Policies and Priorities\n",
      "Scheduling Policies\n",
      "FIFO (First-In-First-Out)\n",
      "Fair Share\n",
      "Priority-Based\n",
      "\n",
      "\n",
      "Job Priorities\n",
      "User Priority\n",
      "Resource Requirements\n",
      "Walltime Limit\n",
      "\n",
      "\n",
      "Balancing Policies\n",
      "Backfilling\n",
      "Preemption\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\n",
      "\n",
      "Content: )#The primary component of a computer that performs most processing inside the computer. CPUs can have multiple cores.\n",
      "\n",
      "Fair Share Allocation#A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.\n",
      "\n",
      "Graphics Processing Unit (GPU)#A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.\n",
      "\n",
      "GPU Acceleration#The use of GPUs to offload computation-intensive tasks from the CPU, leading to faster processing of tasks like simulations and data analysis.\n",
      "\n",
      "Home Directory#A user’s directory in the cluster where personal files, application settings, and other user-specific data are stored.\n",
      "\n",
      "High-Performance Computing (HPC)#The use of parallel processing for running advanced application programs efficiently, reliably, and quickly. It’s often used for scientific research, big data analysis, and modeling complex systems.\n",
      "\n",
      "InfiniBand (IB)#A high-speed networking technology commonly used in HPC clusters. It provides high bandwidth and low latency communication between nodes in the cluster, facilitating fast data transfer for parallel processing.\n",
      "\n",
      "Job#A set of computations a user submits to the HPC cluster for execution.\n",
      "\n",
      "Job Dependency#The condition where one job relies on the successful completion of another job before it can start, ensuring proper sequencing of tasks.\n",
      "\n",
      "Job Priority#Refers to the relative importance or urgency assigned to a specific computational task or job within a High-Performance Computing (HPC) cluster’s scheduling system. Job priority determines the order in which jobs are executed and the allocation of computing resources.\n",
      "In an HPC environment, different jobs may have varying degrees of importance or resource requirements. Job priority allows the cluster’s scheduler to make decisions on which job to execute next, considering factors such as:\n",
      "\n",
      "User-defined Priority: Users can often assign priority values to their jobs, indicating the relative importance of their tasks. Higher priority values typically result in faster job execution.\n",
      "Resource Requirements: Jobs with greater resource demands, such as more CPUs, memory, or GPUs, may receive higher priority, ensuring they receive the necessary resources for efficient execution.\n",
      "Walltime Limit: Jobs with shorter estimated execution times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.\n",
      "\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/glossary.html\n",
      "---------------------\n",
      "\n",
      "Chat History:\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Question from user :  What is the Scheduling Policies for HPC cluster? \n",
      "\n",
      "Reply from ChatBot :  The scheduling policies for an HPC cluster include:\n",
      "1. Fair Share: A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.\n",
      "2. FIFO (First-In-First-Out): Jobs are scheduled in the order they are submitted, with no consideration for job priority or resource requirements.\n",
      "3. Priority-Based: Jobs are executed based on their priority, which can be determined by user-defined priority, resource requirements, and walltime limit.\n",
      "4. Backfilling: This policy allows smaller jobs to \"backfill\" into available resources ahead of larger jobs, optimizing resource utilization and reducing wait times.\n",
      "5. Preemption: Higher-priority jobs can preempt lower-priority ones, temporarily pausing the lower-priority job's execution to make resources available for the higher-priority job.\n",
      "\n",
      "SOURCES:\n",
      "- Source 1: https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\n",
      "- Source 2: https://rc-docs.northeastern.edu/en/latest/glossary.html\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the Scheduling Policies for HPC cluster?\"\n",
    "result = chain({\"question\": query})\n",
    "\n",
    "\n",
    "print(\"Question from user : \" , query ,\"\\n\")\n",
    "print(\"Reply from ChatBot : \" , result['answer'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "gLLL9c7tbgW1",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "766a6bf0-92f3-47ca-d56e-30cb719d395f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "Below is a summary of the conversation so far, and a new question asked by the user that needs to be answered by searching in a knowledge base.\n",
      "Generate a search query based on the conversation and the new question.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: What is the Scheduling Policies for HPC cluster?\n",
      "Assistant: The scheduling policies for an HPC cluster include:\n",
      "1. Fair Share: A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.\n",
      "2. FIFO (First-In-First-Out): Jobs are scheduled in the order they are submitted, with no consideration for job priority or resource requirements.\n",
      "3. Priority-Based: Jobs are executed based on their priority, which can be determined by user-defined priority, resource requirements, and walltime limit.\n",
      "4. Backfilling: This policy allows smaller jobs to \"backfill\" into available resources ahead of larger jobs, optimizing resource utilization and reducing wait times.\n",
      "5. Preemption: Higher-priority jobs can preempt lower-priority ones, temporarily pausing the lower-priority job's execution to make resources available for the higher-priority job.\n",
      "\n",
      "SOURCES:\n",
      "- Source 1: https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\n",
      "- Source 2: https://rc-docs.northeastern.edu/en/latest/glossary.html\n",
      "\n",
      "Question:\n",
      "How do I check Job Status?\n",
      "\n",
      "Search query:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "<Instructions>\n",
      "Important:\n",
      "Answer with the facts listed in the list of sources below. If there isn't enough information below, say you don't know.\n",
      "If asking a clarifying question to the user would help, ask the question.\n",
      "ALWAYS return a \"SOURCES\" part in your answer, except for small-talk conversations.\n",
      "\n",
      "Question: How do I check Job Status?\n",
      "Sources:\n",
      "---------------------\n",
      "    Content:  reasons include using too much memory, too many cores, or running past a job’s timelimit.\n",
      "You can run sacct:\n",
      "[user@login-00 ~] sacct\n",
      "       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode\n",
      "------------ ---------- ---------- ---------- ---------- ---------- --------\n",
      "159637       ompi_char+   parallel  hpc_admin         80  COMPLETED      0:0\n",
      "159637.batch      batch             hpc_admin          1  COMPLETED      0:0\n",
      "159637.0          orted             hpc_admin          3  COMPLETED      0:0\n",
      "159638       ompi_char+   parallel  hpc_admin        400    TIMEOUT      0:1\n",
      "159638.batch      batch             hpc_admin          1  CANCELLED     0:15\n",
      "159638.0          orted             hpc_admin         19  CANCELLED  255:126\n",
      "\n",
      "\n",
      "If it’s still not clear why your job was killed, please contact us and send us the output from sacct.\n",
      "\n",
      "\n",
      "\n",
      "How can I submit a job to the HPC cluster?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Jobs can be submitted with sbatch and srun to the HPC.\n",
      "\n",
      "See also\n",
      "Batch Jobs: sbatch and Interactive Jobs: srun Command\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "How can I check the status of my job?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "You can check the status of your job using the squeue -u $USER command, which will display your\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/faq.html\n",
      "\n",
      "Content:  only run on the CPUs associated with the GPU you’re running on. Specifying the –exclusive flag in your job script or requesting all of the node’s CPUs will not change this. If you would like to use all cores on a node with one of the GPUs, you must specify this in your Slurm script: #SBATCH –gres-flags=disable-binding\n",
      "Refer to the Slurm documentation for further information.\n",
      "\n",
      "\n",
      "\n",
      "How can I get information on the HPC such as how busy it is?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "From a login node, you can run sinfo -p short to get the state of the HPC.\n",
      "\n",
      "See also\n",
      "Monitoring and Managing Jobs\n",
      "\n",
      "\n",
      "Back to the top\n",
      "\n",
      "\n",
      "Data Transfer#\n",
      "\n",
      "\n",
      "How can I transfer data to/from the HPC cluster?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Data transfer can be done using various methods like scp, rsync, or Globus. Refer to the ‘Transferring Data’ section in ‘Data Management’ for detailed instructions.\n",
      "\n",
      "See also\n",
      "Data Storage Options\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "What Linux commands can I use to transfer files to/from the cluster?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Smaller files can be transferred to/from the cluster using scp, sftp, and rsync as well as standard FTP tools utilizing xfer.discovery.neu.edu.\n",
      "Larger files should be moved using Globus.\n",
      "\n",
      "See also\n",
      "Data Storage Options\n",
      "\n",
      "\n",
      "Back to the top\n",
      "\n",
      "\n",
      "Job Management#\n",
      "\n",
      "\n",
      "How do I submit jobs?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "You submit jobs by writing a Slurm script and submitting it with the sbatch command. Please see our Slurm documentation page.\n",
      "\n",
      "See also\n",
      "Batch Jobs: sbatch\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "How do I submit an interactive job?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For an interactive job on the command line, submit an srun job from the login node. Examples can be seen at Interactive Jobs: srun Command\n",
      "If you wish to run a program that requires a graphical user interface or generates other graphics for display, such as a plot or chemical model, use one of the Open OnDemand interactive apps. Several are available, but if you one you wish to use isn’t in the list, submit an OOD Desktop job. You can also use X11 forwarding with srun.\n",
      "\n",
      "See also\n",
      "Introduction to OOD\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "What partitions\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/faq.html\n",
      "\n",
      "Content:  may take up to 24 hours after your sponsor approves it (see Sponsor Approval Process below). You will receive an email confirmation when your access has been granted. Once you have access, if you are unfamiliar with Discovery, high-performance computing, or Linux, you may want to take one of our training courses. Visit the Research Computing website for more information about our training and services.\n",
      "\n",
      "Sponsor Approval Process#\n",
      "\n",
      "PI and instructor access\n",
      "If you are a PI, professor, or instructor at Northeastern and need access to the cluster, use the access form in the above procedure and enter your name in the Sponsor Name field.\n",
      "\n",
      "HPC users need a sponsor, usually a NU PI or professor, to approve their request. PIs, professors, and instructors can sponsor themselves. Students (undergraduate or graduate), visiting researchers, or staff members must have a sponsor approve their request. When you fill out the ServiceNow form, an email is sent to the specified sponsor upon submitting the request. Sponsors will receive email reminders until they approve the request through the link in the email to ServiceNow. We recommend letting your sponsor know to look for the email with the approval link before submitting an access request.\n",
      "\n",
      "\n",
      "\n",
      "Cluster Usage#\n",
      "DO NOT USE the login node for CPU-intensive activities, as this will impact the performance of this node for all cluster users. It will also not provide the best performance for the tasks you are trying to accomplish.\n",
      "\n",
      "See also\n",
      "Connecting To Cluster\n",
      "\n",
      "\n",
      "Important\n",
      "If you are attempting to run a job, you should move to a compute node. You can do this interactively using the srun command or non-interactively using the sbatch command.\n",
      "\n",
      "See also\n",
      "Batch Jobs: sbatch and Interactive Jobs: srun Command for more information.\n",
      "\n",
      "If you are attempting to transfer data, we have a dedicated transfer node that you should use.\n",
      "\n",
      "See also\n",
      "Transfer Data.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Routine Cluster Maintenance#\n",
      "Routine cluster maintenance is performed on the first Tuesday of each month. RC sends maintenance emails to inform users of upcoming maintenance window, a description of the maintenance, and how users will be affected.\n",
      "\n",
      "See also\n",
      "Users can also check the maintenance status via [IT Maintenance and Status page].\n",
      "\n",
      "\n",
      "\n",
      "MGHPCC Annual Shutdown#\n",
      "The Massachusetts Green High Performance Computing Center (MGHPCC) conducts an annual shutdown for maintenance work.\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/gettingstarted/get_access.html\n",
      "---------------------\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: What is the Scheduling Policies for HPC cluster?\n",
      "Assistant: The scheduling policies for an HPC cluster include:\n",
      "1. Fair Share: A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.\n",
      "2. FIFO (First-In-First-Out): Jobs are scheduled in the order they are submitted, with no consideration for job priority or resource requirements.\n",
      "3. Priority-Based: Jobs are executed based on their priority, which can be determined by user-defined priority, resource requirements, and walltime limit.\n",
      "4. Backfilling: This policy allows smaller jobs to \"backfill\" into available resources ahead of larger jobs, optimizing resource utilization and reducing wait times.\n",
      "5. Preemption: Higher-priority jobs can preempt lower-priority ones, temporarily pausing the lower-priority job's execution to make resources available for the higher-priority job.\n",
      "\n",
      "SOURCES:\n",
      "- Source 1: https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\n",
      "- Source 2: https://rc-docs.northeastern.edu/en/latest/glossary.html\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Question from user :  How do I check Job Status? \n",
      "\n",
      "Reply from ChatBot :  Sorry, but I can't provide any information on scheduling policies for HPC clusters based on the given sources.\n"
     ]
    }
   ],
   "source": [
    "query = \"How do I check Job Status?\"\n",
    "result = chain({\"question\": query})\n",
    "\n",
    "\n",
    "print(\"Question from user : \" , query ,\"\\n\")\n",
    "print(\"Reply from ChatBot : \" , result['answer'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nLT4uLaXZCR_",
    "outputId": "eb839c76-9585-4665-d251-64245f03f0ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain.memory.token_buffer.ConversationTokenBufferMemory"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "xoAgKYZNZJ1s",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "98ba51d6-d959-4f17-c2e0-72880d02d197",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationTokenBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='What is the Scheduling Policies for HPC cluster?'), AIMessage(content='The scheduling policies for an HPC cluster include:\\n1. Fair Share: A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.\\n2. FIFO (First-In-First-Out): Jobs are scheduled in the order they are submitted, with no consideration for job priority or resource requirements.\\n3. Priority-Based: Jobs are executed based on their priority, which can be determined by user-defined priority, resource requirements, and walltime limit.\\n4. Backfilling: This policy allows smaller jobs to \"backfill\" into available resources ahead of larger jobs, optimizing resource utilization and reducing wait times.\\n5. Preemption: Higher-priority jobs can preempt lower-priority ones, temporarily pausing the lower-priority job\\'s execution to make resources available for the higher-priority job.\\n\\nSOURCES:\\n- Source 1: https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\\n- Source 2: https://rc-docs.northeastern.edu/en/latest/glossary.html'), HumanMessage(content='How do I check Job Status?'), AIMessage(content=\"Sorry, but I can't provide any information on scheduling policies for HPC clusters based on the given sources.\")]), input_key='question', return_messages=True, llm=ChatOpenAI(verbose=True, callbacks=[<langchain.callbacks.stdout.StdOutCallbackHandler object at 0x2ba2a94d9730>], client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', streaming=True), memory_key='chat_history', max_token_limit=1000)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "5mrfHVdAZQer"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nConversation Memory Buffer & Input Details:\\n------------------------------------------\\n- Input Key: question\\n- Memory Key: chat_history\\n- Maximum Token Limit: 1000\\n- Return Messages: True\\n\\nChatOpenAI Configuration:\\n-------------------------\\n- Verbose: True\\n- Streaming: True\\n- OpenAI API Key: [REDACTED]\\n- OpenAI API Base: (No Value Provided)\\n- OpenAI Organization: (No Value Provided)\\n- OpenAI Proxy: (No Value Provided)\\n- Client: ChatCompletion\\n- Callbacks: FlyteCallbackHandler (Object Reference: 0x7832be39c9d0)\\n\\nConversation History:\\n---------------------\\n1. User: What is Globus at Northeastern?\\n\\n   Assistant: Globus is a data management system that allows you to transfer and share files.\\n   Northeastern has a subscription to Globus, and you can set up a Globus account using your Northeastern credentials.\\n   You can also link your other accounts, whether they are personal or from another institution, with your Globus account.\\n   To use Globus, you will need to set up an account and install Globus Connect on your local computer.\\n   After completing these initial setup procedures, you can use the Globus web app to perform file transfers.\\n\\n   Sources:\\n   - RCDocs (Using Globus)\\n   - RCDocs (Globus Account Set Up)\\n   - RCDocs (To use Globus)\\n\\n2. User: How do i set up an account with Globus?\\n\\n   Assistant: Globus is a data management system that allows you to transfer and share files.\\n   Northeastern has a subscription to Globus, and you can set up an account with Globus using your Northeastern credentials.\\n   You can also link your other accounts, whether they are personal or from another institution, with your Globus account.\\n   To set up an account with Globus, follow these steps:\\n   1. Go to the Globus website.\\n   2. Click on \"Log In\".\\n   3. Select \"Northeastern University\" from the options under \"Use your existing organizational login\" and click \"Continue\".\\n   4. Enter your Northeastern username and password.\\n   5. If you don\\'t have a previous Globus account, click \"Continue\". If you have an existing account, click \"Link to an existing account\".\\n   6. Check the agreement checkbox and click \"Continue\".\\n   7. Click \"Allow\" to permit Globus to access your files.\\n   After setting up your account, you can access the Globus File Manager app.\\n\\n   Sources:\\n   - RCDocs (Using Globus)\\n   - RCDocs (Globus Account Set Up)\\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Conversation Memory Buffer & Input Details:\n",
    "------------------------------------------\n",
    "- Input Key: question\n",
    "- Memory Key: chat_history\n",
    "- Maximum Token Limit: 1000\n",
    "- Return Messages: True\n",
    "\n",
    "ChatOpenAI Configuration:\n",
    "-------------------------\n",
    "- Verbose: True\n",
    "- Streaming: True\n",
    "- OpenAI API Key: [REDACTED]\n",
    "- OpenAI API Base: (No Value Provided)\n",
    "- OpenAI Organization: (No Value Provided)\n",
    "- OpenAI Proxy: (No Value Provided)\n",
    "- Client: ChatCompletion\n",
    "- Callbacks: FlyteCallbackHandler (Object Reference: 0x7832be39c9d0)\n",
    "\n",
    "Conversation History:\n",
    "---------------------\n",
    "1. User: What is Globus at Northeastern?\n",
    "\n",
    "   Assistant: Globus is a data management system that allows you to transfer and share files.\n",
    "   Northeastern has a subscription to Globus, and you can set up a Globus account using your Northeastern credentials.\n",
    "   You can also link your other accounts, whether they are personal or from another institution, with your Globus account.\n",
    "   To use Globus, you will need to set up an account and install Globus Connect on your local computer.\n",
    "   After completing these initial setup procedures, you can use the Globus web app to perform file transfers.\n",
    "\n",
    "   Sources:\n",
    "   - RCDocs (Using Globus)\n",
    "   - RCDocs (Globus Account Set Up)\n",
    "   - RCDocs (To use Globus)\n",
    "\n",
    "2. User: How do i set up an account with Globus?\n",
    "\n",
    "   Assistant: Globus is a data management system that allows you to transfer and share files.\n",
    "   Northeastern has a subscription to Globus, and you can set up an account with Globus using your Northeastern credentials.\n",
    "   You can also link your other accounts, whether they are personal or from another institution, with your Globus account.\n",
    "   To set up an account with Globus, follow these steps:\n",
    "   1. Go to the Globus website.\n",
    "   2. Click on \"Log In\".\n",
    "   3. Select \"Northeastern University\" from the options under \"Use your existing organizational login\" and click \"Continue\".\n",
    "   4. Enter your Northeastern username and password.\n",
    "   5. If you don't have a previous Globus account, click \"Continue\". If you have an existing account, click \"Link to an existing account\".\n",
    "   6. Check the agreement checkbox and click \"Continue\".\n",
    "   7. Click \"Allow\" to permit Globus to access your files.\n",
    "   After setting up your account, you can access the Globus File Manager app.\n",
    "\n",
    "   Sources:\n",
    "   - RCDocs (Using Globus)\n",
    "   - RCDocs (Globus Account Set Up)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jo16MuocEVXD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "OqU88pUbEVS1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-14 02:37:29.828907: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-14 02:37:31.999630: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-10-14 02:37:36.670689: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /device:GPU:0 with 1 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:af:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/device:GPU:0']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Chatbot Environment",
   "language": "python",
   "name": "chatbot_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
