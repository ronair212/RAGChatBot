{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eaa97a-ec89-4fc5-ac9c-311eec42c8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-18 05:51:47.992396: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-18 05:51:49.549059: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import langchain\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "import os\n",
    "import getpass\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import pipeline\n",
    "\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "\n",
    "from langchain.schema.vectorstore import VectorStoreRetriever\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")\n",
    "#os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OPENAI API Key:\")\n",
    "\n",
    "os.environ[\"COHERE_API_KEY\"] = \"Aj7fRPV0FBm1u6baUBuAZc5yMOvOs6krkrqVppam\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "\n",
    "urls = [\"https://rc-docs.northeastern.edu/en/latest/welcome/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/welcome/welcome.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/welcome/services.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/welcome/gettinghelp.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/welcome/introtocluster.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/welcome/casestudiesandtestimonials.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/get_access.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/accountmanager.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/connectingtocluster/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/connectingtocluster/mac.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/connectingtocluster/windows.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/first_steps/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/first_steps/passwordlessssh.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/first_steps/shellenvironment.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/first_steps/usingbash.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/hardware/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/hardware/hardware_overview.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/hardware/partitions.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/introduction.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/accessingood.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/desktopood.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/fileexplore.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/jupyterlab.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/understandingqueuing.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/workingwithgpus.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/recurringjobs.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/debuggingjobs.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/discovery_storage.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/transferringdata.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/globus.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/databackup.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/securityandcompliance.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/modules.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/mpi.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/r.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/matlab.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/packagemanagers/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/packagemanagers/conda.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/packagemanagers/spack.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/fromsource/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/fromsource/makefile.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/fromsource/cmake.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/introductiontoslurm.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmcommands.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmrunningjobs.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmmonitoringandmanaging.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmscripts.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmarray.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmbestpractices.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/classroom/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/classroom/class_use.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/classroom/cps_ood.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/classroom/classroomexamples.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/best-practices/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/best-practices/homequota.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/best-practices/checkpointing.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/best-practices/optimizingperformance.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/best-practices/software.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/tutorialsandtraining/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/tutorialsandtraining/canvasandgithub.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/faq.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/glossary.html\",\n",
    "]\n",
    "loader = WebBaseLoader(urls)\n",
    "data = loader.load()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import tiktoken\n",
    "encoding_name = tiktoken.get_encoding(\"cl100k_base\")\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "text_splitter = TokenTextSplitter(chunk_size=500, chunk_overlap=25)\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "\n",
    "EMB_INSTRUCTOR_XL = \"hkunlp/instructor-xl\"\n",
    "EMB_SBERT_MPNET_BASE = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "\n",
    "LLM_FLAN_T5_XXL = \"google/flan-t5-xxl\"\n",
    "LLM_FLAN_T5_XL = \"google/flan-t5-xl\"\n",
    "LLM_FASTCHAT_T5_XL = \"lmsys/fastchat-t5-3b-v1.0\"\n",
    "LLM_FLAN_T5_SMALL = \"google/flan-t5-small\"\n",
    "LLM_FLAN_T5_BASE = \"google/flan-t5-base\"\n",
    "LLM_FLAN_T5_LARGE = \"google/flan-t5-large\"\n",
    "LLM_FALCON_7B_INSTRUCT = \"tiiuae/falcon-7b-instruct\"\n",
    "LLM_FALCON_40B_INSTRUCT = \"tiiuae/falcon-40b-instruct\"\n",
    "LLM_FALCON_7B = \"tiiuae/falcon-7b\"\n",
    "LLM_FALCON_40B = \"tiiuae/falcon-40b\"\n",
    "LLM_LLAMA2_70B_INSTRUCT = \"upstage/Llama-2-70b-instruct\" \n",
    "LLM_LLAMA2_7B_32K_INSTRUCT = \"togethercomputer/Llama-2-7B-32K-Instruct\"#[INST]\\n<your instruction here>\\n[\\INST]\\n\\n\n",
    "\n",
    "cache_dir='/work/rc/projects/chatbot/models'\n",
    "\n",
    "config = {\"persist_directory\":None,\n",
    "          \"load_in_8bit\":False,\n",
    "          \"llm\":LLM_LLAMA2_7B_32K_INSTRUCT,\n",
    "          }\n",
    "\n",
    "\n",
    "\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/work/rc/projects/chatbot/models'\n",
    "os.environ['PYTORCH_TRANSFORMERS_CACHE'] = '/work/rc/projects/chatbot/models'\n",
    "#os.environ['SENTENCE_TRANSFORMERS_HOME'] = '/work/rc/projects/chatbot/models'\n",
    "os.environ['HUGGINGFACE_HUB_CACHE']  = '/work/rc/projects/chatbot/models'\n",
    "\n",
    "\n",
    "\n",
    "def create_llama2_70b_instruct(load_in_8bit=True):\n",
    "        #model = LLM_LLAMA2_70B_INSTRUCT\n",
    "        #tokenizer = AutoTokenizer.from_pretrained(model , cache_dir=\"/work/rc/projects/chatbot/models\")\n",
    "        model_name = LLM_LLAMA2_70B_INSTRUCT\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=\"/work/rc/projects/chatbot/models\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name , cache_dir=\"/work/rc/projects/chatbot/models\")\n",
    "        hf_pipeline = pipeline(\n",
    "                task=\"text-generation\",\n",
    "                model = model,\n",
    "                do_sample=True, #Whether or not to use sampling ; use greedy decoding otherwise.\n",
    "                tokenizer = tokenizer,\n",
    "                #trust_remote_code = True,\n",
    "                max_new_tokens=500, #The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n",
    "                #cache_dir=cache_dir,\n",
    "                model_kwargs={\n",
    "                    \"device_map\": \"auto\", \n",
    "                    \"load_in_8bit\": load_in_8bit, \n",
    "                    \"max_length\": 512, \n",
    "                    \"temperature\": 0.01,\n",
    "                    \n",
    "                    \"torch_dtype\":torch.bfloat16,\n",
    "                    }\n",
    "            )\n",
    "        return hf_pipeline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_falcon_40b(load_in_8bit=True):\n",
    "        #model = LLM_FALCON_40B\n",
    "        #tokenizer = AutoTokenizer.from_pretrained(model , cache_dir=\"/work/rc/projects/chatbot/models\")\n",
    "        model_name = LLM_FALCON_40B\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=\"/work/rc/projects/chatbot/models\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name , cache_dir=\"/work/rc/projects/chatbot/models\")\n",
    "        hf_pipeline = pipeline(\n",
    "                task=\"text-generation\",\n",
    "                model = model,\n",
    "                do_sample=True, #Whether or not to use sampling ; use greedy decoding otherwise.\n",
    "                tokenizer = tokenizer,\n",
    "                #trust_remote_code = True,\n",
    "                max_new_tokens=500, #The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n",
    "                #cache_dir=cache_dir,\n",
    "                model_kwargs={\n",
    "                    \"device_map\": \"auto\", \n",
    "                    \"load_in_8bit\": load_in_8bit, \n",
    "                    \"max_length\": 512, \n",
    "                    \"temperature\": 0.01,\n",
    "                    \n",
    "                    \"torch_dtype\":torch.bfloat16,\n",
    "                    }\n",
    "            )\n",
    "        return hf_pipeline\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def create_falcon_40b_instruct(load_in_8bit=True):\n",
    "        #model = LLM_FALCON_40B_INSTRUCT\n",
    "        model_name = LLM_FALCON_40B_INSTRUCT\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=\"/work/rc/projects/chatbot/models\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name , cache_dir=\"/work/rc/projects/chatbot/models\")\n",
    "        #tokenizer = AutoTokenizer.from_pretrained(model , cache_dir=\"/work/rc/projects/chatbot/models\")\n",
    "        hf_pipeline = pipeline(\n",
    "                task=\"text-generation\",\n",
    "                model = model,\n",
    "                do_sample=True, #Whether or not to use sampling ; use greedy decoding otherwise.\n",
    "                tokenizer = tokenizer,\n",
    "                #trust_remote_code = True,\n",
    "                max_new_tokens=100, #The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n",
    "                #cache_dir=cache_dir,\n",
    "                model_kwargs={\n",
    "                    \"device_map\": \"auto\", \n",
    "                    \"load_in_8bit\": load_in_8bit, \n",
    "                    \"max_length\": 512, \n",
    "                    \"temperature\": 0.01,\n",
    "                    \n",
    "                    \"torch_dtype\":torch.bfloat16,\n",
    "                    }\n",
    "            )\n",
    "        return hf_pipeline\n",
    "\n",
    "\n",
    "def create_falcon_7b(load_in_8bit=True):\n",
    "        #model = LLM_FALCON_7B\n",
    "        #tokenizer = AutoTokenizer.from_pretrained(model , cache_dir=\"/work/rc/projects/chatbot/models\")\n",
    "        model_name = LLM_FALCON_7B\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=\"/work/rc/projects/chatbot/models\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name , cache_dir=\"/work/rc/projects/chatbot/models\")\n",
    "        hf_pipeline = pipeline(\n",
    "                task=\"text-generation\",\n",
    "                model = model,\n",
    "                do_sample=True, #Whether or not to use sampling ; use greedy decoding otherwise.\n",
    "                tokenizer = tokenizer,\n",
    "                #trust_remote_code = True,\n",
    "                max_new_tokens=100, #The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n",
    "                #cache_dir=cache_dir,\n",
    "                model_kwargs={\n",
    "                    \"device_map\": \"auto\", \n",
    "                    \"load_in_8bit\": load_in_8bit, \n",
    "                    \"max_length\": 512, \n",
    "                    \"temperature\": 0.01,\n",
    "                    \n",
    "                    \"torch_dtype\":torch.bfloat16,\n",
    "                    }\n",
    "            )\n",
    "        return hf_pipeline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_falcon_7b_instruct(load_in_8bit=True):\n",
    "        #model = LLM_FALCON_7B_INSTRUCT\n",
    "        model_name = LLM_FALCON_7B_INSTRUCT\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=\"/work/rc/projects/chatbot/models\")\n",
    "        #tokenizer = AutoTokenizer.from_pretrained(model , cache_dir=\"/work/rc/projects/chatbot/models\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name , cache_dir=\"/work/rc/projects/chatbot/models\")\n",
    "        hf_pipeline = pipeline(\n",
    "                task=\"text-generation\",\n",
    "                model = model,\n",
    "                do_sample=True, #Whether or not to use sampling ; use greedy decoding otherwise.\n",
    "                tokenizer = tokenizer,\n",
    "                #trust_remote_code = True,\n",
    "                max_new_tokens=500, #The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n",
    "                #cache_dir=\"/work/rc/projects/chatbot/models\",\n",
    "                model_kwargs={\n",
    "                    \"device_map\": \"auto\", \n",
    "                    \"load_in_8bit\": load_in_8bit, \n",
    "                    \"max_length\": 512, \n",
    "                    \"temperature\": 0.01,\n",
    "                    \"torch_dtype\":torch.bfloat16,\n",
    "                    }\n",
    "            )\n",
    "        return hf_pipeline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_flan_t5_base(load_in_8bit=True):\n",
    "        # Wrap it in HF pipeline for use with LangChain\n",
    "        model=\"google/flan-t5-base\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model, cache_dir=\"/work/rc/projects/chatbot/models\")\n",
    "        return pipeline(\n",
    "            task=\"text2text-generation\",\n",
    "            model=model,\n",
    "            tokenizer = tokenizer,\n",
    "            max_new_tokens=100,\n",
    "            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.}\n",
    "        )\n",
    "        \n",
    "\n",
    "load_in_8bit = config[\"load_in_8bit\"]\n",
    "if config[\"llm\"] == LLM_FLAN_T5_BASE:\n",
    "    llm = create_flan_t5_base(load_in_8bit=load_in_8bit)\n",
    "    \n",
    "\n",
    "load_in_8bit = config[\"load_in_8bit\"]\n",
    "\n",
    "if config[\"llm\"] == LLM_FALCON_40B:\n",
    "    llm = create_falcon_40b(load_in_8bit=load_in_8bit)\n",
    "    \n",
    "load_in_8bit = config[\"load_in_8bit\"]\n",
    "\n",
    "if config[\"llm\"] == LLM_FALCON_40B_INSTRUCT:\n",
    "    llm = create_falcon_40b_instruct(load_in_8bit=load_in_8bit)\n",
    "\n",
    "load_in_8bit = config[\"load_in_8bit\"]\n",
    "\n",
    "if config[\"llm\"] == LLM_FALCON_7B_INSTRUCT:\n",
    "    llm = create_falcon_7b_instruct(load_in_8bit=load_in_8bit)\n",
    "    \n",
    "load_in_8bit = config[\"load_in_8bit\"]\n",
    "\n",
    "if config[\"llm\"] == LLM_LLAMA2_70B_INSTRUCT:\n",
    "    llm = create_llama2_70b_instruct(load_in_8bit=load_in_8bit)\n",
    "    \n",
    "\n",
    "load_in_8bit = config[\"load_in_8bit\"]\n",
    "\n",
    "if config[\"llm\"] == LLM_FALCON_7B:\n",
    "    llm = create_falcon_7b(load_in_8bit=load_in_8bit)\n",
    "\n",
    "\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "\n",
    "Qdrantdb = Qdrant.from_documents(\n",
    "    docs,\n",
    "    embeddings,\n",
    "    path=\"/work/rc/projects/chatbot/chatbotrc/notebooks/RAG/tmp/local_qdrant\",\n",
    "    collection_name=\"RC_documents\",\n",
    ")\n",
    "\n",
    "\n",
    "hf_llm = HuggingFacePipeline(pipeline = llm, model_kwargs = {'temperature':0})\n",
    "\n",
    "\n",
    "retriever = VectorStoreRetriever(vectorstore=Qdrantdb, search_type=\"mmr\", search_kwargs={'k': 4, 'fetch_k': 10},)\n",
    "compressor = CohereRerank() \n",
    "\n",
    "reranker= ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "\n",
    "\n",
    "context_callback = StdOutCallbackHandler()\n",
    "memory = ConversationTokenBufferMemory(llm=hf_llm,memory_key=\"chat_history\", return_messages=True,input_key='question',max_token_limit=1000)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PromptTemplates = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"question\"],\n",
    "    template=\"\"\"\n",
    "Below is a summary of the conversation so far, and a new question asked by the user that needs to be answered by searching in a knowledge base.\n",
    "Generate a search query based on the conversation and the new question. Frame the question in a way to get good similarity search results from a vector database.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Search query:\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "system_message_prompt = SystemMessagePromptTemplate(prompt=PromptTemplates)\n",
    "\n",
    "chat_prompt_for_ques = ChatPromptTemplate.from_messages(\n",
    "    [system_message_prompt])\n",
    "\n",
    "\n",
    "question_generator = LLMChain(llm=hf_llm, prompt=chat_prompt_for_ques, verbose=True)\n",
    "\n",
    "\n",
    "Answer_Generator_Prompt= '''\n",
    "<Instructions>\n",
    "Important:\n",
    "Answer with the facts listed in the list of sources below. If there isn't enough information below, say you don't know.\n",
    "If asking a clarifying question to the user would help, ask the question.\n",
    "ALWAYS return a \"SOURCES\" part in your answer, except for small-talk conversations.\n",
    "\n",
    "Question: {question}\n",
    "Sources:\n",
    "---------------------\n",
    "    {summaries}\n",
    "---------------------\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "chat_prompt = PromptTemplate(template=Answer_Generator_Prompt, input_variables=[\"question\", \"summaries\",\"chat_history\"])\n",
    "\n",
    "answer_chain = load_qa_with_sources_chain(hf_llm, chain_type=\"stuff\", verbose=True,prompt=chat_prompt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "chain = ConversationalRetrievalChain(\n",
    "            retriever=reranker,\n",
    "            question_generator=question_generator,\n",
    "            combine_docs_chain=answer_chain,\n",
    "            verbose=True,\n",
    "            memory=memory,\n",
    "            rephrase_question=False\n",
    ")\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Assistant: Hey there, I am a chat assistant. What can I help you with today?\")\n",
    "\n",
    "    while True:\n",
    "        query = input(\"User: \")\n",
    "        if query.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "            print(\"Assistant: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        #prompt = f\"User: {query}\\nAssistant:\"\n",
    "        assistant_response = chain({\"question\": query})\n",
    "        print(f\"Assistant: {assistant_response['answer']}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc30871f-485c-4439-b69e-bace00bb1be4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Chatbot Environment",
   "language": "python",
   "name": "chatbot_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
