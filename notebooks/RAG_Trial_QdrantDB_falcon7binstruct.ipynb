{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cad0f427-6c42-4acc-b11b-afac75d65860",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation using ChromaDB and FALCON_7B\n",
    "\n",
    "## Overview\n",
    "\n",
    "Navigating through tokenization to intelligent query processing, this documentation unveils a structured approach to text management and model utilization. Initially, it leverages `tiktoken` for precise tokenization and employs strategic text splitters to ensure optimal text segmentation. Subsequently, a selection of pre-trained transformer models like `SBERT MPNet` and `FALCON_7B` are integrated and configured via meticulously crafted functions and a flexible configuration dictionary. The process culminates by intertwining embedding retrievers with initialized Language Models, establishing a retrieval-based Question Answering system that adeptly navigates user queries, showcasing a judicious amalgamation of structured text management and intelligent data querying in Natural Language Processing applications.\n",
    "\n",
    "\n",
    "\n",
    "## 1. Tokenization and Document Splitting\n",
    "\n",
    "### Token Counting\n",
    "A function named `num_tokens_from_string` utilizes `tiktoken` to calculate and return the number of tokens in a given string. It accepts the text and an encoding name as input arguments, using them to encode the text and return its token length.\n",
    "\n",
    "### Text Splitting\n",
    "- `TokenTextSplitter`: Splits texts into chunks with specified sizes and overlaps.\n",
    "- `RecursiveCharacterTextSplitter`: Further divides texts with considerations for character count, overlap, and potential additional metadata.\n",
    "  \n",
    "Both splitters aim to break down text into manageable sizes for subsequent processing, ensuring that models can handle them within their token limits.\n",
    "\n",
    "## 2. Model Definitions and Setup\n",
    "\n",
    "The code incorporates various pre-trained transformer models for embeddings and Language Model (LM) generation. Model identifiers and a caching directory are specified at the beginning of this section.\n",
    "\n",
    "### Models Used\n",
    "- `EMB_SBERT_MPNET_BASE`: Sentence transformer model for embeddings.\n",
    "- `EMB_INSTRUCTOR_XL`: Not utilized in the provided code.\n",
    "- `LLM_FALCON_7B` and `LLM_FALCON_40B`: Pre-trained transformer models for text generation.\n",
    "\n",
    "### Cache Directory Setup\n",
    "The cache directory (`/work/rc/projects/chatbot/models`) is set in the environment variables to store downloaded model weights, ensuring they are readily available for subsequent runs.\n",
    "\n",
    "## 3. Model Creators\n",
    "\n",
    "The code defines several functions to create models and pipelines, notably:\n",
    "- `create_sbert_mpnet()`: Initializes the SBERT MPNet model.\n",
    "- `create_falcon_40b_instruct()` and `create_falcon_7b_instruct()`: Set up models for text generation via Hugging Face’s pipeline, configuring tokenizers and various model arguments.\n",
    "- `create_flan_t5_base()`: Sets up a T5 model pipeline from Google for text-to-text generation.\n",
    "\n",
    "These functions handle the instantiation and configuration of the models, ensuring they are set up with the appropriate parameters and caching.\n",
    "\n",
    "## 4. Model and Pipeline Configuration \n",
    "\n",
    "A configuration dictionary `config` holds keys for adjusting model parameters and selection. Depending on this configuration:\n",
    "- The corresponding embedding model is initialized.\n",
    "- One of the LLM models (Falcon or T5) is chosen and instantiated based on the specified parameters.\n",
    "\n",
    "## 5. Data Processing and Question Answering Setup \n",
    "\n",
    "Here, a sample data string `data` is defined and split into documents using the earlier mentioned text splitter. Then, it sets up the embeddings and retrieval-based Question Answering (QA) system. \n",
    "\n",
    "### RetrievalQA Setup\n",
    "- An instance of `HuggingFacePipeline` is initialized using the previously created Language Models.\n",
    "- The embedding model's retriever is configured.\n",
    "- The QA model is built using the retriever and the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f97713d5-0e72-48b2-9d3a-2c5e883ded11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MNSwbz8svgCi",
    "outputId": "edbdd442-5caa-4c86-b902-da4a5f72f106"
   },
   "outputs": [],
   "source": [
    "#!pip install pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d545559d-fc07-4386-bf1f-c7ba2ffff1b3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MuNwBfb6BngZ",
    "outputId": "30409fdb-2773-411e-dcfc-170ab7fc63f7"
   },
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52313acc-a137-4988-8c60-b54bf031560e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nwJCPFhtCBmi",
    "outputId": "aa6b6914-32a8-4f4d-e68c-84e5077d0fad"
   },
   "outputs": [],
   "source": [
    "#!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f33747f5-5b69-4ec3-9fb0-4014e1b516d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vwqNT7T0CF-O",
    "outputId": "9fc123df-4163-4f34-f81f-610513bc1fe5"
   },
   "outputs": [],
   "source": [
    "#!pip install cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbb2a466-1414-48a1-92e5-aee7a157c1df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QQl_Bfl8CLL1",
    "outputId": "cc07de5a-2e7c-4b28-a66d-5906fea3a052"
   },
   "outputs": [],
   "source": [
    "#!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c4736a2-64e2-4100-9ec1-8293d9ba1b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c96bce9f-2e23-4b3c-b416-a8308202086a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cc597bf-9b24-460a-91bf-e303782fe92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1263a695-a3ce-48b5-97ba-42e6d5581a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Cohere API Key: ········································\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c009ffff-5efc-46f9-9f84-ae5e78e20dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OPENAI API Key: ···················································\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OPENAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "608c8ac9-1f4c-409e-91e5-c0ce1cf9ff87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04c91c45-ee77-42e4-84aa-6770a18c93c5",
   "metadata": {
    "id": "mq7rbJww6Cjo"
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "\n",
    "urls = [\"https://rc-docs.northeastern.edu/en/latest/welcome/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/welcome/welcome.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/welcome/services.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/welcome/gettinghelp.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/welcome/introtocluster.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/welcome/casestudiesandtestimonials.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/get_access.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/accountmanager.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/connectingtocluster/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/connectingtocluster/mac.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/connectingtocluster/windows.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/first_steps/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/first_steps/passwordlessssh.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/first_steps/shellenvironment.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/first_steps/usingbash.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/hardware/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/hardware/hardware_overview.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/hardware/partitions.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/introduction.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/accessingood.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/desktopood.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/fileexplore.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/jupyterlab.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/understandingqueuing.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/workingwithgpus.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/recurringjobs.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/debuggingjobs.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/discovery_storage.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/transferringdata.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/globus.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/databackup.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/securityandcompliance.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/modules.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/mpi.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/r.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/matlab.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/packagemanagers/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/packagemanagers/conda.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/packagemanagers/spack.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/fromsource/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/fromsource/makefile.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/fromsource/cmake.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/introductiontoslurm.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmcommands.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmrunningjobs.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmmonitoringandmanaging.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmscripts.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmarray.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmbestpractices.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/classroom/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/classroom/class_use.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/classroom/cps_ood.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/classroom/classroomexamples.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/best-practices/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/best-practices/homequota.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/best-practices/checkpointing.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/best-practices/optimizingperformance.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/best-practices/software.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/tutorialsandtraining/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/tutorialsandtraining/canvasandgithub.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/faq.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/glossary.html\",\n",
    "]\n",
    "loader = WebBaseLoader(urls)\n",
    "data = loader.load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcdc2770-3a12-4506-9798-d7eee8501654",
   "metadata": {
    "id": "6twYZJo26GmB"
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "encoding_name = tiktoken.get_encoding(\"cl100k_base\")\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f98e16e5-1a1d-41ce-98cf-5432bb997295",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "9b3T5m1YBVrf",
    "outputId": "4f06dc21-756e-4dd9-f462-c503f4dd675b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\n\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    chunk_size = 700,\\n    chunk_overlap  = 70,\\n    length_function = len,\\n    add_start_index = True,\\n)\\ndocs = text_splitter.create_documents([data])\\n\\nfor idx, text in enumerate(docs):\\n    docs[idx].metadata[\\'source\\'] = \"RCDocs\"\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "text_splitter = TokenTextSplitter(chunk_size=500, chunk_overlap=25)\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "'''\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 700,\n",
    "    chunk_overlap  = 70,\n",
    "    length_function = len,\n",
    "    add_start_index = True,\n",
    ")\n",
    "docs = text_splitter.create_documents([data])\n",
    "\n",
    "for idx, text in enumerate(docs):\n",
    "    docs[idx].metadata['source'] = \"RCDocs\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34c44c26-868e-4d12-b739-aeb81ee76da8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_DfcNy9sT3jj",
    "outputId": "36618e1f-e92c-4ed3-f051-980b780bc57b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain.schema.document.Document"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d6b0f13-2dad-4e7e-b496-bd2193c5bfa2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PILXy2pdVC4v",
    "outputId": "99fdfd5c-a9f0-422a-dded-c4c33e642735"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='\\n\\n\\n\\n\\n\\n\\nResearch Computing - RC RTD\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nContents\\n\\n\\n\\n\\n\\nMenu\\n\\n\\n\\n\\n\\n\\n\\nExpand\\n\\n\\n\\n\\n\\nLight mode\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDark mode\\n\\n\\n\\n\\n\\n\\nAuto light/dark mode\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHide navigation sidebar\\n\\n\\nHide table of contents sidebar\\n\\n\\n\\n\\n\\nToggle site navigation sidebar\\n\\n\\n\\n\\nRC RTD\\n\\n\\n\\n\\nToggle Light / Dark / Auto color theme\\n\\n\\n\\n\\n\\n\\nToggle table of contents sidebar\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nResearch ComputingToggle child pages in navigation\\nWelcome\\nServices We Provide\\nGetting Help\\nIntroduction to HPC and Slurm\\nCase Studies and User Testimonials\\n\\n\\n\\n\\nGetting StartedToggle child pages in navigation\\nGetting Access\\nAccount Manager\\nConnecting To ClusterToggle child pages in navigation\\nMac\\nWindows\\n\\n\\n\\n\\nFirst StepsToggle child pages in navigation\\nPasswordless SSH\\nShell Environment on the Cluster\\nCluster via Command-Line\\n\\n\\n\\nUser Guides\\n\\nHardwareToggle child pages in navigation\\nOverview\\nPartitions\\n\\n\\nOpen OnDemand (OOD)Toggle child pages in navigation\\nIntroduction to OOD\\nAccessing Open OnDemand\\nInteractive Open OnDemand ApplicationsToggle child pages in navigation\\nDesktop App\\nOOD File Explorer\\nJupyterLab\\nStata\\n\\n\\n\\n\\nRunning JobsToggle child pages in navigation\\nUnderstanding the Queuing System\\nJob Scheduling Policies and Priorities\\nInteractive and Batch Mode\\nWorking with GPUs\\nRecurring Jobs\\nDebugging and Troubleshooting Jobs\\n\\n\\nData ManagementToggle child pages in navigation\\nData Storage Options\\nTransfer Data\\nUsing Globus\\nData Backup and Restore\\nSecurity and Compliance\\n\\n\\nSoftwareToggle child pages in navigation\\nSystem WideToggle child pages in navigation\\nModules\\nMPI\\nR\\nMatlab\\n\\n\\nPackage ManagersToggle child pages in navigation\\nConda\\nSpack\\n\\n\\nFrom SourceToggle child pages in navigation\\nMake\\nCMake\\n\\n\\n\\n\\nSlurmToggle child pages in navigation\\nIntroduction to Slurm\\nSlurm Commands\\nSlurm Running Jobs\\nMonitoring and Managing Jobs\\nSlurm Job Scripts\\nSlurm Array Jobs and Dependencies\\nSlurm Best Practices\\n\\n\\nHPC for the ClassroomToggle child pages', metadata={'source': 'https://rc-docs.northeastern.edu/en/latest/welcome/index.html', 'title': 'Research Computing - RC RTD', 'language': 'en'})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "592fd0b3-1578-4e80-ac09-a9ab6a8a7b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_INSTRUCTOR_XL = \"hkunlp/instructor-xl\"\n",
    "EMB_SBERT_MPNET_BASE = \"sentence-transformers/all-mpnet-base-v2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a9af797-f1bb-433a-b495-cc98ac36081e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_FLAN_T5_XXL = \"google/flan-t5-xxl\"\n",
    "LLM_FLAN_T5_XL = \"google/flan-t5-xl\"\n",
    "LLM_FASTCHAT_T5_XL = \"lmsys/fastchat-t5-3b-v1.0\"\n",
    "LLM_FLAN_T5_SMALL = \"google/flan-t5-small\"\n",
    "LLM_FLAN_T5_BASE = \"google/flan-t5-base\"\n",
    "LLM_FLAN_T5_LARGE = \"google/flan-t5-large\"\n",
    "LLM_FALCON_7B = \"tiiuae/falcon-7b-instruct\"\n",
    "LLM_FALCON_40B = \"tiiuae/falcon-40b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d274b67-365b-4fe8-8590-0a0d3f9b2dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir='/work/rc/projects/chatbot/models'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19979ba8-7a98-461d-abbe-54df6d71bcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"persist_directory\":None,\n",
    "          \"load_in_8bit\":False,\n",
    "          \"embedding\" : EMB_SBERT_MPNET_BASE,\n",
    "          \"llm\":LLM_FALCON_7B,\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47672542-7726-458d-8077-f02d45edd67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/work/rc/projects/chatbot/models'\n",
    "#cache_folder=os.getenv('SENTENCE_TRANSFORMERS_HOME')\n",
    "os.environ['SENTENCE_TRANSFORMERS_HOME'] = '/work/rc/projects/chatbot/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d07a707a-255f-41c7-b71e-76df7e75afab",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def create_sbert_mpnet():\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        return HuggingFaceEmbeddings(model_name=EMB_SBERT_MPNET_BASE, model_kwargs={\"device\": device})\n",
    "\n",
    "'''\n",
    "\n",
    "def create_sbert_mpnet():\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        return HuggingFaceEmbeddings(model_name=EMB_SBERT_MPNET_BASE, cache_folder=cache_dir, model_kwargs={\"device\": device})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\", cache_dir=\"new_cache_dir/\")\n",
    "\n",
    "#model = AutoModelForMaskedLM.from_pretrained(\"roberta-base\", cache_dir=\"new_cache_dir/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21f57029-387f-4901-ac8b-04b03ec03f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-13 20:41:09.541010: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-13 20:41:09.593162: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-13 20:41:12.300180: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "216b63a5-6c81-415c-8bf5-e4092448c40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_falcon_40b_instruct(load_in_8bit=False):\n",
    "        model = LLM_FALCON_40B\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model , cache_dir=cache_dir)\n",
    "        hf_pipeline = pipeline(\n",
    "                task=\"text-generation\",\n",
    "                model = model,\n",
    "                do_sample=True,\n",
    "                tokenizer = tokenizer,\n",
    "                #trust_remote_code = True,\n",
    "                max_new_tokens=100,\n",
    "                #cache_dir=cache_dir,\n",
    "                model_kwargs={\n",
    "                    \"device_map\": \"auto\", \n",
    "                    \"load_in_8bit\": load_in_8bit, \n",
    "                    \"max_length\": 512, \n",
    "                    \"temperature\": 0.01,\n",
    "                    \n",
    "                    \"torch_dtype\":torch.bfloat16,\n",
    "                    }\n",
    "            )\n",
    "        return hf_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8499ec1f-2405-42d5-a990-3eec2eb06baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_falcon_7b_instruct(load_in_8bit=False):\n",
    "        model = LLM_FALCON_7B\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model , cache_dir=cache_dir)\n",
    "        hf_pipeline = pipeline(\n",
    "                task=\"text-generation\",\n",
    "                model = model,\n",
    "                do_sample=True,\n",
    "                tokenizer = tokenizer,\n",
    "                #trust_remote_code = True,\n",
    "                max_new_tokens=100,\n",
    "                #cache_dir=cache_dir,\n",
    "                model_kwargs={\n",
    "                    \"device_map\": \"auto\", \n",
    "                    \"load_in_8bit\": load_in_8bit, \n",
    "                    \"max_length\": 512, \n",
    "                    \"temperature\": 0.01,\n",
    "                    \n",
    "                    \"torch_dtype\":torch.bfloat16,\n",
    "                    }\n",
    "            )\n",
    "        return hf_pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cac57891-9948-4517-86f0-c6bd141b5d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n WARNING: You are currently loading Falcon using legacy code contained in the model repository. \\n Falcon has now been fully ported into the Hugging Face transformers library. \\n For the most up-to-date and high-performance version of the Falcon model code, \\n please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def create_flan_t5_base(load_in_8bit=False):\n",
    "        # Wrap it in HF pipeline for use with LangChain\n",
    "        model=\"google/flan-t5-base\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model, cache_dir=cache_dir)\n",
    "        return pipeline(\n",
    "            task=\"text2text-generation\",\n",
    "            model=model,\n",
    "            tokenizer = tokenizer,\n",
    "            max_new_tokens=100,\n",
    "            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.}\n",
    "        )\n",
    "        \n",
    "'''\n",
    " WARNING: You are currently loading Falcon using legacy code contained in the model repository. \n",
    " Falcon has now been fully ported into the Hugging Face transformers library. \n",
    " For the most up-to-date and high-performance version of the Falcon model code, \n",
    " please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "85552c12-701a-4984-8436-a7273d08e7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"embedding\"] == EMB_SBERT_MPNET_BASE:\n",
    "    embedding = create_sbert_mpnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22578aea-3ace-4378-bb16-c6b1ac86bb5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_in_8bit = config[\"load_in_8bit\"]\n",
    "if config[\"llm\"] == LLM_FLAN_T5_BASE:\n",
    "    llm = create_flan_t5_base(load_in_8bit=load_in_8bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "921d211e-6c86-4de8-afc2-09659c45487b",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_in_8bit = config[\"load_in_8bit\"]\n",
    "\n",
    "if config[\"llm\"] == LLM_FALCON_40B:\n",
    "    llm = create_falcon_40b_instruct(load_in_8bit=load_in_8bit)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35c3860e-fc8d-4998-bcfb-08add0ace2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/rc/projects/chatbot/conda_env/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.01` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3927290caaa4c13984e941aa32dde6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_in_8bit = config[\"load_in_8bit\"]\n",
    "\n",
    "if config[\"llm\"] == LLM_FALCON_7B:\n",
    "    llm = create_falcon_7b_instruct(load_in_8bit=load_in_8bit)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e62473d1-107c-4f39-a519-9ac4c6cec48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "encoding_name = tiktoken.get_encoding(\"cl100k_base\")\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b4ff27da-5e0e-498b-91b5-39e178921ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "text_splitter = TokenTextSplitter(chunk_size=500, chunk_overlap=25)\n",
    "docs = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e266f829-1490-44a1-a4dc-d46a87ea3b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.vectorstores import Chroma\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fe3ee27e-6219-4f83-aafa-5d7ee7a5da50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#persist_directory = config[\"persist_directory\"]\n",
    "#vectordb = Chroma.from_documents(documents=texts, embedding=embedding, persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "564ca6f0-0d10-4186-8914-2a529e324beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Local mode, without using the Qdrant server, may also store your vectors on disk so they're persisted between runs.\n",
    "from langchain.vectorstores import Qdrant\n",
    "\n",
    "Qdrantdb = Qdrant.from_documents(\n",
    "    docs,\n",
    "    embedding,\n",
    "    path=\"/work/rc/projects/chatbot/chatbotrc/notebooks/RAG/tmp/local_qdrant\",\n",
    "    collection_name=\"RC_documents\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "397ef9ed-318b-4574-ad5a-66d3a099644b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.vectorstore import VectorStoreRetriever\n",
    "retriever = VectorStoreRetriever(vectorstore=Qdrantdb, search_type=\"mmr\", search_kwargs={'k': 4, 'fetch_k': 10},)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ce193d59-2091-45e3-a547-37547b14862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    PromptTemplate,\n",
    ")\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fa666bf2-2892-4938-bad7-33de5733374f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.vectorstore import VectorStoreRetriever\n",
    "retriever = VectorStoreRetriever(vectorstore=Qdrantdb, search_type=\"mmr\", search_kwargs={'k': 4, 'fetch_k': 10},)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "43074466-ce40-49e4-a390-0039da355117",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor = CohereRerank() #LLMChainExtractor,LLMChainFilter,EmbeddingsFilter\n",
    "# will iterate over the initially returned documents and extract from each only the content that is relevant to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a597f90d-3124-4ee1-a40e-7291795277b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up cohere's reranker\n",
    "''' instead of immediately returning retrieved documents as-is, \n",
    "you can compress them using the context of the given query, so that only the relevant information is returned. '''\n",
    "reranker = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3029ffaa-4bd2-4e2b-a42d-56198248eb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cdae0a63-5cac-4809-87c5-8d287db3df31",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_llm = HuggingFacePipeline(pipeline=llm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "17e2f74d-a308-429b-ace2-b25370d907e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ConversationTokenBufferMemory keeps a buffer of recent interactions in memory,\n",
    "and uses token length rather than number of interactions to determine when to flush interactions.\n",
    "'''\n",
    "memory = ConversationTokenBufferMemory(llm=hf_llm,memory_key=\"chat_history\", return_messages=True,input_key='question',max_token_limit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c94184dc-a2be-48ae-90db-b1536633167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "CONDENSE_QUESTION_PROMPT = '''\n",
    "Below is a summary of the conversation so far, and a new question asked by the user that needs to be answered by searching in a knowledge base.\n",
    "Generate a search query based on the conversation and the new question.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Search query:\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "PromptTemplates = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"question\"],\n",
    "    template=\"\"\"\n",
    "Below is a summary of the conversation so far, and a new question asked by the user that needs to be answered by searching in a knowledge base.\n",
    "Generate a search query based on the conversation and the new question.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Search query:\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "system_message_prompt = SystemMessagePromptTemplate(prompt=PromptTemplates)\n",
    "\n",
    "chat_prompt_for_ques = ChatPromptTemplate.from_messages(\n",
    "    [system_message_prompt])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8db3f34b-587b-44f3-b631-704ef97fa638",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6f8af22b-28f5-42c6-acfd-437e5ec86493",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generator = LLMChain(llm=hf_llm, prompt=chat_prompt_for_ques, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dc4619a9-31f6-480b-bd1c-6a67fa5f1679",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer_Generator_Prompt= '''\n",
    "<Instructions>\n",
    "Important:\n",
    "Answer with the facts listed in the list of sources below. If there isn't enough information below, say you don't know.\n",
    "If asking a clarifying question to the user would help, ask the question.\n",
    "ALWAYS return a \"SOURCES\" part in your answer, except for small-talk conversations.\n",
    "\n",
    "Question: {question}\n",
    "Sources:\n",
    "---------------------\n",
    "    {summaries}\n",
    "---------------------\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ece5e906-b726-4b3f-a45b-452d6240743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "\n",
    "chat_prompt = PromptTemplate(template=Answer_Generator_Prompt, input_variables=[\"question\", \"summaries\",\"chat_history\"])\n",
    "\n",
    "answer_chain = load_qa_with_sources_chain(hf_llm, chain_type=\"stuff\", verbose=True,prompt=chat_prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b3454a53-f403-4909-8c25-1fb43d5ab9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "\n",
    "chain = ConversationalRetrievalChain(\n",
    "            retriever=reranker,\n",
    "            question_generator=question_generator,\n",
    "            combine_docs_chain=answer_chain,\n",
    "            verbose=True,\n",
    "            memory=memory,\n",
    "            rephrase_question=False\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "07d6ef15-1cf4-40b7-a962-400f25f932af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/rc/projects/chatbot/conda_env/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.01` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/work/rc/projects/chatbot/conda_env/lib/python3.9/site-packages/transformers/generation/utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "The current implementation of Falcon calls `torch.scaled_dot_product_attention` directly, this will be deprecated in the future in favor of the `BetterTransformer` API. Please install the latest optimum library with `pip install -U optimum` and call `model.to_bettertransformer()` to benefit from `torch.scaled_dot_product_attention` and future performance optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "<Instructions>\n",
      "Important:\n",
      "Answer with the facts listed in the list of sources below. If there isn't enough information below, say you don't know.\n",
      "If asking a clarifying question to the user would help, ask the question.\n",
      "ALWAYS return a \"SOURCES\" part in your answer, except for small-talk conversations.\n",
      "\n",
      "Question: What is the Scheduling Policies for HPC cluster?\n",
      "Sources:\n",
      "---------------------\n",
      "    Content:  larger resource requirements may be assigned higher priority, as they require more significant resources to execute efficiently.\n",
      "\n",
      "\n",
      "Walltime Limit#\n",
      "Jobs with shorter estimated execution times may receive higher priority, ensuring they are executed promptly and freeing up resources for other jobs.\n",
      "\n",
      "\n",
      "\n",
      "Balancing Policies#\n",
      "\n",
      "Backfilling#\n",
      "This policy allows smaller jobs to “backfill” into available resources ahead of larger jobs, optimizing resource utilization and reducing wait times.\n",
      "\n",
      "\n",
      "Preemption#\n",
      "Higher-priority jobs can preempt lower-priority ones, temporarily pausing the lower-priority job’s execution to make resources available for the higher-priority job.\n",
      "\n",
      "\n",
      "\n",
      "Best Practices#\n",
      "\n",
      "Set Realistic Priorities: Assign accurate priorities to your jobs to reflect their importance and resource requirements.\n",
      "Use Resource Quotas: Be mindful of the resources you request to prevent over- or underutilization.\n",
      "Leverage Backfilling: Submit smaller, shorter jobs that can backfill into available resources while waiting for larger jobs to start.\n",
      "\n",
      "Understanding these scheduling policies and priorities empowers you to make informed decisions when submitting jobs, ensuring that your computational tasks are executed efficiently and promptly. If you need further guidance on selecting the right scheduling policy for your job or optimizing your resource usage, our support team is available at rchelp@northeastern.edu or consult our Frequently Asked Questions (FAQs).\n",
      "Optimize your job execution by maximizing our cluster’s scheduling capabilities. Happy computing!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Next\n",
      "\n",
      "Interactive and Batch Mode\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Previous\n",
      "\n",
      "Understanding the Queuing System\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                Copyright © 2023, RC\n",
      "            \n",
      "            Made with \n",
      "            Furo\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            On this page\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Job Scheduling Policies and Priorities\n",
      "Scheduling Policies\n",
      "FIFO (First-In-First-Out)\n",
      "Fair Share\n",
      "Priority-Based\n",
      "\n",
      "\n",
      "Job Priorities\n",
      "User Priority\n",
      "Resource Requirements\n",
      "Walltime Limit\n",
      "\n",
      "\n",
      "Balancing Policies\n",
      "Backfilling\n",
      "Preemption\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\n",
      "\n",
      "Content: \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Job Scheduling Policies and Priorities - RC RTD\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Menu\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Expand\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Light mode\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dark mode\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Auto light/dark mode\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Hide navigation sidebar\n",
      "\n",
      "\n",
      "Hide table of contents sidebar\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Toggle site navigation sidebar\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RC RTD\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Toggle Light / Dark / Auto color theme\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Toggle table of contents sidebar\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Research ComputingToggle child pages in navigation\n",
      "Welcome\n",
      "Services We Provide\n",
      "Getting Help\n",
      "Introduction to HPC and Slurm\n",
      "Case Studies and User Testimonials\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Getting StartedToggle child pages in navigation\n",
      "Getting Access\n",
      "Account Manager\n",
      "Connecting To ClusterToggle child pages in navigation\n",
      "Mac\n",
      "Windows\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "First StepsToggle child pages in navigation\n",
      "Passwordless SSH\n",
      "Shell Environment on the Cluster\n",
      "Cluster via Command-Line\n",
      "\n",
      "\n",
      "\n",
      "User Guides\n",
      "\n",
      "HardwareToggle child pages in navigation\n",
      "Overview\n",
      "Partitions\n",
      "\n",
      "\n",
      "Open OnDemand (OOD)Toggle child pages in navigation\n",
      "Introduction to OOD\n",
      "Accessing Open OnDemand\n",
      "Interactive Open OnDemand ApplicationsToggle child pages in navigation\n",
      "Desktop App\n",
      "OOD File Explorer\n",
      "JupyterLab\n",
      "Stata\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running JobsToggle child pages in navigation\n",
      "Understanding the Queuing System\n",
      "Job Scheduling Policies and Priorities\n",
      "Interactive and Batch Mode\n",
      "Working with GPUs\n",
      "Recurring Jobs\n",
      "Debugging and Troubleshooting Jobs\n",
      "\n",
      "\n",
      "Data ManagementToggle child pages in navigation\n",
      "Data Storage Options\n",
      "Transfer Data\n",
      "Using Globus\n",
      "Data Backup and Restore\n",
      "Security and Compliance\n",
      "\n",
      "\n",
      "SoftwareToggle child pages in navigation\n",
      "System WideToggle child pages in navigation\n",
      "Modules\n",
      "MPI\n",
      "R\n",
      "Matlab\n",
      "\n",
      "\n",
      "Package ManagersToggle child pages in navigation\n",
      "Conda\n",
      "Spack\n",
      "\n",
      "\n",
      "From SourceToggle child pages in navigation\n",
      "Make\n",
      "CMake\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SlurmToggle child pages in navigation\n",
      "Introduction to Slurm\n",
      "Slurm Commands\n",
      "Slurm Running Jobs\n",
      "Monitoring and Managing Jobs\n",
      "Slurm Job Scripts\n",
      "Slurm Array Jobs and Dependencies\n",
      "Slurm Best Practices\n",
      "\n",
      "\n",
      "HPC for the Class\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\n",
      "\n",
      "Content:  times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.\n",
      "\n",
      "By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.\n",
      "\n",
      "Job Script#A file that contains a series of commands that the HPC cluster will execute.\n",
      "\n",
      "Login Node#A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.\n",
      "\n",
      "Module#In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.\n",
      "\n",
      "Message Passing Interface (MPI)#A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.\n",
      "\n",
      "Node#A single machine within a cluster. A node can have multiple processors and its memory and storage.\n",
      "\n",
      "Node Allocation#The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.\n",
      "\n",
      "Open OnDemand (OOD)#A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.\n",
      "\n",
      "Overcommitment#Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.\n",
      "\n",
      "Package Manager#A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.\n",
      "\n",
      "Parallel Computing#A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.\n",
      "\n",
      "Partition#A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.\n",
      "\n",
      "Quota#A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.\n",
      "\n",
      "Queue#A waiting line for jobs ready to be executed but waiting for resources to become available.\n",
      "\n",
      "Resource Reservation#The process of specifying resources required for a job in advance to ensure availability\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/glossary.html\n",
      "---------------------\n",
      "\n",
      "Chat History:\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afbf0abcd6ee4959966225303da2d9ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d09478011914837988ae298ee4c125f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "551fca38b24f442c99b1cf37fca1e7e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ebc673e3c74251ba95802663a4eb31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question from user :  What is the Scheduling Policies for HPC cluster? \n",
      "\n",
      "Reply from ChatBot :  <p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the Scheduling Policies for HPC cluster?\"\n",
    "result = chain({\"question\": query})\n",
    "\n",
    "\n",
    "print(\"Question from user : \" , query ,\"\\n\")\n",
    "print(\"Reply from ChatBot : \" , result['answer'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8ce45443-846c-4627-945d-74519ad13362",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "Below is a summary of the conversation so far, and a new question asked by the user that needs to be answered by searching in a knowledge base.\n",
      "Generate a search query based on the conversation and the new question.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: What is the Scheduling Policies for HPC cluster?\n",
      "Assistant: <p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "\n",
      "\n",
      "Question:\n",
      "How do I check Job Status?\n",
      "\n",
      "Search query:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "<Instructions>\n",
      "Important:\n",
      "Answer with the facts listed in the list of sources below. If there isn't enough information below, say you don't know.\n",
      "If asking a clarifying question to the user would help, ask the question.\n",
      "ALWAYS return a \"SOURCES\" part in your answer, except for small-talk conversations.\n",
      "\n",
      "Question: How do I check Job Status?\n",
      "Sources:\n",
      "---------------------\n",
      "    Content: #include <time.h>\n",
      "\n",
      "int main(int argc, char** argv) {\n",
      "    MPI_Init(NULL, NULL);\n",
      "\n",
      "    int world_rank;\n",
      "    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n",
      "    srand(time(NULL) * world_rank); // Ensure random numbers on all processes\n",
      "\n",
      "    int local_count = 0;\n",
      "    int global_count = 0;\n",
      "    int flip = 1 << 24;\n",
      "    double x, y, z;\n",
      "\n",
      "    // Calculate hits within circle locally\n",
      "    for (int i = 0; i < flip; i++) {\n",
      "        x = (double)rand() / (double)RAND_MAX;\n",
      "        y = (double)rand() / (double)RAND_MAX;\n",
      "        z = sqrt((x*x) + (y*y));\n",
      "        if (z <= 1.0) {\n",
      "            local_count++;\n",
      "        }\n",
      "    }\n",
      "\n",
      "    // Combine all local sums into the global sum\n",
      "    MPI_Reduce(&local_count, &global_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n",
      "\n",
      "    // Process 0 calculates pi and prints the result\n",
      "    if (world_rank == 0) {\n",
      "        double pi = ((double)global_count / (double)(flip * world_rank)) * 4.0;\n",
      "        printf(\"The estimated value of pi is %f\\n\", pi);\n",
      "    }\n",
      "\n",
      "    MPI_Finalize();\n",
      "}\n",
      "\n",
      "\n",
      "In this code, each process performs its own Monte Carlo simulation and then combines its results with those from other processes using the MPI_Reduce function.\n",
      "\n",
      "\n",
      "Using OpenMPI with Python’s mpi4py#\n",
      "mpi4py is a Python package that provides bindings to\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/software/systemwide/mpi.html\n",
      "\n",
      "Content: urm Array Jobs and Dependencies\n",
      "Slurm Best Practices\n",
      "\n",
      "\n",
      "HPC for the ClassroomToggle child pages in navigation\n",
      "Classroom HPC: FAQ\n",
      "CPS Class Instructions\n",
      "\n",
      "\n",
      "Best PracticesToggle child pages in navigation\n",
      "Home Directory Storage Quota\n",
      "Checkpointing Jobs\n",
      "Optimizing Job Performance\n",
      "Best SW Practices\n",
      "\n",
      "\n",
      "Tutorials and TrainingToggle child pages in navigation\n",
      "Canvas and GitHub\n",
      "\n",
      "\n",
      "Frequently Asked Questions\n",
      "Glossary\n",
      "\n",
      "Contribution\n",
      "\n",
      "Change Log\n",
      "Report Docs Bug or Request\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "    v: latest\n",
      "  \n",
      "\n",
      "\n",
      "Versions\n",
      "latest\n",
      "2.0.0\n",
      "1.2.0\n",
      "v1.1.0\n",
      "\n",
      "\n",
      "Downloads\n",
      "\n",
      "\n",
      "On Read the Docs\n",
      "\n",
      "Project Home\n",
      "\n",
      "\n",
      "Builds\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Back to top\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Edit this page\n",
      "\n",
      "\n",
      "\n",
      "Toggle Light / Dark / Auto color theme\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Toggle table of contents sidebar\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Getting Help#\n",
      "If you need help, you can contact the Research Computing (RC) team via email, ServiceNow ticket, or schedule an appointment through our Bookings page.\n",
      "\n",
      "Email#\n",
      "To contact the RC team, email us at rchelp@northeastern.edu. This will generate a ticket in ServiceNow. Could you be sure to include details about your question or issue, including any commands or scripts you use so that we can direct you to the right person?\n",
      "\n",
      "\n",
      "Submit a ticket#\n",
      "To submit a ticket in ServiceNow, select from the RC ServiceNow catalog. You may need to sign in with your Northeastern username and password to view the catalog.\n",
      "RC users can request services using our ticket system. You can go ahead and select the appropriate category below to access the online ticket.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Get Assistance with RC\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " RC Access Form\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Software Request Form\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Documentation Request\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Partition Access Request\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Storage Request\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Storage Extension Request\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Data Transfer Consultation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Classroom Request Form\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Unsubscribe\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Update Ticket#\n",
      "To check for updates on a submitted ticket, please follow these steps:\n",
      "\n",
      "Log in to your ServiceNow account.\n",
      "Select “My Tickets” to access a\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/welcome/gettinghelp.html\n",
      "\n",
      "Content:  +peri +python +qeq +replica +rigid +shock +snap +spin +srd +user-reaxc +user-misc\n",
      "\n",
      "\n",
      "\n",
      "Type spack find LAMMPS to view your installed software package.\n",
      "Type spack load lammps.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Next\n",
      "\n",
      "From Source\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Previous\n",
      "\n",
      "Conda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                Copyright © 2023, RC\n",
      "            \n",
      "            Made with \n",
      "            Furo\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            On this page\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Spack\n",
      "Install Spack\n",
      "Install a software using Spack\n",
      "Example: Installing LAMMPS\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/software/packagemanagers/spack.html\n",
      "---------------------\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: What is the Scheduling Policies for HPC cluster?\n",
      "Assistant: <p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Question from user :  How do I check Job Status? \n",
      "\n",
      "Reply from ChatBot :  <p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "<p>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"How do I check Job Status?\"\n",
    "result = chain({\"question\": query})\n",
    "\n",
    "\n",
    "print(\"Question from user : \" , query ,\"\\n\")\n",
    "print(\"Reply from ChatBot : \" , result['answer'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "efdd8149-476d-4277-8528-6b47026f2d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationTokenBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='What is the Scheduling Policies for HPC cluster?'), AIMessage(content='<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n'), HumanMessage(content='How do I check Job Status?'), AIMessage(content='<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n<p>\\n')]), input_key='question', return_messages=True, llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x2b705c0d87c0>), memory_key='chat_history', max_token_limit=1000)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1bd38e-40d7-42c1-a47f-e0813f17d462",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Chatbot Environment",
   "language": "python",
   "name": "chatbot_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
