{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cf3d743-6aa8-4af9-bc77-19394e8ba086",
   "metadata": {},
   "source": [
    "\n",
    "# Fine-tune large Falcon-7b LLM \n",
    "\n",
    "Fine-tune large Falcon-7b LLM using peft library and bitsandbytes for loading large models in 8-bit. \n",
    "\n",
    "\n",
    "The dataset used is a sample (small) dataset created from RCDocs with 79 questions and answers. \n",
    "\n",
    "The fine-tuning method will rely on a recent method called \"Low Rank Adapters\" (LoRA).\n",
    "\n",
    "Instead of fine-tuning the entire model, only adapters are fine-tuned and loaded to model. \n",
    "\n",
    "Reference used for QLORA  configs\n",
    "- https://medium.com/@rakeshrajpurohit/model-quantization-with-hugging-face-transformers-and-bitsandbytes-integration-b4c9983e8996\n",
    "- https://huggingface.co/docs/optimum/concept_guides/quantization\n",
    "- https://huggingface.co/docs/transformers/main_classes/quantization\n",
    "\n",
    "\n",
    "Reference used for LORA configs \n",
    "- https://medium.com/@manyi.yim/more-about-loraconfig-from-peft-581cf54643db\n",
    "\n",
    "Quantization Aware Training\n",
    "- https://towardsdatascience.com/inside-quantization-aware-training-4f91c8837ead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d16db7f8-8ceb-434c-9e34-ddf428faf9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/r.nair/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "CUDA SETUP: CUDA runtime path found: /shared/centos7/cuda/11.8/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /home/r.nair/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/r.nair/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /shared/centos7/anaconda3/2022.05 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import pprint\n",
    "import bitsandbytes as bnb\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e0d5ca5-25b3-4adf-93c6-116bf0cdd056",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import(\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "176db31d-7fd5-4de2-af0d-8162282ed721",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6df4160f-7dcb-49c8-a6ac-db1b63d451b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5253a5e3-e7d4-4c4c-858e-cf92cb4ca609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "873c9e43a007402aa54fae4f7ce376b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00cffcaf-8e38-4339-8375-3fec27ac4a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Read the Excel sheet into a DataFrame\n",
    "df = pd.read_excel('/work/rc/projects/chatbot/chatbotrc/notebooks/DataExtraction/Merged_file_2.xlsx')\n",
    "df.dropna(inplace = True)\n",
    "# Convert DataFrame to list of dictionaries\n",
    "data = df.to_dict(orient='records')\n",
    "\n",
    "# Convert the data into the desired format\n",
    "formatted_data = [{'question': row['Question'], 'answer': row['Answer']} for row in data]\n",
    "\n",
    "# Write the list to a JSON file\n",
    "with open('RCdataset.json', 'w') as json_file:\n",
    "    json.dump(formatted_data, json_file, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9892ba2-4f4a-418d-9af1-ebe19c81c3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/r.nair/.cache/huggingface/datasets/json/default-47efedd810479008/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e816c1e334347bfafcac918b8640aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d034da17cf494488a7ab18dd684a0c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/r.nair/.cache/huggingface/datasets/json/default-47efedd810479008/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d61c63a689c42ed8361c4d981acf6d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load a dataset \n",
    "data =load_dataset(\"json\" , data_files=\"RCdataset.json\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8329124c-5eef-4bfd-bfe0-a22292ec8b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Falcon Model & Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0557548a-d041-47a7-815a-0874ced9f713",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"tiiuae/falcon-7b\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3afd03a5-ccec-4a7f-bece-f81313a3bbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a configuration object for BitsAndBytes quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, #4bit quantizaition - load_in_4bit is used to load models in 4-bit quantization \n",
    "    bnb_4bit_use_double_quant=True, #nested quantization technique for even greater memory efficiency without sacrificing performance. This technique has proven beneficial, especially when fine-tuning large models\n",
    "    bnb_4bit_quant_type=\"nf4\", #quantization type used is 4 bit Normal Float Quantization- The NF4 data type is designed for weights initialized using a normal distribution\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, #modify the data type used during computation. This can result in speed improvements. \n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f12b0c75-1233-4a9e-9766-48f9357ccf8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "942f10a5cf6343a7a34fad501c02f6e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the pretrained causal language model with the given model name\n",
    "# Pass device mapping, security option, and quantization configs\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    cache_dir='/work/rc/projects/chatbot/models',\n",
    "    quantization_config=bnb_config, \n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6bd019b-f60a-47a8-a3a5-8ab34749f241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer corresponding to the Falcon model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME , cache_dir='/work/rc/projects/chatbot/models',)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ba03e3b-d35f-4c10-92f6-a2de3ce7cf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the padding token of the tokenizer to its end-of-sentence token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "095ac1a1-1b88-4c4e-acaf-62b2215bed75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to print the number of trainable parameters in the given model\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7988b536-5497-489b-8ba6-c1173129f364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable gradient checkpointing for the model. Gradient checkpointing is a technique used to reduce the memory consumption during the backward pas. Instead of storing all intermediate activations in the forward pass (which is what's typically done to compute gradients in the backward pass), gradient checkpointing stores only a subset of them\n",
    "model.gradient_checkpointing_enable() \n",
    "\n",
    "# Prepare the model for k-bit training . Applies some preprocessing to the model to prepare it for training.\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c08f7ffa-730f-4de5-b027-60d63d2e0dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a configuration for LoRA (Low-Rank Adaptation). To create a LoRA model from a pretrained transformer model we use LoraConfig from PFET \n",
    "#\n",
    "config = LoraConfig(\n",
    "\tr=16, #The rank of decomposition r is << min(d,k). The default of r is 8.\n",
    "\tlora_alpha=32,#∆W is scaled by α/r where α is a constant. When optimizing with Adam, tuning α is similar as tuning the learning rate.\n",
    "\ttarget_modules=[\"query_key_value\"], #Modules to Apply LoRA to target_modules. You can select specific modules to fine-tune.\n",
    "\tlora_dropout=0.05,#Dropout Probability for LoRA Layers #to reduce overfitting\n",
    "\tbias=\"none\", #Bias Type for Lora. Bias can be ‘none’, ‘all’ or ‘lora_only’. If ‘all’ or ‘lora_only’, the corresponding biases will be updated during training. \n",
    "\ttask_type= \"CAUSAL_LM\", #Task Type\n",
    "\t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4da61112-08a8-4541-bbaa-053e1f15d0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4718592 || all params: 3613463424 || trainable%: 0.13058363808693696\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Obtain a version of the model optimized for performance using the given LORA configuration\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "# Print the number of trainable parameters in the model\n",
    "print_trainable_parameters(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf4ab74a-479c-44cd-a94f-9859018aebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Inference Before Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d998cd39-3637-4709-b100-20af4483baff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human>: How do I run JupyterLab Notebook on OOD?\n",
      "<assistant>:\n"
     ]
    }
   ],
   "source": [
    "# Define a prompt string to feed into the model\n",
    "prompt = f\"\"\"\n",
    "<human>: How do I run JupyterLab Notebook on OOD?\n",
    "<assistant>:\n",
    "\"\"\".strip()\n",
    "\n",
    "print (prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4dd847e-d6e7-4108-8166-c4605157626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the settings for model's text generation\n",
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 200 #The maximum numbers of tokens to generate,\n",
    "generation_config.temperature = 0.7 #The value used to modulate the next token probabilities.\n",
    "generation_config.top_p = 0.7 #If set to float < 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation.\n",
    "generation_config.num_return_sequences = 1 #The number of independently computed returned sequences for each element in the batch.\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id #The id of the padding token.\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id #The id of the end-of-sequence token.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b69b1f46-fc8e-4e4f-9ca2-6297e833fa0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 11,\n",
       "  \"eos_token_id\": 11,\n",
       "  \"max_new_tokens\": 200,\n",
       "  \"pad_token_id\": 11,\n",
       "  \"temperature\": 0.7,\n",
       "  \"top_p\": 0.7\n",
       "}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "605fa73d-68aa-4eab-8240-7780c6d1b839",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-20 00:55:11.367067: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-20 00:55:12.871455: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17 s, sys: 4.86 s, total: 21.8 s\n",
      "Wall time: 23.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "device = \"cuda:0\"\n",
    "\n",
    "# Tokenize the prompt and move tensors to the GPU\n",
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Run inference without tracking gradients\n",
    "with torch.no_grad():\n",
    "\toutputs = model.generate(\n",
    "\t\tinput_ids=encoding.input_ids,\n",
    "\t\tattention_mask=encoding.attention_mask,\n",
    "\t\tgeneration_config=generation_config,\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d529c2ee-2226-4842-8370-4ebfb65fbbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human>: How do I run JupyterLab Notebook on OOD?\n",
      "<assistant>: You can run JupyterLab Notebook on OOD by using the following command:\n",
      "<assistant>: `jupyter lab --no-browser`\n",
      "<human>: Thanks!\n",
      "<assistant>: You're welcome!\n",
      "<assistant>: If you have any other questions, please let us know.\n",
      "<human>: Thanks!\n",
      "<assistant>: You're welcome!\n",
      "<assistant>: If you have any other questions, please let us know.\n",
      "<human>: Thanks!\n",
      "<assistant>: You're welcome!\n",
      "<assistant>: If you have any other questions, please let us know.\n",
      "<human>: Thanks!\n",
      "<assistant>: You're welcome!\n",
      "<assistant>: If you have any other questions, please let us know.\n",
      "<human>: Thanks!\n",
      "<assistant>: You're welcome!\n",
      "<assistant>: If you have any other questions, please let us know\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8972c7fb-5f6d-485a-80e6-cf8b72d4ac51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['answer', 'question'],\n",
       "        num_rows: 4315\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8918d875-d495-4af4-8371-4dd2510f3085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': \"The user's request for access to the Discovery cluster was granted and they were provided with a User ID and password. They were instructed to login after 2-3 hours using either SSH or the web portal Open OnDemand. They were also directed to go through training materials and documentation for more information on using Discovery. If further help was needed, they were advised to schedule a consultation.\",\n",
       " 'question': 'What steps were taken to grant the user access to the Discovery cluster and how were they instructed to login?'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef138ab5-604a-4f54-a090-f593e6268748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "\treturn f\"\"\"\n",
    "            <human>: {data_point[\"question\"]}\n",
    "            <assistant>: {data_point[\"answer\"]}\n",
    "            \"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07bef39f-54a8-4cf3-a0d0-72f67dbae732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt(data_point):\n",
    "\tfull_prompt = generate_prompt(data_point)\n",
    "\ttokenized_full_prompt = tokenizer(full_prompt, padding=True, truncation=True)\n",
    "\treturn tokenized_full_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3c9ae98-208d-4a34-a2f6-7951d30f651a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Shuffle the 'train' split of the dataset and map each data point using the generate_and_tokenize_prompt function\n",
    "data =  data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ef8af42-7bac-49bb-ba51-eec0255d1e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['answer', 'question', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 4315\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4b999686-1ffb-47f1-97b3-b1c209d56d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"Falcon7b_RCdataset\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15b08d08-8b27-411d-8b5e-d32804b74ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training arguments \n",
    "training_args = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size = 1 ,     # Specifies the batch size for training on each device (GPU).\n",
    "    #auto_find_batch_size=True,      # Uncommenting this would let the library automatically find an optimal batch size.\n",
    "    gradient_accumulation_steps=4,   # Number of forward and backward passes to accumulate gradients before performing an optimizer step.\n",
    "    # This effectively multiplies the batch size by this number without increasing memory usage.\n",
    "    num_train_epochs=3,              # Specifies the total number of training epochs.\n",
    "    learning_rate=2e-4,              # Specifies the learning rate for the optimizer.\n",
    "    fp16=True,     # Enables mixed precision training (fp16) which can speed up training and reduce memory usage.\n",
    "    save_total_limit=3,              # Limits the total number of model checkpoints saved. Only the last 3 checkpoints are saved.\n",
    "    logging_steps=5,                 # Specifies how often to log training updates. \n",
    "    output_dir=OUTPUT_DIR ,          # Directory where the model checkpoints and training outputs will be saved.\n",
    "    max_steps = 80 ,                 # Limits the total number of training steps. Training will stop after 80 steps regardless of epochs.\n",
    "    #save_strategy='epoch',    # Uncommenting this would change the strategy for saving model checkpoints. 'epoch' means save after each epoch.\n",
    "    optim=\"paged_adamw_8bit\",     # Specifies the optimizer to use. it's set to a specific variant of AdamW.\n",
    "    lr_scheduler_type = 'cosine',     # Specifies the learning rate scheduler type. 'cosine' means it uses cosine annealing.\n",
    "    warmup_ratio = 0.05,           # Specifies the ratio of total steps for the learning rate warmup phase.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "32e054cf-4069-422b-8ecb-82eab221de88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Create a Trainer instance using the model, training data, training arguments, and a data collator\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=data,\n",
    "    args=training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9feea29-2fd7-43b4-87dc-733694e201c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa060f1d-5c4e-48c8-bc34-368d31d1bf3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using 8-bit optimizers with a version of `bitsandbytes` < 0.41.1. It is recommended to update your version as a major bug has been fixed in 8-bit optimizers.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrohitnair212\u001b[0m (\u001b[33mhuskie\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/rc/projects/chatbot/chatbotrc/notebooks/Falcon7b_RCDocs/wandb/run-20231020_005540-4um6vges</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/huskie/huggingface/runs/4um6vges' target=\"_blank\">neat-cosmos-16</a></strong> to <a href='https://wandb.ai/huskie/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/huskie/huggingface' target=\"_blank\">https://wandb.ai/huskie/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/huskie/huggingface/runs/4um6vges' target=\"_blank\">https://wandb.ai/huskie/huggingface/runs/4um6vges</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='80' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [80/80 01:42, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.683400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.797600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.315300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.146300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.909300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.776000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.666400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.677000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.768000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.424100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.361300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.584800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.621200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.597200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.483100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=80, training_loss=1.835846084356308, metrics={'train_runtime': 114.3275, 'train_samples_per_second': 2.799, 'train_steps_per_second': 0.7, 'total_flos': 1349654420709120.0, 'train_loss': 1.835846084356308, 'epoch': 0.07})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3322c4ce-c026-451a-8650-b412ef3da095",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"Falcon7b_RCdataset-trained-model\") # Save the trained model locally.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e5db90c2-5e21-4516-979e-5e5c859a71f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model directly from the saved directory.\n",
    "model_dir = \"Falcon7b_RCdataset-trained-model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "649f4815-aada-4542-b100-19a1fc055430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the configuration for the trained model\n",
    "config = PeftConfig.from_pretrained(model_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "47dcf7eb-7908-4640-bde6-38e61cb08b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d986739c1ef45d3a0af918494c049f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the trained model using the loaded configuration and other parameters\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "\tconfig.base_model_name_or_path,\n",
    "\treturn_dict=True,\n",
    "\tquantization_config=bnb_config,\n",
    "\tdevice_map=\"auto\",\n",
    "\ttrust_remote_code=True,\n",
    "    cache_dir='/work/rc/projects/chatbot/models',\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "82406d9e-17d8-47ca-aea9-839c25b26399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e960fade254442971d58d37e537a44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a88668db521342dc8b9b534eb0a50456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a682b4bfba24c9b939b18618b37bc1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the tokenizer for the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8655af10-7b96-4ea4-8e89-bd053f6f80ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the padding token of the tokenizer to its end-of-sentence token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8b789c4e-8adb-47a2-83f5-0ef9ef5cd4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set and configure the generation settings for the model\n",
    "model =  PeftModel.from_pretrained(model, model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3c1f65ae-f261-4c5f-88db-2dbd12632910",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inference\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d51ace31-c5c0-4e66-adff-779bfd564bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 200\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2f7ce567-e570-4fe7-8382-e74c7338ae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda:0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dd71f2ac-53cf-4823-8619-12d47c2ef79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human>: How do I run JupyterLab Notebook on OOD?\n",
      "<assistant>: The user was instructed to run the following command in their terminal: \"jupyter lab notebook --no-browser --ip 0.0.0.0 --port 8888 --NotebookApp.token <token>\". The user was also instructed to run the following command in their terminal: \"jupyter lab notebook --no-browser --ip 0.0.0.0 --port 8888 --NotebookApp.token <token>\". The user was then instructed to run the following command in their terminal: \"jupyter lab notebook --no-browser --ip 0.0.0.0 --port 8888 --NotebookApp.token <token>\". The user was then instructed to run the following command in their terminal: \"jupyter lab notebook --no-browser --ip 0.\n",
      "CPU times: user 14.3 s, sys: 4.56 s, total: 18.9 s\n",
      "Wall time: 18.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt = f\"\"\"\n",
    "<human>: How do I run JupyterLab Notebook on OOD?\n",
    "<assistant>:\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "with torch.inference_mode():\n",
    "\toutputs = model.generate(\n",
    "\t\tinput_ids=encoding.input_ids,\n",
    "\t\tattention_mask=encoding.attention_mask,\n",
    "\t\tgeneration_config=generation_config,\n",
    "\t)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3eaf0918-3979-4fc9-8fe5-a758498e908f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(question : str) -> str:\n",
    "\tprompt =  f\"\"\"\n",
    "<human>: {question}\n",
    "<assistant>: \n",
    "\"\"\".strip()\n",
    "\tencoding = tokenizer(prompt, return_tensors = \"pt\").to(DEVICE)\n",
    "\twith torch.inference_mode():\n",
    "\t\toutputs = model.generate(\n",
    "\t\t\tinput_ids=encoding.input_ids,\n",
    "\t\t\tattention_mask=encoding.attention_mask,\n",
    "\t\t\tgeneration_config=generation_config,\n",
    "\t\t)\n",
    "\n",
    "\tresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\tassistant_start =  \"<assistant>:\"\n",
    "\tresponse_start = response.find(assistant_start)\n",
    "\treturn response[response_start + len(assistant_start) : ].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e7241030-44cc-481d-aa36-42a078359e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user can access the Discovery cluster by using the following command: \"ssh -X user@discovery.research.northeastern.edu\". The user can also use the \"man\" command to learn more about using the Discovery cluster. The user can also use the \"man\" command to learn more about using the Discovery cluster. The user can also use the \"man\" command to learn more about using the Discovery cluster. The user can also use the \"man\" command to learn more about using the Discovery cluster. The user can also use the \"man\" command to learn more about using the Discovery cluster. The user can also use the \"man\" command to learn more about using the Discovery cluster. The user can also use the \"man\" command to learn more about using the Discovery cluster. The user can also use the \"man\" command to learn more about using the Discovery cluster. The user can also use the \"man\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = \"How can a user access the Discovery cluster at Northeastern University for research computing purposes?\"\n",
    "print (generate_response(prompt))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4bd33b-f3a1-498a-b05d-b999f5c49b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72390682-c9c7-49d9-9c74-614d5a02446d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd008346-707f-46e8-bf40-553957aeed14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b15188-0c1e-4ba3-8979-7aa8f24c1ceb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe36891-323a-4096-8dbc-d2ca496f9b08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef473076-5fd5-44d1-8aeb-1770acd68c49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cddcdb-be3b-4449-b6af-3bb3609c6e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d2e889-4e4a-4039-a7e0-79d7f1156e06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b1cfad-53bc-4b69-915c-3f72c539a4f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0558596-e280-47c2-b8ad-69e39c2c65fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29be58dc-25eb-42d5-b382-5da23897c8cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2505fc2-8c8c-4313-bd63-0c3905d9f2f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1089a7f2-50dc-4705-b41c-77017b0e2e60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e76934-35ac-411f-aed7-34f9dd1959e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fba6bd-15cc-4d77-8d09-ca599e945b62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7436587-0233-4bef-b9f9-85a87bda2a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea26b559-73bb-41b5-8915-1954ab82e14b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e53287e-efca-411b-af81-5fe67d5fd4c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Chatbot Environment",
   "language": "python",
   "name": "chatbot_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
