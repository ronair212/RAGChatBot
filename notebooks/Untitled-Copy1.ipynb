{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47b591a6-0629-4943-896e-626534d81adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-17 13:50:25.373452: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-17 13:50:27.203311: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-10-17 13:50:42.047166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /device:GPU:0 with 78923 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:41:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/device:GPU:0']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d07e60c-79bf-4898-a746-a6e1456cf7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91b96f2d-6340-410d-a784-7dd50d6d5e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "900b2960-9d29-4a66-950e-d8c171fda643",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cb3bbcb-2f8a-450d-92af-f520b95c1315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Cohere API Key: ········································\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3c316a5-485b-40e4-bc17-ef0d17acf66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OPENAI API Key: ···················································\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OPENAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf4e987c-f100-4783-b559-f31bde6cb67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "\n",
    "urls = [\"https://rc-docs.northeastern.edu/en/latest/welcome/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/welcome/welcome.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/welcome/services.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/welcome/gettinghelp.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/welcome/introtocluster.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/welcome/casestudiesandtestimonials.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/get_access.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/accountmanager.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/connectingtocluster/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/connectingtocluster/mac.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/connectingtocluster/windows.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/first_steps/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/first_steps/passwordlessssh.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/first_steps/shellenvironment.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/first_steps/usingbash.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/hardware/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/hardware/hardware_overview.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/hardware/partitions.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/introduction.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/accessingood.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/desktopood.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/fileexplore.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/jupyterlab.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/understandingqueuing.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/workingwithgpus.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/recurringjobs.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/debuggingjobs.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/discovery_storage.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/transferringdata.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/globus.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/databackup.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/securityandcompliance.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/modules.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/mpi.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/r.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/matlab.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/packagemanagers/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/packagemanagers/conda.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/packagemanagers/spack.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/fromsource/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/fromsource/makefile.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/fromsource/cmake.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/introductiontoslurm.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmcommands.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmrunningjobs.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmmonitoringandmanaging.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmscripts.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmarray.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmbestpractices.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/classroom/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/classroom/class_use.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/classroom/cps_ood.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/classroom/classroomexamples.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/best-practices/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/best-practices/homequota.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/best-practices/checkpointing.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/best-practices/optimizingperformance.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/best-practices/software.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/tutorialsandtraining/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/tutorialsandtraining/canvasandgithub.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/faq.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/glossary.html\",\n",
    "]\n",
    "loader = WebBaseLoader(urls)\n",
    "data = loader.load()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78c149f5-e805-4e22-a4ef-203f517071bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "encoding_name = tiktoken.get_encoding(\"cl100k_base\")\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fff02684-83d8-452e-aca9-3e24d4993e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "text_splitter = TokenTextSplitter(chunk_size=500, chunk_overlap=25)\n",
    "docs = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b37a4ba-32f6-43b7-a09f-7cf48b4e7ede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d43a98a2-94c3-4c46-82b2-83895f946d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_INSTRUCTOR_XL = \"hkunlp/instructor-xl\"\n",
    "EMB_SBERT_MPNET_BASE = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "039ca58e-65a5-4780-9c91-d2b3bc3f67a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_FLAN_T5_XXL = \"google/flan-t5-xxl\"\n",
    "LLM_FLAN_T5_XL = \"google/flan-t5-xl\"\n",
    "LLM_FASTCHAT_T5_XL = \"lmsys/fastchat-t5-3b-v1.0\"\n",
    "LLM_FLAN_T5_SMALL = \"google/flan-t5-small\"\n",
    "LLM_FLAN_T5_BASE = \"google/flan-t5-base\"\n",
    "LLM_FLAN_T5_LARGE = \"google/flan-t5-large\"\n",
    "LLM_FALCON_7B_INSTRUCT = \"tiiuae/falcon-7b-instruct\"\n",
    "LLM_FALCON_40B_INSTRUCT = \"tiiuae/falcon-40b-instruct\"\n",
    "LLM_FALCON_7B = \"tiiuae/falcon-7b\"\n",
    "LLM_FALCON_40B = \"tiiuae/falcon-40b\"\n",
    "LLM_LLAMA2_70B_INSTRUCT = \"upstage/Llama-2-70b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d839bbf-4377-4cdc-9c23-ff5e2cb79c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir='/work/rc/projects/chatbot/models'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae518ccf-4075-46f8-893e-5d715c43658a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"persist_directory\":None,\n",
    "          \"load_in_8bit\":False,\n",
    "          \"embedding\" : EMB_SBERT_MPNET_BASE,\n",
    "          \"llm\":LLM_FALCON_7B_INSTRUCT,\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78a52923-6b76-4a50-a4bd-9bc886af2a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/work/rc/projects/chatbot/models'\n",
    "#cache_folder=os.getenv('SENTENCE_TRANSFORMERS_HOME')\n",
    "os.environ['SENTENCE_TRANSFORMERS_HOME'] = '/work/rc/projects/chatbot/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c434a8c-1ce9-4aaf-9568-3581faa68403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sbert_mpnet():\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        return HuggingFaceEmbeddings(model_name=EMB_SBERT_MPNET_BASE, cache_folder=cache_dir, model_kwargs={\"device\": device})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e84c01f7-80cf-44cc-bb16-e973c4b7e2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c16a556c-29a4-4283-a14c-d6c5762227ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_llama2_70b_instruct(load_in_8bit=True):\n",
    "        model = LLM_LLAMA2_70B_INSTRUCT\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model , cache_dir=cache_dir)\n",
    "        hf_pipeline = pipeline(\n",
    "                task=\"text-generation\",\n",
    "                model = model,\n",
    "                do_sample=True, #Whether or not to use sampling ; use greedy decoding otherwise.\n",
    "                tokenizer = tokenizer,\n",
    "                #trust_remote_code = True,\n",
    "                max_new_tokens=500, #The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n",
    "                #cache_dir=cache_dir,\n",
    "                model_kwargs={\n",
    "                    \"device_map\": \"auto\", \n",
    "                    \"load_in_8bit\": load_in_8bit, \n",
    "                    \"max_length\": 512, \n",
    "                    \"temperature\": 0.01,\n",
    "                    \n",
    "                    \"torch_dtype\":torch.bfloat16,\n",
    "                    }\n",
    "            )\n",
    "        return hf_pipeline\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6ded4db-00ab-457d-9ad3-30417b6f4b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_falcon_40b(load_in_8bit=True):\n",
    "        model = LLM_FALCON_40B\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model , cache_dir=cache_dir)\n",
    "        hf_pipeline = pipeline(\n",
    "                task=\"text-generation\",\n",
    "                model = model,\n",
    "                do_sample=True, #Whether or not to use sampling ; use greedy decoding otherwise.\n",
    "                tokenizer = tokenizer,\n",
    "                #trust_remote_code = True,\n",
    "                max_new_tokens=500, #The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n",
    "                #cache_dir=cache_dir,\n",
    "                model_kwargs={\n",
    "                    \"device_map\": \"auto\", \n",
    "                    \"load_in_8bit\": load_in_8bit, \n",
    "                    \"max_length\": 512, \n",
    "                    \"temperature\": 0.01,\n",
    "                    \n",
    "                    \"torch_dtype\":torch.bfloat16,\n",
    "                    }\n",
    "            )\n",
    "        return hf_pipeline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca182968-c28f-41e5-a0ff-de1a03008a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_falcon_40b_instruct(load_in_8bit=True):\n",
    "        model = LLM_FALCON_40B_INSTRUCT\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model , cache_dir=cache_dir)\n",
    "        hf_pipeline = pipeline(\n",
    "                task=\"text-generation\",\n",
    "                model = model,\n",
    "                do_sample=True, #Whether or not to use sampling ; use greedy decoding otherwise.\n",
    "                tokenizer = tokenizer,\n",
    "                #trust_remote_code = True,\n",
    "                max_new_tokens=100, #The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n",
    "                #cache_dir=cache_dir,\n",
    "                model_kwargs={\n",
    "                    \"device_map\": \"auto\", \n",
    "                    \"load_in_8bit\": load_in_8bit, \n",
    "                    \"max_length\": 512, \n",
    "                    \"temperature\": 0.01,\n",
    "                    \n",
    "                    \"torch_dtype\":torch.bfloat16,\n",
    "                    }\n",
    "            )\n",
    "        return hf_pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c69bf69c-2e78-4690-b534-875648744310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_falcon_7b(load_in_8bit=True):\n",
    "        model = LLM_FALCON_7B\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model , cache_dir=cache_dir)\n",
    "        hf_pipeline = pipeline(\n",
    "                task=\"text-generation\",\n",
    "                model = model,\n",
    "                do_sample=True, #Whether or not to use sampling ; use greedy decoding otherwise.\n",
    "                tokenizer = tokenizer,\n",
    "                #trust_remote_code = True,\n",
    "                max_new_tokens=100, #The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n",
    "                #cache_dir=cache_dir,\n",
    "                model_kwargs={\n",
    "                    \"device_map\": \"auto\", \n",
    "                    \"load_in_8bit\": load_in_8bit, \n",
    "                    \"max_length\": 512, \n",
    "                    \"temperature\": 0.01,\n",
    "                    \n",
    "                    \"torch_dtype\":torch.bfloat16,\n",
    "                    }\n",
    "            )\n",
    "        return hf_pipeline\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cfc19c2e-57f7-4bc4-9d1b-f20687fb0606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_falcon_7b_instruct(load_in_8bit=True):\n",
    "        model = LLM_FALCON_7B_INSTRUCT\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model , cache_dir=cache_dir)\n",
    "        hf_pipeline = pipeline(\n",
    "                task=\"text-generation\",\n",
    "                model = model,\n",
    "                do_sample=True, #Whether or not to use sampling ; use greedy decoding otherwise.\n",
    "                tokenizer = tokenizer,\n",
    "                #trust_remote_code = True,\n",
    "                max_new_tokens=500, #The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n",
    "                #cache_dir=cache_dir,\n",
    "                model_kwargs={\n",
    "                    \"device_map\": \"auto\", \n",
    "                    \"load_in_8bit\": load_in_8bit, \n",
    "                    \"max_length\": 512, \n",
    "                    \"temperature\": 0.01,\n",
    "                    \n",
    "                    \"torch_dtype\":torch.bfloat16,\n",
    "                    }\n",
    "            )\n",
    "        return hf_pipeline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b66995a-1b3d-4989-94e5-e96c848896a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_flan_t5_base(load_in_8bit=True):\n",
    "        # Wrap it in HF pipeline for use with LangChain\n",
    "        model=\"google/flan-t5-base\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model, cache_dir=cache_dir)\n",
    "        return pipeline(\n",
    "            task=\"text2text-generation\",\n",
    "            model=model,\n",
    "            tokenizer = tokenizer,\n",
    "            max_new_tokens=100,\n",
    "            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.}\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca743027-6ffc-40ae-b624-a05ad8b307ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_in_8bit = config[\"load_in_8bit\"]\n",
    "if config[\"llm\"] == LLM_FLAN_T5_BASE:\n",
    "    llm = create_flan_t5_base(load_in_8bit=load_in_8bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32bbeab2-1d7c-4aad-ba51-14a19da0d58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_in_8bit = config[\"load_in_8bit\"]\n",
    "\n",
    "if config[\"llm\"] == LLM_FALCON_40B:\n",
    "    llm = create_falcon_40b(load_in_8bit=load_in_8bit)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "810f0f3f-4222-4b8c-9039-14aceaf0c261",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_in_8bit = config[\"load_in_8bit\"]\n",
    "\n",
    "if config[\"llm\"] == LLM_FALCON_40B_INSTRUCT:\n",
    "    llm = create_falcon_40b_instruct(load_in_8bit=load_in_8bit)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a89e1adb-3d0d-43c2-bc65-2ba44c1a5d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/rc/projects/chatbot/conda_env/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.01` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc9be8766c7543e38e3884dc8dab1405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_in_8bit = config[\"load_in_8bit\"]\n",
    "\n",
    "if config[\"llm\"] == LLM_FALCON_7B_INSTRUCT:\n",
    "    llm = create_falcon_7b_instruct(load_in_8bit=load_in_8bit)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21e0a433-5876-4ea4-9c60-d48669464314",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_in_8bit = config[\"load_in_8bit\"]\n",
    "\n",
    "if config[\"llm\"] == LLM_LLAMA2_70B_INSTRUCT:\n",
    "    llm = create_llama2_70b_instruct(load_in_8bit=load_in_8bit)\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4cc8d24e-778c-492d-937e-394c9743c538",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_in_8bit = config[\"load_in_8bit\"]\n",
    "\n",
    "if config[\"llm\"] == LLM_FALCON_7B:\n",
    "    llm = create_falcon_7b(load_in_8bit=load_in_8bit)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10f0356d-b829-4c9a-89c6-45b197a0ff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "#embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "# Equivalent to SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56928516-e35c-4cb0-aa09-7c87ba7dbc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Qdrant\n",
    "\n",
    "Qdrantdb = Qdrant.from_documents(\n",
    "    docs,\n",
    "    embeddings,\n",
    "    path=\"/work/rc/projects/chatbot/chatbotrc/notebooks/RAG/tmp/local_qdrant\",\n",
    "    collection_name=\"RC_documents\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09546165-5f43-4dcf-8baa-7fa6b9058d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "\n",
    "\n",
    "hf_llm = HuggingFacePipeline(pipeline = llm, model_kwargs = {'temperature':0})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "93a8a801-ab16-471d-8ae3-1f3c9b63559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "#from langchain.vectorstores import Pinecone\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e61d42cb-2244-4f7c-8473-ea2c5b10fddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.vectorstore import VectorStoreRetriever\n",
    "retriever = VectorStoreRetriever(vectorstore=Qdrantdb, search_type=\"mmr\", search_kwargs={'k': 4, 'fetch_k': 10},)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8a74bb3-9624-4922-987b-b223c0c3b445",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor = CohereRerank() #LLMChainExtractor,LLMChainFilter,EmbeddingsFilter\n",
    "# will iterate over the initially returned documents and extract from each only the content that is relevant to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3d16eabe-b278-40bf-868c-da2d4a5e5c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up cohere's reranker\n",
    "''' instead of immediately returning retrieved documents as-is, \n",
    "you can compress them using the context of the given query, so that only the relevant information is returned. '''\n",
    "reranker = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "09181435-ced9-4a61-b0d8-3d61d1006e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "885e86e4-bcb2-4e79-b254-633f54025024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "196b2abf-2122-45b6-bd18-e498c17542c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_callback = StdOutCallbackHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8916f339-335b-4c5c-b20f-5a064d941751",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=hf_llm,memory_key=\"chat_history\", return_messages=True,input_key='question',max_token_limit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7d78e8a8-b390-46f8-83f7-17c5ea4cedaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "\n",
    "PromptTemplates = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"question\"],\n",
    "    template=\"\"\"\n",
    "Below is a summary of the conversation so far, and a new question asked by the user that needs to be answered by searching in a knowledge base.\n",
    "Generate a search query based on the conversation and the new question. Frame the question in a way to get good similarity search results from a vector database.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Search query:\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "system_message_prompt = SystemMessagePromptTemplate(prompt=PromptTemplates)\n",
    "\n",
    "chat_prompt_for_ques = ChatPromptTemplate.from_messages(\n",
    "    [system_message_prompt])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7fa4ae65-6f2e-4790-87f5-86336ec4c2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e37d246e-0efa-47e0-97b1-2a386f851af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generator = LLMChain(llm=hf_llm, prompt=chat_prompt_for_ques, verbose=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4c410695-666c-4371-9c81-846757a3c597",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer_Generator_Prompt= '''\n",
    "<Instructions>\n",
    "Important:\n",
    "Answer with the facts listed in the list of sources below. If there isn't enough information below, say you don't know.\n",
    "If asking a clarifying question to the user would help, ask the question.\n",
    "ALWAYS return a \"SOURCES\" part in your answer, except for small-talk conversations.\n",
    "\n",
    "Question: {question}\n",
    "Sources:\n",
    "---------------------\n",
    "    {summaries}\n",
    "---------------------\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "43d0fb39-3d0f-4b62-be0f-d1ed0c72bc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "\n",
    "chat_prompt = PromptTemplate(template=Answer_Generator_Prompt, input_variables=[\"question\", \"summaries\",\"chat_history\"])\n",
    "\n",
    "answer_chain = load_qa_with_sources_chain(hf_llm, chain_type=\"stuff\", verbose=True,prompt=chat_prompt)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "766f3709-9727-444a-9dd3-9ced62d5a454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "\n",
    "chain = ConversationalRetrievalChain(\n",
    "            retriever=reranker,\n",
    "            question_generator=question_generator,\n",
    "            combine_docs_chain=answer_chain,\n",
    "            verbose=True,\n",
    "            memory=memory,\n",
    "            rephrase_question=False\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1edfd9d4-327f-49e1-8df1-d5f32ec2f19c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/rc/projects/chatbot/conda_env/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.01` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/work/rc/projects/chatbot/conda_env/lib/python3.9/site-packages/transformers/generation/utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "The current implementation of Falcon calls `torch.scaled_dot_product_attention` directly, this will be deprecated in the future in favor of the `BetterTransformer` API. Please install the latest optimum library with `pip install -U optimum` and call `model.to_bettertransformer()` to benefit from `torch.scaled_dot_product_attention` and future performance optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "<Instructions>\n",
      "Important:\n",
      "Answer with the facts listed in the list of sources below. If there isn't enough information below, say you don't know.\n",
      "If asking a clarifying question to the user would help, ask the question.\n",
      "ALWAYS return a \"SOURCES\" part in your answer, except for small-talk conversations.\n",
      "\n",
      "Question: Can you tell me what is a cluster\n",
      "Sources:\n",
      "---------------------\n",
      "    Content:  duration. However, the cluster is a shared resource, so we ask that you test out your assignments on the cluster so that you are requesting an appropriate amount of resources for your class. We can always increase your reservation if a class needs more resources due to higher-than-expected use. But, if a reservation is not being used to capacity, we will ask you to review the need for the requested resources and adjust the reservation accordingly. We understand that sometimes it takes time to determine precisely your resource needs. Still, we do need to keep a reservation to a reasonable limit to keep the shared resources available to all users.\n",
      "\n",
      "\n",
      "How long do my students have access to the cluster?#\n",
      "Students will have access to the cluster for the whole class duration. They must request an individual account if they want to continue accessing the cluster after that period.\n",
      "\n",
      "\n",
      "How do I get an account on the cluster?#\n",
      "If you are a professor or instructor at Northeastern, you can request an account on the cluster. See Sponsor Approval Process for more information.\n",
      "\n",
      "\n",
      "How do my students get help with the cluster?#\n",
      "You and your students can submit a Get Assistance with Research Computing ticket or email rchelp@northeastern.edu.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Next\n",
      "\n",
      "CPS Class Instructions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Previous\n",
      "\n",
      "Classroom Resources\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                Copyright © 2023, RC\n",
      "            \n",
      "            Made with \n",
      "            Furo\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            On this page\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classroom HPC: FAQ\n",
      "How can I use Discovery with my class?\n",
      "How do I get my class access to the cluster?\n",
      "Is there any training on the cluster for my class?\n",
      "Do my students have to learn Linux to work with the cluster?\n",
      "What software is available to use with my class on the cluster?\n",
      "My class needs access to a specific software application that I do not see installed on the cluster or Open OnDemand (OOD). What should I do?\n",
      "I just need my class to access Open OnDemand. How do I request that?\n",
      "I\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/classroom/class_use.html\n",
      "\n",
      "Content:  duration. However, the cluster is a shared resource, so we ask that you test out your assignments on the cluster so that you are requesting an appropriate amount of resources for your class. We can always increase your reservation if a class needs more resources due to higher-than-expected use. But, if a reservation is not being used to capacity, we will ask you to review the need for the requested resources and adjust the reservation accordingly. We understand that sometimes it takes time to determine precisely your resource needs. Still, we do need to keep a reservation to a reasonable limit to keep the shared resources available to all users.\n",
      "\n",
      "\n",
      "How long do my students have access to the cluster?#\n",
      "Students will have access to the cluster for the whole class duration. They must request an individual account if they want to continue accessing the cluster after that period.\n",
      "\n",
      "\n",
      "How do I get an account on the cluster?#\n",
      "If you are a professor or instructor at Northeastern, you can request an account on the cluster. See Sponsor Approval Process for more information.\n",
      "\n",
      "\n",
      "How do my students get help with the cluster?#\n",
      "You and your students can submit a Get Assistance with Research Computing ticket or email rchelp@northeastern.edu.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Next\n",
      "\n",
      "CPS Class Instructions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Previous\n",
      "\n",
      "Classroom Resources\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                Copyright © 2023, RC\n",
      "            \n",
      "            Made with \n",
      "            Furo\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            On this page\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classroom HPC: FAQ\n",
      "How can I use Discovery with my class?\n",
      "How do I get my class access to the cluster?\n",
      "Is there any training on the cluster for my class?\n",
      "Do my students have to learn Linux to work with the cluster?\n",
      "What software is available to use with my class on the cluster?\n",
      "My class needs access to a specific software application that I do not see installed on the cluster or Open OnDemand (OOD). What should I do?\n",
      "I just need my class to access Open OnDemand. How do I request that?\n",
      "I\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/classroom/class_use.html\n",
      "\n",
      "Content:  duration. However, the cluster is a shared resource, so we ask that you test out your assignments on the cluster so that you are requesting an appropriate amount of resources for your class. We can always increase your reservation if a class needs more resources due to higher-than-expected use. But, if a reservation is not being used to capacity, we will ask you to review the need for the requested resources and adjust the reservation accordingly. We understand that sometimes it takes time to determine precisely your resource needs. Still, we do need to keep a reservation to a reasonable limit to keep the shared resources available to all users.\n",
      "\n",
      "\n",
      "How long do my students have access to the cluster?#\n",
      "Students will have access to the cluster for the whole class duration. They must request an individual account if they want to continue accessing the cluster after that period.\n",
      "\n",
      "\n",
      "How do I get an account on the cluster?#\n",
      "If you are a professor or instructor at Northeastern, you can request an account on the cluster. See Sponsor Approval Process for more information.\n",
      "\n",
      "\n",
      "How do my students get help with the cluster?#\n",
      "You and your students can submit a Get Assistance with Research Computing ticket or email rchelp@northeastern.edu.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Next\n",
      "\n",
      "CPS Class Instructions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Previous\n",
      "\n",
      "Classroom Resources\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                Copyright © 2023, RC\n",
      "            \n",
      "            Made with \n",
      "            Furo\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            On this page\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classroom HPC: FAQ\n",
      "How can I use Discovery with my class?\n",
      "How do I get my class access to the cluster?\n",
      "Is there any training on the cluster for my class?\n",
      "Do my students have to learn Linux to work with the cluster?\n",
      "What software is available to use with my class on the cluster?\n",
      "My class needs access to a specific software application that I do not see installed on the cluster or Open OnDemand (OOD). What should I do?\n",
      "I just need my class to access Open OnDemand. How do I request that?\n",
      "I\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/classroom/class_use.html\n",
      "---------------------\n",
      "\n",
      "Chat History:\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Question from user :  Can you tell me what is a cluster \n",
      "\n",
      "Reply from ChatBot :  <p>Classroom HPC: FAQ</p>\n",
      "\n",
      "<p>How can I use Discovery with my class?</p>\n",
      "\n",
      "<p>How do I get my class access to the cluster?</p>\n",
      "\n",
      "<p>Is there any training on the cluster for my class?</p>\n",
      "\n",
      "<p>Do my students have to learn Linux to work with the cluster?</p>\n",
      "\n",
      "<p>What software is available to use with my class on the cluster?</p>\n",
      "\n",
      "<p>My class needs access to a specific software application that I do not see installed on the cluster or Open OnDemand (OOD). What should I do?</p>\n",
      "\n",
      "<p>I just need my class to access Open OnDemand. How do I request that?</p>\n",
      "\n",
      "<p>I</p>\n",
      "\n",
      "<p>Source: https://rc-docs.northeastern.edu/en/latest/classroom/class_use.html</p>\n"
     ]
    }
   ],
   "source": [
    "query = \"Can you tell me what is a cluster\"\n",
    "result = chain({\"question\": query})\n",
    "\n",
    "\n",
    "print(\"Question from user : \" , query ,\"\\n\")\n",
    "print(\"Reply from ChatBot : \" , result['answer'])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "45de448b-fdad-47e2-b9d2-0d3c00833bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "Below is a summary of the conversation so far, and a new question asked by the user that needs to be answered by searching in a knowledge base.\n",
      "Generate a search query based on the conversation and the new question. Frame the question in a way to get good similarity search results from a vector database.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: Can you tell me what is a cluster\n",
      "Assistant: <p>Classroom HPC: FAQ</p>\n",
      "\n",
      "<p>How can I use Discovery with my class?</p>\n",
      "\n",
      "<p>How do I get my class access to the cluster?</p>\n",
      "\n",
      "<p>Is there any training on the cluster for my class?</p>\n",
      "\n",
      "<p>Do my students have to learn Linux to work with the cluster?</p>\n",
      "\n",
      "<p>What software is available to use with my class on the cluster?</p>\n",
      "\n",
      "<p>My class needs access to a specific software application that I do not see installed on the cluster or Open OnDemand (OOD). What should I do?</p>\n",
      "\n",
      "<p>I just need my class to access Open OnDemand. How do I request that?</p>\n",
      "\n",
      "<p>I</p>\n",
      "\n",
      "<p>Source: https://rc-docs.northeastern.edu/en/latest/classroom/class_use.html</p>\n",
      "\n",
      "Question:\n",
      "What is the Scheduling Policies for HPC cluster?\n",
      "\n",
      "Search query:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "<Instructions>\n",
      "Important:\n",
      "Answer with the facts listed in the list of sources below. If there isn't enough information below, say you don't know.\n",
      "If asking a clarifying question to the user would help, ask the question.\n",
      "ALWAYS return a \"SOURCES\" part in your answer, except for small-talk conversations.\n",
      "\n",
      "Question: What is the Scheduling Policies for HPC cluster?\n",
      "Sources:\n",
      "---------------------\n",
      "    Content:  resources to become available.\n",
      "\n",
      "Resource Reservation#The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.\n",
      "\n",
      "Scheduling Policy#A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their priority, resource requirements, and other factors.\n",
      "\n",
      "Scratch Space#Temporary storage that allows users to store intermediate data during job execution. Data in scratch space is not preserved between jobs.\n",
      "\n",
      "Storage Cluster#A set of networked storage devices used to provide centralized and scalable storage solutions for the HPC environment.\n",
      "\n",
      "Scheduler#A program that manages the cluster’s resources and allocates them to jobs based on priority, requested resources, and fair use policies.\n",
      "\n",
      "Singularity#A containerization platform commonly used in HPC environments. It allows users to create and run containers focusing on security and compatibility, making it suitable for running scientific applications.\n",
      "\n",
      "Slurm#An open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small HPC clusters.\n",
      "\n",
      "Task#A unit of work within a job that can be executed independently. A job can consist of multiple tasks.\n",
      "\n",
      "VPN#A technology that creates a secure and encrypted connection over a public network, such as the Internet. It often provides remote access to HPC clusters, ensuring data privacy and security during remote cluster interactions.\n",
      "\n",
      "\n",
      "\n",
      "This glossary is not exhaustive. If you come across a term not listed here, please check the specific section of the documentation or ask in our User Community and Forums.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Next\n",
      "\n",
      "Change Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Previous\n",
      "\n",
      "Frequently Asked Questions (FAQs)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                Copyright © 2023, RC\n",
      "            \n",
      "            Made with \n",
      "            Furo\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/glossary.html\n",
      "\n",
      "Content:  times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.\n",
      "\n",
      "By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.\n",
      "\n",
      "Job Script#A file that contains a series of commands that the HPC cluster will execute.\n",
      "\n",
      "Login Node#A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.\n",
      "\n",
      "Module#In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.\n",
      "\n",
      "Message Passing Interface (MPI)#A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.\n",
      "\n",
      "Node#A single machine within a cluster. A node can have multiple processors and its memory and storage.\n",
      "\n",
      "Node Allocation#The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.\n",
      "\n",
      "Open OnDemand (OOD)#A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.\n",
      "\n",
      "Overcommitment#Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.\n",
      "\n",
      "Package Manager#A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.\n",
      "\n",
      "Parallel Computing#A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.\n",
      "\n",
      "Partition#A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.\n",
      "\n",
      "Quota#A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.\n",
      "\n",
      "Queue#A waiting line for jobs ready to be executed but waiting for resources to become available.\n",
      "\n",
      "Resource Reservation#The process of specifying resources required for a job in advance to ensure availability\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/glossary.html\n",
      "\n",
      "Content:  times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.\n",
      "\n",
      "By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.\n",
      "\n",
      "Job Script#A file that contains a series of commands that the HPC cluster will execute.\n",
      "\n",
      "Login Node#A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.\n",
      "\n",
      "Module#In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.\n",
      "\n",
      "Message Passing Interface (MPI)#A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.\n",
      "\n",
      "Node#A single machine within a cluster. A node can have multiple processors and its memory and storage.\n",
      "\n",
      "Node Allocation#The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.\n",
      "\n",
      "Open OnDemand (OOD)#A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.\n",
      "\n",
      "Overcommitment#Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.\n",
      "\n",
      "Package Manager#A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.\n",
      "\n",
      "Parallel Computing#A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.\n",
      "\n",
      "Partition#A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.\n",
      "\n",
      "Quota#A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.\n",
      "\n",
      "Queue#A waiting line for jobs ready to be executed but waiting for resources to become available.\n",
      "\n",
      "Resource Reservation#The process of specifying resources required for a job in advance to ensure availability\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/glossary.html\n",
      "---------------------\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: Can you tell me what is a cluster\n",
      "Assistant: <p>Classroom HPC: FAQ</p>\n",
      "\n",
      "<p>How can I use Discovery with my class?</p>\n",
      "\n",
      "<p>How do I get my class access to the cluster?</p>\n",
      "\n",
      "<p>Is there any training on the cluster for my class?</p>\n",
      "\n",
      "<p>Do my students have to learn Linux to work with the cluster?</p>\n",
      "\n",
      "<p>What software is available to use with my class on the cluster?</p>\n",
      "\n",
      "<p>My class needs access to a specific software application that I do not see installed on the cluster or Open OnDemand (OOD). What should I do?</p>\n",
      "\n",
      "<p>I just need my class to access Open OnDemand. How do I request that?</p>\n",
      "\n",
      "<p>I</p>\n",
      "\n",
      "<p>Source: https://rc-docs.northeastern.edu/en/latest/classroom/class_use.html</p>\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Question from user :  What is the Scheduling Policies for HPC cluster? \n",
      "\n",
      "Reply from ChatBot :  \n",
      "Content:  times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.\n",
      "\n",
      "By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.\n",
      "\n",
      "Job Script#A file that contains a series of commands that the HPC cluster will execute.\n",
      "\n",
      "Login Node#A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.\n",
      "\n",
      "Module#In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.\n",
      "\n",
      "Node#A single machine within a cluster. A node can have multiple processors and its memory and storage.\n",
      "\n",
      "Node Allocation#The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.\n",
      "\n",
      "Overcommitment#Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.\n",
      "\n",
      "Partition#A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.\n",
      "\n",
      "Partition#A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.\n",
      "\n",
      "Partition#A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.\n",
      "\n",
      "Partition#A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.\n",
      "\n",
      "Partition#A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.\n",
      "\n",
      "Partition#A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.\n",
      "\n",
      "Partition#A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.\n",
      "\n",
      "Partition#A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.\n",
      "\n",
      "Partition#A division of the cluster resources. Each\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the Scheduling Policies for HPC cluster?\"\n",
    "result = chain({\"question\": query})\n",
    "\n",
    "\n",
    "print(\"Question from user : \" , query ,\"\\n\")\n",
    "print(\"Reply from ChatBot : \" , result['answer'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eb8a097c-ac1c-46ba-8441-78db484c653c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationTokenBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='Can you tell me what is a cluster'), AIMessage(content='<p>Classroom HPC: FAQ</p>\\n\\n<p>How can I use Discovery with my class?</p>\\n\\n<p>How do I get my class access to the cluster?</p>\\n\\n<p>Is there any training on the cluster for my class?</p>\\n\\n<p>Do my students have to learn Linux to work with the cluster?</p>\\n\\n<p>What software is available to use with my class on the cluster?</p>\\n\\n<p>My class needs access to a specific software application that I do not see installed on the cluster or Open OnDemand (OOD). What should I do?</p>\\n\\n<p>I just need my class to access Open OnDemand. How do I request that?</p>\\n\\n<p>I</p>\\n\\n<p>Source: https://rc-docs.northeastern.edu/en/latest/classroom/class_use.html</p>'), HumanMessage(content='What is the Scheduling Policies for HPC cluster?'), AIMessage(content='\\nContent:  times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.\\n\\nBy adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.\\n\\nJob Script#A file that contains a series of commands that the HPC cluster will execute.\\n\\nLogin Node#A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.\\n\\nModule#In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.\\n\\nNode#A single machine within a cluster. A node can have multiple processors and its memory and storage.\\n\\nNode Allocation#The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.\\n\\nOvercommitment#Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.\\n\\nPartition#A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.\\n\\nPartition#A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.\\n\\nPartition#A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.\\n\\nPartition#A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.\\n\\nPartition#A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.\\n\\nPartition#A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.\\n\\nPartition#A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.\\n\\nPartition#A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.\\n\\nPartition#A division of the cluster resources. Each')]), input_key='question', return_messages=True, llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x2acec45e9220>, model_kwargs={'temperature': 0}), memory_key='chat_history', max_token_limit=1000)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0689077e-f506-463f-8c17-eb75b2a1e257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850efdb7-845b-4fea-84f1-a42810b0c053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "62a72dc0-7fe2-4496-b232-95523a632361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom langchain import PromptTemplate,  LLMChain\\n\\ntemplate = \"\"\"\\n<Instructions>\\nImportant:\\nYou are an intelligent chatbot. Answer the question with the facts listed in Content below. If there isn\\'t enough information below, say you don\\'t know.\\n\\nQuestion: {question}\\nContent:   {content}\\n\"\"\"\\nprompt = PromptTemplate(template=template, input_variables=[\"question\",\"content\"])\\n\\n\\nllm_chain = LLMChain(prompt=prompt, llm=hf_llm)\\nretriever = vectordb.as_retriever(search_kwargs={\"k\":4})\\nqa = RetrievalQA.from_chain_type(llm=hf_llm, chain_type=\"stuff\",retriever=retriever)\\n\\n\\n\\n\\nquestion = \"What is the Scheduling Policies for HPC cluster?\" \\n\\nimport time\\nstart_time = time.time()\\ncontent = \\nprint(llm_chain.run(question , content))\\n\\nend_time = time.time()\\nelapsed_time = end_time - start_time\\nprint(\"Time take : \" , elapsed_time)\\n\\n\\n'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to get one shot eanswer based on context from retrival db \n",
    "'''\n",
    "from langchain import PromptTemplate,  LLMChain\n",
    "\n",
    "template = \"\"\"\n",
    "<Instructions>\n",
    "Important:\n",
    "You are an intelligent chatbot. Answer the question with the facts listed in Content below. If there isn't enough information below, say you don't know.\n",
    "\n",
    "Question: {question}\n",
    "Content:   {content}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\",\"content\"])\n",
    "\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=hf_llm)\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\":4})\n",
    "qa = RetrievalQA.from_chain_type(llm=hf_llm, chain_type=\"stuff\",retriever=retriever)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "question = \"What is the Scheduling Policies for HPC cluster?\" \n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "content = \n",
    "print(llm_chain.run(question , content))\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Time take : \" , elapsed_time)\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a83cef4-bd28-4211-babe-bcc67aa81318",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Chatbot Environment",
   "language": "python",
   "name": "chatbot_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
