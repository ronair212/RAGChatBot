{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47b591a6-0629-4943-896e-626534d81adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-16 15:26:11.665603: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-16 15:26:13.421630: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-10-16 15:26:17.529250: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /device:GPU:0 with 61601 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:c1:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/device:GPU:0']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d07e60c-79bf-4898-a746-a6e1456cf7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91b96f2d-6340-410d-a784-7dd50d6d5e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "900b2960-9d29-4a66-950e-d8c171fda643",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cb3bbcb-2f8a-450d-92af-f520b95c1315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Cohere API Key: ········································\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3c316a5-485b-40e4-bc17-ef0d17acf66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OPENAI API Key: ···················································\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OPENAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf4e987c-f100-4783-b559-f31bde6cb67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "\n",
    "urls = [\"https://rc-docs.northeastern.edu/en/latest/welcome/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/welcome/welcome.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/welcome/services.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/welcome/gettinghelp.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/welcome/introtocluster.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/welcome/casestudiesandtestimonials.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/get_access.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/accountmanager.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/connectingtocluster/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/connectingtocluster/mac.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/connectingtocluster/windows.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/first_steps/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/first_steps/passwordlessssh.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/first_steps/shellenvironment.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/first_steps/usingbash.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/hardware/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/hardware/hardware_overview.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/hardware/partitions.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/introduction.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/accessingood.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/desktopood.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/fileexplore.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/jupyterlab.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/understandingqueuing.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/workingwithgpus.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/recurringjobs.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/debuggingjobs.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/discovery_storage.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/transferringdata.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/globus.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/databackup.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/securityandcompliance.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/modules.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/mpi.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/r.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/matlab.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/packagemanagers/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/packagemanagers/conda.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/packagemanagers/spack.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/fromsource/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/fromsource/makefile.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/fromsource/cmake.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/introductiontoslurm.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmcommands.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmrunningjobs.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmmonitoringandmanaging.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmscripts.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmarray.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmbestpractices.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/classroom/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/classroom/class_use.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/classroom/cps_ood.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/classroom/classroomexamples.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/best-practices/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/best-practices/homequota.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/best-practices/checkpointing.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/best-practices/optimizingperformance.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/best-practices/software.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/tutorialsandtraining/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/tutorialsandtraining/canvasandgithub.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/faq.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/glossary.html\",\n",
    "]\n",
    "loader = WebBaseLoader(urls)\n",
    "data = loader.load()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78c149f5-e805-4e22-a4ef-203f517071bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "encoding_name = tiktoken.get_encoding(\"cl100k_base\")\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fff02684-83d8-452e-aca9-3e24d4993e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "text_splitter = TokenTextSplitter(chunk_size=500, chunk_overlap=25)\n",
    "docs = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b37a4ba-32f6-43b7-a09f-7cf48b4e7ede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d43a98a2-94c3-4c46-82b2-83895f946d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_INSTRUCTOR_XL = \"hkunlp/instructor-xl\"\n",
    "EMB_SBERT_MPNET_BASE = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "039ca58e-65a5-4780-9c91-d2b3bc3f67a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_FLAN_T5_XXL = \"google/flan-t5-xxl\"\n",
    "LLM_FLAN_T5_XL = \"google/flan-t5-xl\"\n",
    "LLM_FASTCHAT_T5_XL = \"lmsys/fastchat-t5-3b-v1.0\"\n",
    "LLM_FLAN_T5_SMALL = \"google/flan-t5-small\"\n",
    "LLM_FLAN_T5_BASE = \"google/flan-t5-base\"\n",
    "LLM_FLAN_T5_LARGE = \"google/flan-t5-large\"\n",
    "LLM_FALCON_7B_INSTRUCT = \"tiiuae/falcon-7b-instruct\"\n",
    "LLM_FALCON_40B_INSTRUCT = \"tiiuae/falcon-40b-instruct\"\n",
    "LLM_FALCON_7B = \"tiiuae/falcon-7b\"\n",
    "LLM_FALCON_40B = \"tiiuae/falcon-40b\"\n",
    "LLM_LLAMA2_70B_INSTRUCT = \"upstage/Llama-2-70b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d839bbf-4377-4cdc-9c23-ff5e2cb79c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir='/work/rc/projects/chatbot/models'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae518ccf-4075-46f8-893e-5d715c43658a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"persist_directory\":None,\n",
    "          \"load_in_8bit\":False,\n",
    "          \"embedding\" : EMB_SBERT_MPNET_BASE,\n",
    "          \"llm\":LLM_FALCON_40B,\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78a52923-6b76-4a50-a4bd-9bc886af2a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/work/rc/projects/chatbot/models'\n",
    "#cache_folder=os.getenv('SENTENCE_TRANSFORMERS_HOME')\n",
    "os.environ['SENTENCE_TRANSFORMERS_HOME'] = '/work/rc/projects/chatbot/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c434a8c-1ce9-4aaf-9568-3581faa68403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sbert_mpnet():\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        return HuggingFaceEmbeddings(model_name=EMB_SBERT_MPNET_BASE, cache_folder=cache_dir, model_kwargs={\"device\": device})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e84c01f7-80cf-44cc-bb16-e973c4b7e2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-16 15:47:30.602312: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-16 15:47:31.749781: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c16a556c-29a4-4283-a14c-d6c5762227ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_llama2_70b_instruct(load_in_8bit=True):\n",
    "        model = LLM_LLAMA2_70B_INSTRUCT\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model , cache_dir=cache_dir)\n",
    "        hf_pipeline = pipeline(\n",
    "                task=\"text-generation\",\n",
    "                model = model,\n",
    "                do_sample=True, #Whether or not to use sampling ; use greedy decoding otherwise.\n",
    "                tokenizer = tokenizer,\n",
    "                #trust_remote_code = True,\n",
    "                max_new_tokens=500, #The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n",
    "                #cache_dir=cache_dir,\n",
    "                model_kwargs={\n",
    "                    \"device_map\": \"auto\", \n",
    "                    \"load_in_8bit\": load_in_8bit, \n",
    "                    \"max_length\": 512, \n",
    "                    \"temperature\": 0.01,\n",
    "                    \n",
    "                    \"torch_dtype\":torch.bfloat16,\n",
    "                    }\n",
    "            )\n",
    "        return hf_pipeline\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6ded4db-00ab-457d-9ad3-30417b6f4b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_falcon_40b(load_in_8bit=True):\n",
    "        model = LLM_FALCON_40B\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model , cache_dir=cache_dir)\n",
    "        hf_pipeline = pipeline(\n",
    "                task=\"text-generation\",\n",
    "                model = model,\n",
    "                do_sample=True, #Whether or not to use sampling ; use greedy decoding otherwise.\n",
    "                tokenizer = tokenizer,\n",
    "                #trust_remote_code = True,\n",
    "                max_new_tokens=500, #The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n",
    "                #cache_dir=cache_dir,\n",
    "                model_kwargs={\n",
    "                    \"device_map\": \"auto\", \n",
    "                    \"load_in_8bit\": load_in_8bit, \n",
    "                    \"max_length\": 512, \n",
    "                    \"temperature\": 0.01,\n",
    "                    \n",
    "                    \"torch_dtype\":torch.bfloat16,\n",
    "                    }\n",
    "            )\n",
    "        return hf_pipeline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca182968-c28f-41e5-a0ff-de1a03008a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_falcon_40b_instruct(load_in_8bit=True):\n",
    "        model = LLM_FALCON_40B_INSTRUCT\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model , cache_dir=cache_dir)\n",
    "        hf_pipeline = pipeline(\n",
    "                task=\"text-generation\",\n",
    "                model = model,\n",
    "                do_sample=True, #Whether or not to use sampling ; use greedy decoding otherwise.\n",
    "                tokenizer = tokenizer,\n",
    "                #trust_remote_code = True,\n",
    "                max_new_tokens=100, #The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n",
    "                #cache_dir=cache_dir,\n",
    "                model_kwargs={\n",
    "                    \"device_map\": \"auto\", \n",
    "                    \"load_in_8bit\": load_in_8bit, \n",
    "                    \"max_length\": 512, \n",
    "                    \"temperature\": 0.01,\n",
    "                    \n",
    "                    \"torch_dtype\":torch.bfloat16,\n",
    "                    }\n",
    "            )\n",
    "        return hf_pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c69bf69c-2e78-4690-b534-875648744310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_falcon_7b(load_in_8bit=True):\n",
    "        model = LLM_FALCON_7B\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model , cache_dir=cache_dir)\n",
    "        hf_pipeline = pipeline(\n",
    "                task=\"text-generation\",\n",
    "                model = model,\n",
    "                do_sample=True, #Whether or not to use sampling ; use greedy decoding otherwise.\n",
    "                tokenizer = tokenizer,\n",
    "                #trust_remote_code = True,\n",
    "                max_new_tokens=100, #The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n",
    "                #cache_dir=cache_dir,\n",
    "                model_kwargs={\n",
    "                    \"device_map\": \"auto\", \n",
    "                    \"load_in_8bit\": load_in_8bit, \n",
    "                    \"max_length\": 512, \n",
    "                    \"temperature\": 0.01,\n",
    "                    \n",
    "                    \"torch_dtype\":torch.bfloat16,\n",
    "                    }\n",
    "            )\n",
    "        return hf_pipeline\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfc19c2e-57f7-4bc4-9d1b-f20687fb0606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_falcon_7b_instruct(load_in_8bit=True):\n",
    "        model = LLM_FALCON_7B_INSTRUCT\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model , cache_dir=cache_dir)\n",
    "        hf_pipeline = pipeline(\n",
    "                task=\"text-generation\",\n",
    "                model = model,\n",
    "                do_sample=True, #Whether or not to use sampling ; use greedy decoding otherwise.\n",
    "                tokenizer = tokenizer,\n",
    "                #trust_remote_code = True,\n",
    "                max_new_tokens=500, #The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n",
    "                #cache_dir=cache_dir,\n",
    "                model_kwargs={\n",
    "                    \"device_map\": \"auto\", \n",
    "                    \"load_in_8bit\": load_in_8bit, \n",
    "                    \"max_length\": 512, \n",
    "                    \"temperature\": 0.01,\n",
    "                    \n",
    "                    \"torch_dtype\":torch.bfloat16,\n",
    "                    }\n",
    "            )\n",
    "        return hf_pipeline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b66995a-1b3d-4989-94e5-e96c848896a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_flan_t5_base(load_in_8bit=True):\n",
    "        # Wrap it in HF pipeline for use with LangChain\n",
    "        model=\"google/flan-t5-base\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model, cache_dir=cache_dir)\n",
    "        return pipeline(\n",
    "            task=\"text2text-generation\",\n",
    "            model=model,\n",
    "            tokenizer = tokenizer,\n",
    "            max_new_tokens=100,\n",
    "            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.}\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca743027-6ffc-40ae-b624-a05ad8b307ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_in_8bit = config[\"load_in_8bit\"]\n",
    "if config[\"llm\"] == LLM_FLAN_T5_BASE:\n",
    "    llm = create_flan_t5_base(load_in_8bit=load_in_8bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32bbeab2-1d7c-4aad-ba51-14a19da0d58c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9bda7cea0094f789d9513808f54d562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b6d604613934263862932c067d351f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a8e6c64585549758e2e0686d94773b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "523ebd7354144c3d826500665a41645a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d79b643a318e49b7b738d8d1d6232e9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d864f2e45854dd0864b51b7b1b38cd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b06afcbad24ecfb6ed08c8a3f775bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7579de96ca94d72ad2f43c790153e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "711487a2828f42af8ff209b33debaeef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/rc/projects/chatbot/conda_env/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.01` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be3ab6d8cb6844c8bf21480883b06787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b9d6c1e1104f118a863043f8115ff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_in_8bit = config[\"load_in_8bit\"]\n",
    "\n",
    "if config[\"llm\"] == LLM_FALCON_40B:\n",
    "    llm = create_falcon_40b(load_in_8bit=load_in_8bit)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "810f0f3f-4222-4b8c-9039-14aceaf0c261",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_in_8bit = config[\"load_in_8bit\"]\n",
    "\n",
    "if config[\"llm\"] == LLM_FALCON_40B_INSTRUCT:\n",
    "    llm = create_falcon_40b_instruct(load_in_8bit=load_in_8bit)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a89e1adb-3d0d-43c2-bc65-2ba44c1a5d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_in_8bit = config[\"load_in_8bit\"]\n",
    "\n",
    "if config[\"llm\"] == LLM_FALCON_7B_INSTRUCT:\n",
    "    llm = create_falcon_7b_instruct(load_in_8bit=load_in_8bit)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21e0a433-5876-4ea4-9c60-d48669464314",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_in_8bit = config[\"load_in_8bit\"]\n",
    "\n",
    "if config[\"llm\"] == LLM_LLAMA2_70B_INSTRUCT:\n",
    "    llm = create_llama2_70b_instruct(load_in_8bit=load_in_8bit)\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4cc8d24e-778c-492d-937e-394c9743c538",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_in_8bit = config[\"load_in_8bit\"]\n",
    "\n",
    "if config[\"llm\"] == LLM_FALCON_7B:\n",
    "    llm = create_falcon_7b(load_in_8bit=load_in_8bit)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10f0356d-b829-4c9a-89c6-45b197a0ff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "#embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "# Equivalent to SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "56928516-e35c-4cb0-aa09-7c87ba7dbc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Qdrant\n",
    "\n",
    "Qdrantdb = Qdrant.from_documents(\n",
    "    docs,\n",
    "    embeddings,\n",
    "    path=\"/work/rc/projects/chatbot/chatbotrc/notebooks/RAG/tmp/local_qdrant\",\n",
    "    collection_name=\"RC_documents\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09546165-5f43-4dcf-8baa-7fa6b9058d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "\n",
    "\n",
    "hf_llm = HuggingFacePipeline(pipeline = llm, model_kwargs = {'temperature':0})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "93a8a801-ab16-471d-8ae3-1f3c9b63559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "#from langchain.vectorstores import Pinecone\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e61d42cb-2244-4f7c-8473-ea2c5b10fddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.vectorstore import VectorStoreRetriever\n",
    "retriever = VectorStoreRetriever(vectorstore=Qdrantdb, search_type=\"mmr\", search_kwargs={'k': 4, 'fetch_k': 10},)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a8a74bb3-9624-4922-987b-b223c0c3b445",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor = CohereRerank() #LLMChainExtractor,LLMChainFilter,EmbeddingsFilter\n",
    "# will iterate over the initially returned documents and extract from each only the content that is relevant to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d16eabe-b278-40bf-868c-da2d4a5e5c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up cohere's reranker\n",
    "''' instead of immediately returning retrieved documents as-is, \n",
    "you can compress them using the context of the given query, so that only the relevant information is returned. '''\n",
    "reranker = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "09181435-ced9-4a61-b0d8-3d61d1006e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "885e86e4-bcb2-4e79-b254-633f54025024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "196b2abf-2122-45b6-bd18-e498c17542c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_callback = StdOutCallbackHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8916f339-335b-4c5c-b20f-5a064d941751",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=hf_llm,memory_key=\"chat_history\", return_messages=True,input_key='question',max_token_limit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7d78e8a8-b390-46f8-83f7-17c5ea4cedaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "\n",
    "PromptTemplates = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"question\"],\n",
    "    template=\"\"\"\n",
    "Below is a summary of the conversation so far, and a new question asked by the user that needs to be answered by searching in a knowledge base.\n",
    "Generate a search query based on the conversation and the new question. Frame the question in a way to get good similarity search results from a vector database.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Search query:\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "system_message_prompt = SystemMessagePromptTemplate(prompt=PromptTemplates)\n",
    "\n",
    "chat_prompt_for_ques = ChatPromptTemplate.from_messages(\n",
    "    [system_message_prompt])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7fa4ae65-6f2e-4790-87f5-86336ec4c2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e37d246e-0efa-47e0-97b1-2a386f851af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generator = LLMChain(llm=hf_llm, prompt=chat_prompt_for_ques, verbose=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c410695-666c-4371-9c81-846757a3c597",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer_Generator_Prompt= '''\n",
    "<Instructions>\n",
    "Important:\n",
    "Answer with the facts listed in the list of sources below. If there isn't enough information below, say you don't know.\n",
    "If asking a clarifying question to the user would help, ask the question.\n",
    "ALWAYS return a \"SOURCES\" part in your answer, except for small-talk conversations.\n",
    "\n",
    "Question: {question}\n",
    "Sources:\n",
    "---------------------\n",
    "    {summaries}\n",
    "---------------------\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "43d0fb39-3d0f-4b62-be0f-d1ed0c72bc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "\n",
    "chat_prompt = PromptTemplate(template=Answer_Generator_Prompt, input_variables=[\"question\", \"summaries\",\"chat_history\"])\n",
    "\n",
    "answer_chain = load_qa_with_sources_chain(hf_llm, chain_type=\"stuff\", verbose=True,prompt=chat_prompt)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "766f3709-9727-444a-9dd3-9ced62d5a454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "\n",
    "chain = ConversationalRetrievalChain(\n",
    "            retriever=reranker,\n",
    "            question_generator=question_generator,\n",
    "            combine_docs_chain=answer_chain,\n",
    "            verbose=True,\n",
    "            memory=memory,\n",
    "            rephrase_question=False\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edfd9d4-327f-49e1-8df1-d5f32ec2f19c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/rc/projects/chatbot/conda_env/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.01` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/work/rc/projects/chatbot/conda_env/lib/python3.9/site-packages/transformers/generation/utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "The current implementation of Falcon calls `torch.scaled_dot_product_attention` directly, this will be deprecated in the future in favor of the `BetterTransformer` API. Please install the latest optimum library with `pip install -U optimum` and call `model.to_bettertransformer()` to benefit from `torch.scaled_dot_product_attention` and future performance optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "<Instructions>\n",
      "Important:\n",
      "Answer with the facts listed in the list of sources below. If there isn't enough information below, say you don't know.\n",
      "If asking a clarifying question to the user would help, ask the question.\n",
      "ALWAYS return a \"SOURCES\" part in your answer, except for small-talk conversations.\n",
      "\n",
      "Question: Can you tell me what is a cluster\n",
      "Sources:\n",
      "---------------------\n",
      "    Content:  duration. However, the cluster is a shared resource, so we ask that you test out your assignments on the cluster so that you are requesting an appropriate amount of resources for your class. We can always increase your reservation if a class needs more resources due to higher-than-expected use. But, if a reservation is not being used to capacity, we will ask you to review the need for the requested resources and adjust the reservation accordingly. We understand that sometimes it takes time to determine precisely your resource needs. Still, we do need to keep a reservation to a reasonable limit to keep the shared resources available to all users.\n",
      "\n",
      "\n",
      "How long do my students have access to the cluster?#\n",
      "Students will have access to the cluster for the whole class duration. They must request an individual account if they want to continue accessing the cluster after that period.\n",
      "\n",
      "\n",
      "How do I get an account on the cluster?#\n",
      "If you are a professor or instructor at Northeastern, you can request an account on the cluster. See Sponsor Approval Process for more information.\n",
      "\n",
      "\n",
      "How do my students get help with the cluster?#\n",
      "You and your students can submit a Get Assistance with Research Computing ticket or email rchelp@northeastern.edu.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Next\n",
      "\n",
      "CPS Class Instructions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Previous\n",
      "\n",
      "Classroom Resources\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                Copyright © 2023, RC\n",
      "            \n",
      "            Made with \n",
      "            Furo\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            On this page\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classroom HPC: FAQ\n",
      "How can I use Discovery with my class?\n",
      "How do I get my class access to the cluster?\n",
      "Is there any training on the cluster for my class?\n",
      "Do my students have to learn Linux to work with the cluster?\n",
      "What software is available to use with my class on the cluster?\n",
      "My class needs access to a specific software application that I do not see installed on the cluster or Open OnDemand (OOD). What should I do?\n",
      "I just need my class to access Open OnDemand. How do I request that?\n",
      "I\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/classroom/class_use.html\n",
      "\n",
      "Content:  duration. However, the cluster is a shared resource, so we ask that you test out your assignments on the cluster so that you are requesting an appropriate amount of resources for your class. We can always increase your reservation if a class needs more resources due to higher-than-expected use. But, if a reservation is not being used to capacity, we will ask you to review the need for the requested resources and adjust the reservation accordingly. We understand that sometimes it takes time to determine precisely your resource needs. Still, we do need to keep a reservation to a reasonable limit to keep the shared resources available to all users.\n",
      "\n",
      "\n",
      "How long do my students have access to the cluster?#\n",
      "Students will have access to the cluster for the whole class duration. They must request an individual account if they want to continue accessing the cluster after that period.\n",
      "\n",
      "\n",
      "How do I get an account on the cluster?#\n",
      "If you are a professor or instructor at Northeastern, you can request an account on the cluster. See Sponsor Approval Process for more information.\n",
      "\n",
      "\n",
      "How do my students get help with the cluster?#\n",
      "You and your students can submit a Get Assistance with Research Computing ticket or email rchelp@northeastern.edu.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Next\n",
      "\n",
      "CPS Class Instructions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Previous\n",
      "\n",
      "Classroom Resources\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                Copyright © 2023, RC\n",
      "            \n",
      "            Made with \n",
      "            Furo\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            On this page\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classroom HPC: FAQ\n",
      "How can I use Discovery with my class?\n",
      "How do I get my class access to the cluster?\n",
      "Is there any training on the cluster for my class?\n",
      "Do my students have to learn Linux to work with the cluster?\n",
      "What software is available to use with my class on the cluster?\n",
      "My class needs access to a specific software application that I do not see installed on the cluster or Open OnDemand (OOD). What should I do?\n",
      "I just need my class to access Open OnDemand. How do I request that?\n",
      "I\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/classroom/class_use.html\n",
      "\n",
      "Content:  duration. However, the cluster is a shared resource, so we ask that you test out your assignments on the cluster so that you are requesting an appropriate amount of resources for your class. We can always increase your reservation if a class needs more resources due to higher-than-expected use. But, if a reservation is not being used to capacity, we will ask you to review the need for the requested resources and adjust the reservation accordingly. We understand that sometimes it takes time to determine precisely your resource needs. Still, we do need to keep a reservation to a reasonable limit to keep the shared resources available to all users.\n",
      "\n",
      "\n",
      "How long do my students have access to the cluster?#\n",
      "Students will have access to the cluster for the whole class duration. They must request an individual account if they want to continue accessing the cluster after that period.\n",
      "\n",
      "\n",
      "How do I get an account on the cluster?#\n",
      "If you are a professor or instructor at Northeastern, you can request an account on the cluster. See Sponsor Approval Process for more information.\n",
      "\n",
      "\n",
      "How do my students get help with the cluster?#\n",
      "You and your students can submit a Get Assistance with Research Computing ticket or email rchelp@northeastern.edu.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Next\n",
      "\n",
      "CPS Class Instructions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Previous\n",
      "\n",
      "Classroom Resources\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                Copyright © 2023, RC\n",
      "            \n",
      "            Made with \n",
      "            Furo\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            On this page\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classroom HPC: FAQ\n",
      "How can I use Discovery with my class?\n",
      "How do I get my class access to the cluster?\n",
      "Is there any training on the cluster for my class?\n",
      "Do my students have to learn Linux to work with the cluster?\n",
      "What software is available to use with my class on the cluster?\n",
      "My class needs access to a specific software application that I do not see installed on the cluster or Open OnDemand (OOD). What should I do?\n",
      "I just need my class to access Open OnDemand. How do I request that?\n",
      "I\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/classroom/class_use.html\n",
      "---------------------\n",
      "\n",
      "Chat History:\n",
      "\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "query = \"Can you tell me what is a cluster\"\n",
    "result = chain({\"question\": query})\n",
    "\n",
    "\n",
    "print(\"Question from user : \" , query ,\"\\n\")\n",
    "print(\"Reply from ChatBot : \" , result['answer'])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45de448b-fdad-47e2-b9d2-0d3c00833bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the Scheduling Policies for HPC cluster?\"\n",
    "result = chain({\"question\": query})\n",
    "\n",
    "\n",
    "print(\"Question from user : \" , query ,\"\\n\")\n",
    "print(\"Reply from ChatBot : \" , result['answer'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8a097c-ac1c-46ba-8441-78db484c653c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0689077e-f506-463f-8c17-eb75b2a1e257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850efdb7-845b-4fea-84f1-a42810b0c053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a72dc0-7fe2-4496-b232-95523a632361",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get one shot eanswer based on context from retrival db \n",
    "'''\n",
    "from langchain import PromptTemplate,  LLMChain\n",
    "\n",
    "template = \"\"\"\n",
    "<Instructions>\n",
    "Important:\n",
    "You are an intelligent chatbot. Answer the question with the facts listed in Content below. If there isn't enough information below, say you don't know.\n",
    "\n",
    "Question: {question}\n",
    "Content:   {content}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\",\"content\"])\n",
    "\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=hf_llm)\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\":4})\n",
    "qa = RetrievalQA.from_chain_type(llm=hf_llm, chain_type=\"stuff\",retriever=retriever)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "question = \"What is the Scheduling Policies for HPC cluster?\" \n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "content = \n",
    "print(llm_chain.run(question , content))\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Time take : \" , elapsed_time)\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a83cef4-bd28-4211-babe-bcc67aa81318",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Chatbot Environment",
   "language": "python",
   "name": "chatbot_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
