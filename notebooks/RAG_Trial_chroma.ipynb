{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNSwbz8svgCi",
        "outputId": "edbdd442-5caa-4c86-b902-da4a5f72f106"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pinecone-client\n",
            "  Downloading pinecone_client-2.2.4-py3-none-any.whl (179 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/179.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m174.1/179.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.4/179.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (6.0.1)\n",
            "Collecting loguru>=0.5.0 (from pinecone-client)\n",
            "  Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.5.0)\n",
            "Collecting dnspython>=2.0.0 (from pinecone-client)\n",
            "  Downloading dnspython-2.4.2-py3-none-any.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.8.2)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.0.6)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.5.3->pinecone-client) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client) (2023.7.22)\n",
            "Installing collected packages: loguru, dnspython, pinecone-client\n",
            "Successfully installed dnspython-2.4.2 loguru-0.7.2 pinecone-client-2.2.4\n"
          ]
        }
      ],
      "source": [
        "!pip install pinecone-client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuNwBfb6BngZ",
        "outputId": "30409fdb-2773-411e-dcfc-170ab7fc63f7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.309-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.21)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.5)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.40 (from langchain)\n",
            "  Downloading langsmith-0.0.42-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.1 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.309 langsmith-0.0.42 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwJCPFhtCBmi",
        "outputId": "aa6b6914-32a8-4f4d-e68c-84e5077d0fad"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cohere"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwqNT7T0CF-O",
        "outputId": "9fc123df-4163-4f34-f81f-610513bc1fe5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cohere\n",
            "  Downloading cohere-4.27-py3-none-any.whl (47 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/47.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp<4.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (3.8.5)\n",
            "Collecting backoff<3.0,>=2.0 (from cohere)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting fastavro==1.8.2 (from cohere)\n",
            "  Downloading fastavro-1.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib_metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (6.8.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.25.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.31.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.0.6)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata<7.0,>=6.0->cohere) (3.17.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (2023.7.22)\n",
            "Installing collected packages: fastavro, backoff, cohere\n",
            "Successfully installed backoff-2.2.1 cohere-4.27 fastavro-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQl_Bfl8CLL1",
        "outputId": "cc07de5a-2e7c-4b28-a66d-5906fea3a052"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m71.7/77.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import pinecone\n"
      ],
      "metadata": {
        "id": "3oMYj3f-vmG7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pinecone.init(api_key=\"b360318b-4fc8-4580-bf6c-d88959179985\",\n",
        "#              environment=\"us-west1-gcp-free\")\n"
      ],
      "metadata": {
        "id": "njgYMfbQvnjn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pinecone.whoami()"
      ],
      "metadata": {
        "id": "drKj9gcYvx36"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pinecone.list_indexes()"
      ],
      "metadata": {
        "id": "B979WTowvyGz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pinecone.list_indexes()"
      ],
      "metadata": {
        "id": "71U2rh7xwQIy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import WebBaseLoader\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "urls = [\"https://rc-docs.northeastern.edu/en/latest/runningjobs/understandingqueuing.html\",\n",
        "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\",\n",
        "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/interactiveandbatch.html\",\n",
        "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/workingwithgpus.html\",\n",
        "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/recurringjobs.html\",\n",
        "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/debuggingjobs.html\",\n",
        "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/../datamanagement/index.html\",\n",
        "]\n",
        "loader = WebBaseLoader(urls)\n",
        "data = loader.load()\n"
      ],
      "metadata": {
        "id": "mq7rbJww6Cjo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "data = \"\"\"\n",
        "\n",
        "Introduction to OOD#\n",
        "Open OnDemand (OOD) is a web portal to the Discovery cluster. A Discovery account is necessary for you to access OOD. If you need an account, see Request an account. If you already have an account, in a web browser go to http://ood.discovery.neu.edu and sign in with your Northeastern username and password.\n",
        "OOD provides you with several resources for interacting with the Discovery cluster:\n",
        "\n",
        "Launch a terminal within your web browser without needing a separate terminal program. This is an advantage if you use Windows, as otherwise, you need to download and use a separately installed program, such as MobaXterm.\n",
        "Use software applications like SAS Studio that run in your browser without further configuration. See Interactive Open OnDemand Applications for more information.\n",
        "View, download, copy, and delete files using the OOD File Explorer feature.\n",
        "\n",
        "Note\n",
        "OOD is a web-based application. You access it by using a web browser. Like many web-based applications, it has compatibility issues with specific web browsers. Use OOD with newer Chrome, Firefox, or Internet Explorer versions for optimal results. OOD does not currently support Safari or mobile devices (phones and tablets).\n",
        "\n",
        "https://rc-docs.northeastern.edu/en/latest/using-ood/accessingood.html\n",
        "\n",
        "Accessing Open OnDemand#\n",
        "Open OnDemand (OOD) is a web portal to the HPC cluster.\n",
        "This topic is for connecting to the HPC cluster through the browser application Open OnDemand. If you want to access the HPC directly on your system rather than through a browser, please see Connecting To Cluster, whether Mac or Windows.\n",
        "A cluster account is necessary for you to access OOD. If you need an account, see Getting Access. After you have created a cluster account, access the cluster through Open OnDemand (OOD) via the following steps:\n",
        "\n",
        "In a web browser, go to http://ood.discovery.neu.edu.\n",
        "At the prompt, enter your Northeastern username and password. Note that your username is the first part of your email without the @northeastern, such as j.smith.\n",
        "Press Enter or click Sign in.\n",
        "\n",
        "Watch the following video for a short tutorial. If you do not see any controls on the video,\n",
        "right-click on the video to see viewing options.\n",
        "\n",
        "  Your browser does not support the video tag.\n",
        "\n",
        "https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/index.html\n",
        "\n",
        "Interactive Open OnDemand Applications#\n",
        "\n",
        "Desktop App\n",
        "\n",
        "OOD File Explorer\n",
        "\n",
        "JupyterLab\n",
        "\n",
        "The OOD web portal provides a range of applications. Upon clicking launch, the Slurm scheduler assigns a compute node with a specified number of cores and memory. By default, applications run for one hour. If you require more than an hour, you may have to wait for Slurm to allocate resources for the duration of your request.\n",
        "\n",
        "Applications on OOD#\n",
        "\n",
        "The Open OnDemand interface offers several applications, which as of June 2023, include:\n",
        "\n",
        "JupyterLab\n",
        "RStudio (Rocker)\n",
        "Matlab\n",
        "Schrodinger (Maestro)\n",
        "Desktop\n",
        "Gaussian (GaussView)\n",
        "KNIME\n",
        "TensorBoard\n",
        "SAS\n",
        "\n",
        "These applications can be accessed from the OOD web interface’s Interactive Apps drop-down menu.\n",
        "\n",
        "Note\n",
        "Specific applications in the Interactive Apps section, particularly those with graphical user interfaces (GUIs), may require X11 forwarding and the setup of passwordless SSH. For tips and troubleshooting information on X11 forwarding setup and usage, please look at the [Using X11] section of our documentation.\n",
        "\n",
        "Additionally, we offer a selection of modified standard applications intended to support specific coursework. These applications are under the Courses menu on the OOD web interface. Please note that these course-specific applications are only accessible to students enrolled in the respective courses.\n",
        "\n",
        "Note\n",
        "Certain apps are reserved for specific research groups and are not publicly accessible, as indicated by the “Restricted” label next to the application name. If you receive an access error when attempting to open a restricted app, and you believe you should have access to it, please email rchelp@northeastern.edu with the following information: your username, research group, the app you are trying to access, and a screenshot of the error message. We will investigate and address the issue.\n",
        "\n",
        "Go to [Open On Demand] in a web browser. If prompted, enter your MyNortheastern username and password.\n",
        "Select Interactive Apps, then select the application you want to use.\n",
        "Keep the default options for most apps, then click Launch. You might have to wait a minute or two for a compute node to be available for your requested time and resource.\n",
        "\n",
        "https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/desktopood.html\n",
        "\n",
        "Desktop App#\n",
        "Open OnDemand provides a containerized desktop to run on the HPC cluster.\n",
        "The following tools and programs are accessible on our Desktop App:\n",
        "\n",
        "Slurm (for running Slurm commands via the terminal in the desktop and interacting on compute nodes)\n",
        "Module command (for loading and running HPC-ready modules)\n",
        "File explorer (able to traverse and view files that you have access to on the HPC)\n",
        "Firefox web browser\n",
        "VLC media player\n",
        "LibreOffice suite of applications (word, spreadsheet, and presentation processing)\n",
        "\n",
        "Note\n",
        "The desktop application is a Singularity container; a Singularity container cannot run inside the desktop application. It fails if users run a container-based module or program via the desktop application.\n",
        "\n",
        "https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/fileexplore.html\n",
        "\n",
        "OOD File Explorer#\n",
        "When working with the resources in OOD, your files are stored in your home directory on the storage space on the Discovery cluster. Like any file navigation system, you can work with your files and directories through the OOD Files feature, as detailed below. For example, you can download a Jupyter Notebook file in OOD that you have been working on to your local hard drive, rename a file, or delete a file you no longer need.\n",
        "\n",
        "Note\n",
        "Your home directory has a file size limit of 75GB. Please check your home directory regularly, and remove any files you do not need to make sure you have enough space.\n",
        "\n",
        "In a web browser, go to ood.discovery.neu.edu. If prompted, enter your MyNortheastern username and password.\n",
        "Select Files > Home Directory. The contents of your home directory display in a new tab.\n",
        "To download a file to your hard drive, navigate to the file you want to download,\n",
        "select the file, and click Download. If prompted by your browser,\n",
        "click OK to save your file to your hard drive.\n",
        "To navigate to another folder on the Discovery file system, click Go To,\n",
        "enter the path to the folder you want to access and click OK.\n",
        "\n",
        "Note\n",
        "From the Files > Home Directory view, the Edit button will not launch your .ipynb file in a Jupyter Notebook. It will open the file in a text editor. You must be in Jupyter Notebook to launch a .ipynb file from your /home directory. See Interactive Open OnDemand Applications to access a Jupyter Notebook through OOD.\n",
        "\n",
        "https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/jupyterlab.html\n",
        "\n",
        "JupyterLab#\n",
        "JupyterLab Notebook is one of the interactive apps on OOD. This section will provide a walk through of setting up and using this app. The general workflow is to create a virtual Python environment, ensure that JupyterLab Notebook uses your virtual environment, and reference this environment when you start the JupyterLab Notebook OOD interactive app.\n",
        "To find the JupyterLab Notebook on OOD, follow these steps:\n",
        "\n",
        "Go to [Open On Demand].\n",
        "Click on Interactive Apps.\n",
        "Select JupyterLab Notebook from the drop-down list.\n",
        "\n",
        "The OOD form for launching JupyterLab Notebook will appear.\n",
        "\n",
        "Conda virtual environment\n",
        "You can import Python packages in your JupyterLab Notebook session by creating a conda virtual environment and activating that environment when starting a JupyterLab Notebook instance.\n",
        "\n",
        "First, set up a virtual Python environment. See Creating Environments for how to set up a virtual Python environment on the HPC using the terminal.\n",
        "Type source activate <yourenvironmentname> where <yourenvironmentname> is the name of your custom environment.\n",
        "Type conda install jupyterlab -y to install JupyterLab in your environment.\n",
        "\n",
        "Using OOD to launch JupyterLab Notebook#\n",
        "\n",
        "Go to [Open On Demand].\n",
        "Click Interactive Apps, then select JupyterLab Notebook.\n",
        "Enter your Working Directory (e.g., /home/<username> or /work/<project>) that you want JupyterLab Notebook to launch in.\n",
        "Select from the Partition drop-down menu the partition you want to use for your session. Refer to Partitions for the resource restrictions for the different partitions. If you need a GPU, select the gpu partition.\n",
        "Select the compute node features for the job:\n",
        "\n",
        "In the Time field, enter the number of hour(s) needed for the job.\n",
        "Enter the memory you need for the job in the Memory (in Gb) field.\n",
        "If you selected the gpu partition from the drop-down menu, select the GPU you would like to use and the version of CUDA that you would like to use for your session under the respective drop-down menus.\n",
        "\n",
        "Select the Anaconda version you used to create your virtual Python environment in the System-wide Conda Module field.\n",
        "Check the Custom Anaconda Environment box, and enter the name of your custom virtual Python environment in the Name of Custom Conda Environment field.\n",
        "Click Launch to join the queue for a compute node. This might take a few minutes, depending on what you asked for.\n",
        "When allocated a compute node, click Connect to Jupyter.\n",
        "\n",
        "When your JupyterLab Notebook is running and open, type conda list in a cell and run the cell to confirm that the environment is your custom conda environment (you should see this on the first line). This command will also list all of your available packages.\n",
        "\n",
        "\n",
        "Understanding the Queuing System#\n",
        "The queuing system in a high-performance computing (HPC) environment manages and schedules computing tasks. Our HPC cluster uses the Slurm Workload Manager as our queuing system. This section aims to help you understand how the queuing system works and how to interact effectively.\n",
        "\n",
        "Introduction to Queuing Systems#\n",
        "The Slurm scheduler manages jobs in the queue. When you submit a job, it gets placed in the queue. The scheduler then assigns resources to the job when they become available, according to the job’s priority and the available resources.\n",
        "\n",
        "Job Submission and Scheduling#\n",
        "Jobs are submitted to the queue via a script specifying the resources required (e.g., number of CPUs, memory, and GPUs) and the commands to be executed. Once submitted, the queuing system schedules the job based on the resources requested, the current system load, and scheduling policies.\n",
        "\n",
        "Scheduling Policies**#\n",
        "Our cluster uses a fair-share scheduling policy. This means that usage is tracked for each user or group, and the system attempts to balance resource allocation over time. If a user or group has been using many resources, their job priority may be temporarily reduced to allow others to use the system. Conversely, users or groups that have used fewer resources will have their jobs prioritized.\n",
        "The following policies ensure fair use of the cluster resources:\n",
        "\n",
        "Single job size: The maximum number of nodes a single job depends on the partition (see Partitions).\n",
        "Run time limit: The maximum run time for a job depends on the partition (see Partitions).\n",
        "Priority decay: If a job remains in the queue without running for an extended period, its priority may slowly decrease.\n",
        "\n",
        "Job Priority**#\n",
        "Several factors determine job priority:\n",
        "\n",
        "Fair-share: This is based on the historical resource usage of your group. The more resources your group has used, the lower your job’s priority becomes, and vice versa.\n",
        "Job size: Smaller jobs (regarding requested nodes) typically have higher priority.\n",
        "Queue wait time: The longer a job has been in the queue, the higher its priority becomes.\n",
        "\n",
        "Job States#\n",
        "Each job in the queue has a state. The main job states are:\n",
        "\n",
        "Pending (PD): The job is waiting for resources to become available.\n",
        "Running (R): The job is currently running.\n",
        "Completed (CG): The job has been completed successfully.\n",
        "\n",
        "A complete list of job states can be found in the Slurm documentation.\n",
        "\n",
        "Monitoring the Queue**#\n",
        "You can use the following commands to interact with the queue:\n",
        "\n",
        "squeue: Displays the state of jobs or job steps. It has a wide variety of filtering, sorting, and formatting options. For example, to display your jobs:\n",
        "\n",
        "squeue -u your_username\n",
        "\n",
        "scontrol: Used to view and modify Slurm configuration and state. For example, to show the details of a specific job:\n",
        "\n",
        "scontrol show job your_job_id\n",
        "\n",
        "Tips for Efficient Queue Usage**#\n",
        "\n",
        "Request only the resources you need: Overestimating your job’s requirements can result in longer queue times.\n",
        "Break up large jobs: Large jobs tend to wait in the queue longer than small jobs. Break up large jobs into smaller ones.\n",
        "Use idle resources: Sometimes, idle resources can be used. If your job is flexible regarding start time and duration, you can use the --begin and --time options to take advantage of these idle resources.\n",
        "\n",
        "https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\n",
        "\n",
        "Job Scheduling Policies and Priorities#\n",
        "In an HPC environment, efficient job scheduling is crucial for allocating computing resources and ensuring optimal cluster utilization. Job scheduling policies and priorities determine the order in which jobs are executed and the resources they receive. Understanding these policies is essential for maximizing job efficiency and minimizing wait times.\n",
        "\n",
        "Scheduling Policies#\n",
        "\n",
        "FIFO (First-In-First-Out)#\n",
        "Jobs are executed in the order they are submitted. Although simple, this policy may lead to long wait times for large, resource-intensive jobs if smaller jobs are constantly being submitted.\n",
        "\n",
        "Fair Share#\n",
        "This policy ensures that all users receive a fair share of cluster resources over time. Users with high resource usage may experience reduced priority, allowing others to access resources more regularly.\n",
        "\n",
        "Priority-Based#\n",
        "Jobs are assigned priorities based on user-defined criteria or system-wide rules. Higher-priority jobs are executed before lower-priority ones, allowing for resource allocation based on user requirements.\n",
        "\n",
        "Job Priorities#\n",
        "\n",
        "User Priority#\n",
        "Users can assign priority values to their jobs. Higher values result in increased job priority and faster access to resources.\n",
        "\n",
        "Resource Requirements#\n",
        "Jobs with larger resource requirements may be assigned higher priority, as they require more significant resources to execute efficiently.\n",
        "\n",
        "Walltime Limit#\n",
        "Jobs with shorter estimated execution times may receive higher priority, ensuring they are executed promptly and freeing up resources for other jobs.\n",
        "\n",
        "Balancing Policies#\n",
        "\n",
        "Backfilling#\n",
        "This policy allows smaller jobs to “backfill” into available resources ahead of larger jobs, optimizing resource utilization and reducing wait times.\n",
        "\n",
        "Preemption#\n",
        "Higher-priority jobs can preempt lower-priority ones, temporarily pausing the lower-priority job’s execution to make resources available for the higher-priority job.\n",
        "\n",
        "Best Practices#\n",
        "\n",
        "Set Realistic Priorities: Assign accurate priorities to your jobs to reflect their importance and resource requirements.\n",
        "Use Resource Quotas: Be mindful of the resources you request to prevent over- or underutilization.\n",
        "Leverage Backfilling: Submit smaller, shorter jobs that can backfill into available resources while waiting for larger jobs to start.\n",
        "\n",
        "Understanding these scheduling policies and priorities empowers you to make informed decisions when submitting jobs, ensuring that your computational tasks are executed efficiently and promptly. If you need further guidance on selecting the right scheduling policy for your job or optimizing your resource usage, our support team is available at rchelp@northeastern.edu or consult our Frequently Asked Questions (FAQs).\n",
        "Optimize your job execution by maximizing our cluster’s scheduling capabilities. Happy computing!\n",
        "\n",
        "https://rc-docs.northeastern.edu/en/latest/runningjobs/interactiveandbatch.html\n",
        "\n",
        "Interactive and Batch Mode#\n",
        "In our High-Performance Computing (HPC) environment, users can run jobs in two primary modes: Interactive and Batch. This page provides an in-depth guide to both, assisting users in selecting the appropriate mode for their specific tasks.\n",
        "\n",
        "Interactive Mode#\n",
        "Interactive mode allows users to run jobs that need immediate execution and feedback.\n",
        "\n",
        "Getting Started with Interactive Mode#\n",
        "To launch an interactive session, use the following command:\n",
        "# Request an interactive session\n",
        "srun --pty /bin/bash\n",
        "\n",
        "This command allocates resources and gives you a shell prompt on the allocated node.\n",
        "\n",
        "Interactive Mode Use Cases#\n",
        "\n",
        "Development and Testing: Ideal for code development and testing.\n",
        "Short Tasks: Best for tasks that require less time and immediate results.\n",
        "\n",
        "See also\n",
        "ADD LINK for More Examples and Guides for Interactive Mode\n",
        "\n",
        "Batch Mode#\n",
        "Batch mode enables users to write scripts that manage job execution, making it suitable for more complex or longer-running jobs.\n",
        "\n",
        "Creating Batch Scripts#\n",
        "A typical batch script includes directives for resource allocation, job names, and commands. Here is an example:\n",
        "#!/bin/bash\n",
        "#SBATCH --job-name=my_job\n",
        "#SBATCH --nodes=1\n",
        "#SBATCH --ntasks=4\n",
        "#SBATCH --time=01:00:00\n",
        "\n",
        "# Commands to execute\n",
        "module load my_program\n",
        "srun my_program.exe\n",
        "\n",
        "Save this script with a .sh extension, e.g., my_script.sh.\n",
        "\n",
        "Submitting Batch Jobs#\n",
        "You can submit your batch script using the sbatch command.\n",
        "sbatch my_script.sh\n",
        "\n",
        "Monitoring Batch Jobs#\n",
        "You can monitor the status of your batch job using the squeue command.\n",
        "squeue -u username\n",
        "\n",
        "Where username is your actual username.\n",
        "\n",
        "Use Cases#\n",
        "\n",
        "Long-Running Jobs: Suitable for extensive simulations or calculations.\n",
        "Scheduled Tasks: Execute jobs at specific times or under certain conditions.\n",
        "Automated Workflows: Manage complex workflows using multiple scripts.\n",
        "\n",
        "\n",
        "\n",
        "Transfer Data#\n",
        "The HPC has a dedicated transfer node that you must use to transfer data to and from the cluster. You cannot transfer data from any other node or the HPC to your local machine. The node name is <username>@xfer.discovery.neu.edu: where <username> is your Northeastern username to login into the transfer node.\n",
        "You can also transfer files using Globus. This is highly recommended if you need to transfer large amounts of data. See Using Globus for more information.\n",
        "If you are transferring data from different directories on the HPC, you need to use a compute node (see Interactive Jobs: srun Command or Batch Jobs: sbatch) with SCP, rsync, or the copy command to complete these tasks. You should use the --constraint=ib flag (see Hardware Overview) to ensure the fastest data transfer rate.\n",
        "\n",
        "Caution\n",
        "The /scratch space is for temporary file storage only. It is not backed up. If you have directed your output files to /scratch, you should transfer your data from /scratch to another location as soon as possible. See Data Storage Options for more information.\n",
        "\n",
        "Transfer via Terminal#\n",
        "\n",
        "SCP\n",
        "You can use scp to transfer files/directories to and from your local machine and the HPC. As an example, you can use this command to transfer a file to your /scratch space on the HPC from your local machine:\n",
        "scp <filename> <username>@xfer.discovery.neu.edu:/scratch/<username>\n",
        "\n",
        "where <filename> is the name of the file in your current directory you want to transfer, and <username> is your Northeastern username. So that you know, this command is run on your local machine.\n",
        "If you want to transfer a directory in your /scratch called test-data from the HPC to your local machine’s current working directory, an example of that command would be:\n",
        "scp -r <username>@xfer.discovery.neu.edu:/scratch/<username>/test-data .\n",
        "\n",
        "where -r flag is for the recursive transfer because it is a directory. So that you know, this command is run on your local machine.\n",
        "\n",
        "Rsync\n",
        "You can use the rsync command to transfer data to and from the HPC and local machine. You can also use rsync to transfer data from different directories on the cluster.\n",
        "The syntex of rsync is\n",
        "rsync [options] <source> <destination>\n",
        "\n",
        "An example of using rsync to transfer a directory called test-data in your current working directory on your local machine to your /scratch on the HPC is\n",
        "rsync -av test-data/ <username>@xfer.discovery.neu.edu:/scratch/<username>\n",
        "\n",
        "where this command is run on your local machine in the directory that contains test-data.\n",
        "Similarly, rsync can be used to copy from the current working directory on the HPC to your current working directory on your local machine:\n",
        "rsync -av <username>@xfer.discovery.neu.edu:/scratch/<username>/test-data .\n",
        "\n",
        "where this command is run on your local machine in the current directory that you want to save the directory test-data.\n",
        "You can also use rsync to copy data from different directories on the HPC:\n",
        "srun --partition=short --nodes=1 --ntasks=1 --time=01:05:00 --constraint=ib --pty /bin/bash\n",
        "rsync -av /scratch/<username>/source_folder /home/<username>/destination_folder\n",
        "\n",
        "sbatch\n",
        "You can use a sbatch job to complete data transfers by submitting the job to the HPC queue. An example of using rsync through a sbatch script is as follows:\n",
        "#!/bin/bash\n",
        "#SBATCH --nodes=1\n",
        "#SBATCH --ntasks=2\n",
        "#SBATCH --time=0:05:00\n",
        "#SBATCH --job-name=DataTransfer\n",
        "#SBATCH --mem=2G\n",
        "#SBATCH --partition=short\n",
        "#SBATCH --constraint=ib\n",
        "#SBATCH -o %j.out\n",
        "#SBATCH -e %j.err\n",
        "\n",
        "rsync -av /scratch/<username>/source_folder /home/<username>/destination_folder\n",
        "\n",
        "where we are transferring the data from source_folder to the destination_folder.\n",
        "\n",
        "SSHFS\n",
        "If you want to use sshfs, use it with the dedicated transfer node xfer.discovery.neu.edu. It will not work on the login or compute nodes. On a Mac, you will also have to install macFUSE and sshfs (please refer to macFUSE) to use the sshfs command.\n",
        "Use this syntax to perform file transfers with sshfs:\n",
        "sshfs <username>@xfer.discovery.neu.edu:</your/remote/path> <your/local/path> -<options>\n",
        "\n",
        "For example, this will mount a directory in your /scratch named test-data to a local directory on your machine ~/mount_point:\n",
        "sshfs <username>@xfer.discovery.neu.edu:/scratch/<username>/test-data ~/mount_point\n",
        "\n",
        "You can interact with the directory from your GUI or use the terminal to perform tasks on it.\n",
        "\n",
        "Transfer via GUI Application#\n",
        "\n",
        "OOD’s File Explorer\n",
        "You can use OOD’s File Explorer application to transfer data from different directories on the HPC and also to transfer data to and from your local machine to the HPC. For more information to complete this please see OOD File Explorer.\n",
        "\n",
        "MobaXterm\n",
        "You can use MobaXterm to transfer data to and from the HPC. Please check out MobaXterm to download MobaXterm.\n",
        "\n",
        "Open MobaXterm.\n",
        "Click Session, then select SFTP.\n",
        "In the Remote host field, type xfer.discovery.neu.edu\n",
        "In the Username field, type your Northeastern username.\n",
        "In the Port field, type 22.\n",
        "In the Password box, type your Northeastern password and click OK. Click No if prompted to save your password.\n",
        "\n",
        "You will now be connected to the transfer node and can transfer files through MobaXterm. Please refer to MobaXterm for further information.\n",
        "\n",
        "FileZilla\n",
        "You can use FileZilla to transfer data to and from the HPC. Please check out FileZilla to download.\n",
        "\n",
        "Open FileZilla.\n",
        "In the Host field, type sftp://xfer.discovery.neu.edu\n",
        "In the Username field, type your Northeastern username.\n",
        "In the Password field, type your Northeastern password.\n",
        "In the Port field, type 22.\n",
        "\n",
        "You will now be connected to the transfer node and can transfer files through FileZilla. Please refer to FileZilla for further information.\n",
        "\n",
        "https://rc-docs.northeastern.edu/en/latest/datamanagement/globus.html\n",
        "\n",
        "Using Globus#\n",
        "Globus is a data management system that you can use to transfer and share files. Northeastern has a subscription to Globus, and you can set up a Globus account with your Northeastern credentials. You can link your accounts if you have another account, either personal or through another institution.\n",
        "To use Globus, you will need to set up an account, as detailed below. Then, as detailed below, you will need to install Globus Connect to create an endpoint on your local computer. After completing these two initial setup procedures, you can use the Globus web app to perform file transfers. See Using the Northeastern endpoint for a walkthrough of using the Northeastern endpoint on Globus.\n",
        "\n",
        "Globus Account Set Up#\n",
        "You can use the following instructions to set up an account with Globus using your Northeastern credentials.\n",
        "\n",
        "Go to Globus.\n",
        "Click Log In.\n",
        "From the Use your existing organizational login, select Northeastern University, and then click Continue.\n",
        "Enter your Northeastern username and password.\n",
        "If you do not have a previous Globus account, click Continue. If you have a previous account, click the Link to an existing account.\n",
        "Check the agreement checkbox, and then click Continue.\n",
        "Click Allow to permit Globus to access your files.\n",
        "\n",
        "You can then access the Globus File Manager app.\n",
        "\n",
        "Tip\n",
        "If you received an account identity that includes your NUID number (for example, 000123456@northeastern.edu), you can follow the “Creating and linking a new account identity” instructions below to get a different account identity if you want a more user-friendly account identity. You can then link the two accounts together.\n",
        "\n",
        "Creating and linking a new account identity (Optional)#\n",
        "If you created an account through Northeastern University’s existing organizational login and received a username that included your NUID, you can create a new identity with a different username and link the two accounts together. A username you select instead of one with your NUID can make it easier to remember your login credentials.\n",
        "\n",
        "Go to Globus.\n",
        "Click Log In.\n",
        "Click Globus ID to sign in.\n",
        "Click Need a Globus ID? Sign up.\n",
        "Enter your Globus ID information.\n",
        "Enter the verification code that Globus sends to your email.\n",
        "Click Link to an existing account to link this new account with your primary account.\n",
        "Select Northeastern University from the drop-down box and click Continue to be taken to the Northeastern University single sign-on page.\n",
        "Enter your Northeastern username and password.\n",
        "\n",
        "You should now see your two accounts linked in the Account section on the Globus web app.\n",
        "\n",
        "Install Globus Connect Personal (GCP)#\n",
        "Use Globus Connect Personal (GCP) as an endpoint for your laptop. You first need to install GCP using the following procedure and be logged in to Globus before you can install GCP.\n",
        "\n",
        "Go to Globus File Manager.\n",
        "Enter a name for your endpoint in the Endpoint Display Name field.\n",
        "Click Generate Setup Key to generate a setup key for your endpoint.\n",
        "Click the Copy icon next to the generated setup key to copy the key to your clipboard. You will need this key during the installation of GCP in step 6.\n",
        "Click the appropriate OS icon for your computer to download the installation file.\n",
        "After downloading the installation file to your computer, double-click on the file to launch the installer.\n",
        "\n",
        "Accept the defaults on the install wizard. After the installation, you can use your laptop as an endpoint within Globus.\n",
        "\n",
        "Note\n",
        "You cannot modify an endpoint after you have created it. If you need an endpoint with different options, you must delete and recreate it. Follow the instructions on the Globus website for deleting and recreating an endpoint.\n",
        "\n",
        "Working with Globus#\n",
        "After you have an account and set up a personal endpoint using Globus Connect personal, you can perform basic file management tasks using the Globus File Manager interface, such as transferring files, renaming files, and creating new folders. You can also download and use the Globus Command Line Interface (CLI) tool. Globus also has extensive documentation and training files for you to practice with.\n",
        "\n",
        "Using the Northeastern endpoint#\n",
        "To access the Northeastern endpoint on Globus, on the Globus web app, click File Manager, then in the Collection text box, type Northeastern. The endpoints owned by Northeastern University are displayed in the collection area. The general Northeastern endpoint is northeastern#discovery. Using the File Manager interface, you can easily change directories, switch the direction of transferring to and from, and specify options such as transferring only new or changed files. Below is a procedure for transferring files from Discovery to your personal computer, but with the flexibility of the File Manager interface, you can adjust the endpoints, file view, direction of the transfer, and many other options.\n",
        "To transfer files from Discovery to your personal computer, do the following\n",
        "\n",
        "Create an endpoint on your computer using the procedure above “Install Globus Connect,” if you have not done so already.\n",
        "In the File Manager on the Globus web app, in the Collections textbox, type Northeastern, then in the collection list, click the northeastern#discovery endpoint.\n",
        "click Transfer or Sync to in the right-pane menu.\n",
        "Click in the Search text box, and then click the name of your endpoint on the Your Collections tab. You can now see the list of your files on Discovery on the left and on your personal computer on the right.\n",
        "Select the file or files from the right-side list of Discovery files that you want to transfer to your personal computer.\n",
        "Select the destination folder from the left-side list of the files on your computer.\n",
        "(Optional) Click Transfer & Sync Options and select the transfer options you need.\n",
        "Click Start.\n",
        "\n",
        "Connecting to Google Drive#\n",
        "The version of Globus currently on Discovery allows you to connect to Google Drive by first setting up the connection in GCP. This will add your Google Drive to your current personal endpoint.\n",
        "Just so you know, you will first need a personal endpoint, as outlined in the procedure above. This procedure is slightly different from using the Google Drive Connector with\n",
        "Globus version 5.5. You will need your Google Drive downloaded to your local computer.\n",
        "To add Google Drive to your endpoint, do the following\n",
        "\n",
        "Open the GCP app. Right-click the G icon in your taskbar on Windows and select Options. Click the G icon in the menu bar on Mac and select Preferences.\n",
        "On the Access tab, click the + button to open the Choose a directory dialog box.\n",
        "Navigate to your Google Drive on your computer and click Choose.\n",
        "Click the Shareable checkbox to make this a shareable folder in Globus File Manager, and then click Save.\n",
        "\n",
        "You can now go to Globus File Manager and see that your Google Drive is available as a folder on your endpoint.\n",
        "\n",
        "Command Line Interface (CLI)#\n",
        "The Globus Command Line Interface (CLI) tool allows you to access Globus from the command line. It is a stand-alone app that requires a separate download\n",
        "and installation. Please refer to the Globus CLI documentation for working with this app.\n",
        "\n",
        "Globus documentation and test files#\n",
        "Globus provides detailed instructions on using Globus and has test files for you to practice with. These are free for you to access and use. We would like to encourage you to use the test files to become familiar with the Globus interface. You can access the Globus documentation and training files on the Globus How To website.\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "'''"
      ],
      "metadata": {
        "id": "_Ox6hCqd-2nC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "encoding_name = tiktoken.get_encoding(\"cl100k_base\")\n",
        "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    encoding = tiktoken.get_encoding(encoding_name)\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens"
      ],
      "metadata": {
        "id": "6twYZJo26GmB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import TokenTextSplitter\n",
        "text_splitter = TokenTextSplitter(chunk_size=500, chunk_overlap=25)\n",
        "docs = text_splitter.split_documents(data)\n",
        "\n",
        "'''\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 700,\n",
        "    chunk_overlap  = 70,\n",
        "    length_function = len,\n",
        "    add_start_index = True,\n",
        ")\n",
        "docs = text_splitter.create_documents([data])\n",
        "\n",
        "for idx, text in enumerate(docs):\n",
        "    docs[idx].metadata['source'] = \"RCDocs\"\n",
        "'''"
      ],
      "metadata": {
        "id": "9b3T5m1YBVrf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "4f06dc21-756e-4dd9-f462-c503f4dd675b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\n\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    chunk_size = 700,\\n    chunk_overlap  = 70,\\n    length_function = len,\\n    add_start_index = True,\\n)\\ndocs = text_splitter.create_documents([data])\\n\\nfor idx, text in enumerate(docs):\\n    docs[idx].metadata[\\'source\\'] = \"RCDocs\"\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(docs[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DfcNy9sT3jj",
        "outputId": "36618e1f-e92c-4ed3-f051-980b780bc57b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain.schema.document.Document"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PILXy2pdVC4v",
        "outputId": "99fdfd5c-a9f0-422a-dded-c4c33e642735"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='\\n\\n\\n\\n\\n\\n\\nUnderstanding the Queuing System - RC RTD\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nContents\\n\\n\\n\\n\\n\\nMenu\\n\\n\\n\\n\\n\\n\\n\\nExpand\\n\\n\\n\\n\\n\\nLight mode\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDark mode\\n\\n\\n\\n\\n\\n\\nAuto light/dark mode\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHide navigation sidebar\\n\\n\\nHide table of contents sidebar\\n\\n\\n\\n\\n\\nToggle site navigation sidebar\\n\\n\\n\\n\\nRC RTD\\n\\n\\n\\n\\nToggle Light / Dark / Auto color theme\\n\\n\\n\\n\\n\\n\\nToggle table of contents sidebar\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nResearch ComputingToggle child pages in navigation\\nWelcome\\nServices We Provide\\nGetting Help\\nIntroduction to HPC and Slurm\\nCase Studies and User Testimonials\\n\\n\\n\\n\\nGetting StartedToggle child pages in navigation\\nGetting Access\\nAccount Manager\\nConnecting To ClusterToggle child pages in navigation\\nMac\\nWindows\\n\\n\\n\\n\\nFirst StepsToggle child pages in navigation\\nPasswordless SSH\\nShell Environment on the Cluster\\nCluster via Command-Line\\n\\n\\n\\nUser Guides\\n\\nHardwareToggle child pages in navigation\\nOverview\\nPartitions\\n\\n\\nOpen OnDemand (OOD)Toggle child pages in navigation\\nIntroduction to OOD\\nAccessing Open OnDemand\\nInteractive Open OnDemand ApplicationsToggle child pages in navigation\\nDesktop App\\nOOD File Explorer\\nJupyterLab\\nStata\\n\\n\\n\\n\\nRunning JobsToggle child pages in navigation\\nUnderstanding the Queuing System\\nJob Scheduling Policies and Priorities\\nInteractive and Batch Mode\\nWorking with GPUs\\nRecurring Jobs\\nDebugging and Troubleshooting Jobs\\n\\n\\nData ManagementToggle child pages in navigation\\nData Storage Options\\nTransfer Data\\nUsing Globus\\nData Backup and Restore\\nSecurity and Compliance\\n\\n\\nSoftwareToggle child pages in navigation\\nSystem WideToggle child pages in navigation\\nModules\\nMPI\\nR\\nMatlab\\n\\n\\nPackage ManagersToggle child pages in navigation\\nConda\\nSpack\\n\\n\\nFrom SourceToggle child pages in navigation\\nMake\\nCMake\\n\\n\\n\\n\\nSlurmToggle child pages in navigation\\nIntroduction to Slurm\\nSlurm Commands\\nSlurm Running Jobs\\nMonitoring and Managing Jobs\\nSlurm Job Scripts\\nSlurm Array Jobs and Dependencies\\nSlurm Best Practices\\n\\n\\nHPC for the ClassroomT', metadata={'source': 'https://rc-docs.northeastern.edu/en/latest/runningjobs/understandingqueuing.html', 'title': 'Understanding the Queuing System - RC RTD', 'language': 'en'})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from langchain.vectorstores import Pinecone\n",
        "#import pinecone\n",
        "from langchain.embeddings import CohereEmbeddings\n"
      ],
      "metadata": {
        "id": "kh1JnImJ6Mt2"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = CohereEmbeddings(model='embed-english-light-v2.0',cohere_api_key='Aj7fRPV0FBm1u6baUBuAZc5yMOvOs6krkrqVppam')\n"
      ],
      "metadata": {
        "id": "yZiQ1hc_6PYl"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pinecone.init(\n",
        "#\tapi_key='b360318b-4fc8-4580-bf6c-d88959179985',\n",
        "#\tenvironment='us-west1-gcp-free'\n",
        "#)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bumy27EP6Q7F",
        "outputId": "19048b6b-2076-4af7-b351-78053cf34a9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "send: b'GET /actions/whoami HTTP/1.1\\r\\nHost: controller.us-west1-gcp-free.pinecone.io\\r\\nUser-Agent: python-requests/2.31.0\\r\\nAccept-Encoding: gzip, deflate\\r\\nAccept: */*\\r\\nConnection: keep-alive\\r\\napi-key: b360318b-4fc8-4580-bf6c-d88959179985\\r\\n\\r\\n'\n",
            "reply: 'HTTP/1.1 200 OK\\r\\n'\n",
            "header: content-type: application/json; charset=UTF-8\n",
            "header: date: Thu, 05 Oct 2023 00:56:21 GMT\n",
            "header: x-envoy-upstream-service-time: 1\n",
            "header: content-length: 72\n",
            "header: server: envoy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pinecone.delete_index(\"chatbot1\")\n"
      ],
      "metadata": {
        "id": "8U4W4qHt_m79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pinecone.create_index(\"chatbot1\", dimension=1024)\n"
      ],
      "metadata": {
        "id": "XYqGH05t_nXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#index = pinecone.Index('chatbot1')\n",
        "\n",
        "#index_name = \"chatbot1\""
      ],
      "metadata": {
        "id": "yHrQSGHpWSMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#docsearch = Pinecone.from_documents(docs, embeddings, index_name=index_name)"
      ],
      "metadata": {
        "id": "tLU7yDoS6Stw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n"
      ],
      "metadata": {
        "id": "5oEHdLS1mfma"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6HvTbqqnibV",
        "outputId": "8cce1c61-a3d2-464a-c283-309345d2231e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb\n",
            "  Downloading chromadb-0.4.13-py3-none-any.whl (437 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.8/437.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.31.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.10.13)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.103.2-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.3/66.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
            "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.0.2-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.5.0)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb)\n",
            "  Downloading pulsar_client-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers>=0.13.2 (from chromadb)\n",
            "  Downloading tokenizers-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.1.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_28_x86_64.whl (593 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.7/593.7 kB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.23.5)\n",
            "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (3.7.1)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: python-dateutil>2.1 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from pulsar-client>=3.1.0->chromadb) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (2.0.6)\n",
            "Collecting huggingface_hub<0.17,>=0.16.4 (from tokenizers>=0.13.2->chromadb)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (428 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m428.8/428.8 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (6.0.1)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.20.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi>=0.95.2->chromadb) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi>=0.95.2->chromadb) (1.1.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.12.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=86a35551d8abe6c8f8a74366f757eeb3a213c24eacc40c1bc8d3aadebb7879ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, websockets, uvloop, python-dotenv, pulsar-client, overrides, humanfriendly, httptools, h11, chroma-hnswlib, bcrypt, watchfiles, uvicorn, starlette, posthog, huggingface_hub, coloredlogs, tokenizers, onnxruntime, fastapi, chromadb\n",
            "Successfully installed bcrypt-4.0.1 chroma-hnswlib-0.7.3 chromadb-0.4.13 coloredlogs-15.0.1 fastapi-0.103.2 h11-0.14.0 httptools-0.6.0 huggingface_hub-0.16.4 humanfriendly-10.0 monotonic-1.6 onnxruntime-1.16.0 overrides-7.4.0 posthog-3.0.2 pulsar-client-3.3.0 pypika-0.48.9 python-dotenv-1.0.0 starlette-0.27.0 tokenizers-0.14.0 uvicorn-0.23.2 uvloop-0.17.0 watchfiles-0.20.0 websockets-11.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "db = Chroma.from_documents(docs, embeddings)\n"
      ],
      "metadata": {
        "id": "39b7bDIInas5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import CohereRerank\n",
        "#from langchain.vectorstores import Pinecone\n"
      ],
      "metadata": {
        "id": "CF8AHNQm6Ucf"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load index\n",
        "#docsearch = Pinecone.from_existing_index(index_name, embeddings)\n"
      ],
      "metadata": {
        "id": "aeEbBgCS6dHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize base retriever\n",
        "#retriever = docsearch.as_retriever(search_kwargs={\"k\": 4})\n",
        "#retriever = Chroma.as_retriever(search_kwargs={\"k\": 4})\n",
        "#retriever = Chroma.as_retriever(\n",
        "#    search_type=\"mmr\",\n",
        "#    search_kwargs={'k': 4, 'fetch_k': 50} )\n",
        "\n",
        "from langchain.schema.vectorstore import VectorStoreRetriever\n",
        "retriever = VectorStoreRetriever(vectorstore=db, search_type=\"mmr\", search_kwargs={'k': 4, 'fetch_k': 50},)\n",
        "\n",
        "'''\n",
        "\n",
        "VectorStoreRetriever\n",
        "\n",
        "Return VectorStoreRetriever initialized from this VectorStore.\n",
        "\n",
        "Args:\n",
        "    search_type (Optional[str]): Defines the type of search that\n",
        "        the Retriever should perform.\n",
        "Can be \"similarity\" (default), \"mmr\", or\n",
        "\"similarity_score_threshold\".\n",
        "    search_kwargs (Optional[Dict]): Keyword arguments to pass to the\n",
        "        search function. Can include things like:\n",
        "            k: Amount of documents to return (Default: 4)\n",
        "            score_threshold: Minimum relevance threshold\n",
        "                for similarity_score_threshold\n",
        "            fetch_k: Amount of documents to pass to MMR algorithm (Default: 20)\n",
        "            lambda_mult: Diversity of results returned by MMR;\n",
        "                1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
        "            filter: Filter by document metadata\n",
        "\n",
        "Returns:\n",
        "    VectorStoreRetriever: Retriever class for VectorStore.\n",
        "\n",
        "Examples:\n",
        "\n",
        "# Retrieve more documents with higher diversity\n",
        "# Useful if your dataset has many similar documents\n",
        "docsearch.as_retriever(\n",
        "    search_type=\"mmr\",\n",
        "    search_kwargs={'k': 6, 'lambda_mult': 0.25}\n",
        ")\n",
        "\n",
        "# Fetch more documents for the MMR algorithm to consider\n",
        "# But only return the top 5\n",
        "docsearch.as_retriever(\n",
        "    search_type=\"mmr\",\n",
        "    search_kwargs={'k': 5, 'fetch_k': 50}\n",
        ")\n",
        "\n",
        "# Only retrieve documents that have a relevance score\n",
        "# Above a certain threshold\n",
        "docsearch.as_retriever(\n",
        "    search_type=\"similarity_score_threshold\",\n",
        "    search_kwargs={'score_threshold': 0.8}\n",
        ")\n",
        "\n",
        "# Only get the single most similar document from the dataset\n",
        "docsearch.as_retriever(search_kwargs={'k': 1})\n",
        "\n",
        "# Use a filter to only retrieve documents from a specific paper\n",
        "docsearch.as_retriever(\n",
        "    search_kwargs={'filter': {'paper_title':'GPT-4 Technical Report'}}\n",
        ")\n",
        "'''"
      ],
      "metadata": {
        "id": "zp19GFSp6ed2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "b8cf41d0-4243-43bc-a35d-08a2eb34aba8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nVectorStoreRetriever\\n\\nReturn VectorStoreRetriever initialized from this VectorStore.\\n\\nArgs:\\n    search_type (Optional[str]): Defines the type of search that\\n        the Retriever should perform.\\nCan be \"similarity\" (default), \"mmr\", or\\n\"similarity_score_threshold\".\\n    search_kwargs (Optional[Dict]): Keyword arguments to pass to the\\n        search function. Can include things like:\\n            k: Amount of documents to return (Default: 4)\\n            score_threshold: Minimum relevance threshold\\n                for similarity_score_threshold\\n            fetch_k: Amount of documents to pass to MMR algorithm (Default: 20)\\n            lambda_mult: Diversity of results returned by MMR;\\n                1 for minimum diversity and 0 for maximum. (Default: 0.5)\\n            filter: Filter by document metadata\\n\\nReturns:\\n    VectorStoreRetriever: Retriever class for VectorStore.\\n\\nExamples:\\n\\n# Retrieve more documents with higher diversity\\n# Useful if your dataset has many similar documents\\ndocsearch.as_retriever(\\n    search_type=\"mmr\",\\n    search_kwargs={\\'k\\': 6, \\'lambda_mult\\': 0.25}\\n)\\n\\n# Fetch more documents for the MMR algorithm to consider\\n# But only return the top 5\\ndocsearch.as_retriever(\\n    search_type=\"mmr\",\\n    search_kwargs={\\'k\\': 5, \\'fetch_k\\': 50}\\n)\\n\\n# Only retrieve documents that have a relevance score\\n# Above a certain threshold\\ndocsearch.as_retriever(\\n    search_type=\"similarity_score_threshold\",\\n    search_kwargs={\\'score_threshold\\': 0.8}\\n)\\n\\n# Only get the single most similar document from the dataset\\ndocsearch.as_retriever(search_kwargs={\\'k\\': 1})\\n\\n# Use a filter to only retrieve documents from a specific paper\\ndocsearch.as_retriever(\\n    search_kwargs={\\'filter\\': {\\'paper_title\\':\\'GPT-4 Technical Report\\'}}\\n)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2i8aTRA6gUB",
        "outputId": "e1bb2f61-7f75-4b8c-e53e-bfeaa7da6574"
      },
      "execution_count": 30,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cohere API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OPENAI API Key:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7416XLY97FAL",
        "outputId": "a4c513e2-a505-44f9-fffb-9feb62dea2f7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OPENAI API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compressor = CohereRerank()\n"
      ],
      "metadata": {
        "id": "LcR414tm7UAd"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up cohere's reranker\n",
        "reranker = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=retriever\n",
        ")\n"
      ],
      "metadata": {
        "id": "dmf4wxNb7Xr0"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationTokenBufferMemory\n"
      ],
      "metadata": {
        "id": "E6L9I9sU7Yy7"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n"
      ],
      "metadata": {
        "id": "EQuyfiGR7aO5"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from langchain.callbacks import ContextCallbackHandler\n",
        "#from langchain.callbacks import FlyteCallbackHandler\n",
        "from langchain.callbacks import StdOutCallbackHandler\n"
      ],
      "metadata": {
        "id": "lUJilunXIox7"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#context_callback = ContextCallbackHandler(token=\"T1gM1n4RzGWLFSsJnQ5ziLUW\")\n",
        "#context_callback = FlyteCallbackHandler()\n",
        "context_callback = StdOutCallbackHandler()"
      ],
      "metadata": {
        "id": "M18gm_YBHKE8"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0.7, verbose=True, openai_api_key = os.environ.get(\"OPENAI_API_KEY\"), streaming=True, callbacks=[context_callback])\n"
      ],
      "metadata": {
        "id": "-BFTmJkK-na4"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationTokenBufferMemory(llm=llm,memory_key=\"chat_history\", return_messages=True,input_key='question',max_token_limit=1000)"
      ],
      "metadata": {
        "id": "4_aSlIy6F1e8"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "CONDENSE_QUESTION_PROMPT = '''\n",
        "Below is a summary of the conversation so far, and a new question asked by the user that needs to be answered by searching in a knowledge base.\n",
        "Generate a search query based on the conversation and the new question.\n",
        "\n",
        "Chat History:\n",
        "{chat_history}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Search query:\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "PromptTemplates = PromptTemplate(\n",
        "    input_variables=[\"chat_history\", \"question\"],\n",
        "    template=\"\"\"\n",
        "Below is a summary of the conversation so far, and a new question asked by the user that needs to be answered by searching in a knowledge base.\n",
        "Generate a search query based on the conversation and the new question.\n",
        "\n",
        "Chat History:\n",
        "{chat_history}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Search query:\"\"\"\n",
        ")\n",
        "\n",
        "\n",
        "system_message_prompt = SystemMessagePromptTemplate(prompt=PromptTemplates)\n",
        "\n",
        "chat_prompt_for_ques = ChatPromptTemplate.from_messages(\n",
        "    [system_message_prompt])\n",
        "\n"
      ],
      "metadata": {
        "id": "BAbvsetCTEIL"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n"
      ],
      "metadata": {
        "id": "wJiRbW27TaZp"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_generator = LLMChain(llm=llm, prompt=chat_prompt_for_ques, verbose=True)\n"
      ],
      "metadata": {
        "id": "WiKNdmX3S1Zj"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Answer_Generator_Prompt= '''\n",
        "<Instructions>\n",
        "Important:\n",
        "Answer with the facts listed in the list of sources below. If there isn't enough information below, say you don't know.\n",
        "If asking a clarifying question to the user would help, ask the question.\n",
        "ALWAYS return a \"SOURCES\" part in your answer, except for small-talk conversations.\n",
        "\n",
        "Question: {question}\n",
        "Sources:\n",
        "---------------------\n",
        "    {summaries}\n",
        "---------------------\n",
        "\n",
        "Chat History:\n",
        "{chat_history}\n",
        "'''"
      ],
      "metadata": {
        "id": "CzcMctULXMY9"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
        "\n",
        "chat_prompt = PromptTemplate(template=Answer_Generator_Prompt, input_variables=[\"question\", \"summaries\",\"chat_history\"])\n",
        "\n",
        "answer_chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\", verbose=True,prompt=chat_prompt)\n"
      ],
      "metadata": {
        "id": "C2yZpFpRVJPD"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "\n",
        "chain = ConversationalRetrievalChain(\n",
        "            retriever=reranker,\n",
        "            question_generator=question_generator,\n",
        "            combine_docs_chain=answer_chain,\n",
        "            verbose=True,\n",
        "            memory=memory,\n",
        "            rephrase_question=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "WgdFwotlZtyJ"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the Scheduling Policies for HPC cluster?\"\n",
        "result = chain({\"question\": query})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClKEHA56ZxqI",
        "outputId": "d6d1b3c3-fa3d-4e9b-aef7-1894e9d2f630"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: \n",
            "Below is a summary of the conversation so far, and a new question asked by the user that needs to be answered by searching in a knowledge base.\n",
            "Generate a search query based on the conversation and the new question.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: What is Globus at northeaster?\n",
            "Assistant: I'm sorry, but there is no information available about \"Globus at northeaster\" in the provided sources.\n",
            "Human: How do i set up an account with Globus?\n",
            "Assistant: SOURCES:\n",
            "1. There is no information available about \"Globus at northeaster\" in the provided sources.\n",
            "\n",
            "Question:\n",
            "What is the Scheduling Policies for HPC cluster?\n",
            "\n",
            "Search query:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 50 is greater than number of elements in index 30, updating n_results = 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "<Instructions>\n",
            "Important:\n",
            "Answer with the facts listed in the list of sources below. If there isn't enough information below, say you don't know.\n",
            "If asking a clarifying question to the user would help, ask the question.\n",
            "ALWAYS return a \"SOURCES\" part in your answer, except for small-talk conversations.\n",
            "\n",
            "Question: What is the Scheduling Policies for HPC cluster?\n",
            "Sources:\n",
            "---------------------\n",
            "    Content:  Job Scripts\n",
            "Slurm Array Jobs and Dependencies\n",
            "Slurm Best Practices\n",
            "\n",
            "\n",
            "HPC for the ClassroomToggle child pages in navigation\n",
            "Classroom HPC: FAQ\n",
            "CPS Class Instructions\n",
            "\n",
            "\n",
            "Best PracticesToggle child pages in navigation\n",
            "Home Directory Storage Quota\n",
            "Checkpointing Jobs\n",
            "Optimizing Job Performance\n",
            "Best SW Practices\n",
            "\n",
            "\n",
            "Tutorials and TrainingToggle child pages in navigation\n",
            "Canvas and GitHub\n",
            "\n",
            "\n",
            "Frequently Asked Questions\n",
            "Glossary\n",
            "\n",
            "Contribution\n",
            "\n",
            "Change Log\n",
            "Report Docs Bug or Request\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "    v: latest\n",
            "  \n",
            "\n",
            "\n",
            "Versions\n",
            "latest\n",
            "2.0.0\n",
            "1.2.0\n",
            "v1.1.0\n",
            "\n",
            "\n",
            "Downloads\n",
            "\n",
            "\n",
            "On Read the Docs\n",
            "\n",
            "Project Home\n",
            "\n",
            "\n",
            "Builds\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Back to top\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Edit this page\n",
            "\n",
            "\n",
            "\n",
            "Toggle Light / Dark / Auto color theme\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Toggle table of contents sidebar\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Job Scheduling Policies and Priorities#\n",
            "In an HPC environment, efficient job scheduling is crucial for allocating computing resources and ensuring optimal cluster utilization. Job scheduling policies and priorities determine the order in which jobs are executed and the resources they receive. Understanding these policies is essential for maximizing job efficiency and minimizing wait times.\n",
            "\n",
            "Scheduling Policies#\n",
            "\n",
            "FIFO (First-In-First-Out)#\n",
            "Jobs are executed in the order they are submitted. Although simple, this policy may lead to long wait times for large, resource-intensive jobs if smaller jobs are constantly being submitted.\n",
            "\n",
            "\n",
            "Fair Share#\n",
            "This policy ensures that all users receive a fair share of cluster resources over time. Users with high resource usage may experience reduced priority, allowing others to access resources more regularly.\n",
            "\n",
            "\n",
            "Priority-Based#\n",
            "Jobs are assigned priorities based on user-defined criteria or system-wide rules. Higher-priority jobs are executed before lower-priority ones, allowing for resource allocation based on user requirements.\n",
            "\n",
            "\n",
            "\n",
            "Job Priorities#\n",
            "\n",
            "User Priority#\n",
            "Users can assign priority values to their jobs. Higher values result in increased job priority and faster access to resources.\n",
            "\n",
            "\n",
            "Resource Requirements#\n",
            "Jobs with larger resource requirements may be assigned higher priority, as they require more significant resources to execute efficiently.\n",
            "\n",
            "\n",
            "Walltime Limit#\n",
            "Source: https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\n",
            "\n",
            "Content: SBATCH --job-name=my_job\n",
            "#SBATCH --nodes=1\n",
            "#SBATCH --ntasks=4\n",
            "#SBATCH --time=01:00:00\n",
            "\n",
            "# Commands to execute\n",
            "module load my_program\n",
            "srun my_program.exe\n",
            "\n",
            "\n",
            "Save this script with a .sh extension, e.g., my_script.sh.\n",
            "\n",
            "\n",
            "Submitting Batch Jobs#\n",
            "You can submit your batch script using the sbatch command.\n",
            "sbatch my_script.sh\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Monitoring Batch Jobs#\n",
            "You can monitor the status of your batch job using the squeue command.\n",
            "squeue -u username\n",
            "\n",
            "\n",
            "Where username is your actual username.\n",
            "\n",
            "\n",
            "Use Cases#\n",
            "\n",
            "Long-Running Jobs: Suitable for extensive simulations or calculations.\n",
            "Scheduled Tasks: Execute jobs at specific times or under certain conditions.\n",
            "Automated Workflows: Manage complex workflows using multiple scripts.\n",
            "\n",
            "\n",
            "See also\n",
            "ADD LINK for More Examples and Guides for Batch Mode\n",
            "\n",
            "Interactive and Batch modes cater to different needs and scenarios in the HPC environment. You can explore both modes to choose the one that best aligns with your tasks. For more detailed guides and support, please consult the above guides or contact our support team at rchelp@northeastern.edu.\n",
            "Happy computing!\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Next\n",
            "\n",
            "Working with GPUs\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Previous\n",
            "\n",
            "Job Scheduling Policies and Priorities\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                Copyright © 2023, RC\n",
            "            \n",
            "            Made with \n",
            "            Furo\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "            On this page\n",
            "          \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Interactive and Batch Mode\n",
            "Interactive Mode\n",
            "Getting Started with Interactive Mode\n",
            "Interactive Mode Use Cases\n",
            "\n",
            "\n",
            "Batch Mode\n",
            "Creating Batch Scripts\n",
            "Submitting Batch Jobs\n",
            "Monitoring Batch Jobs\n",
            "Use Cases\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Source: https://rc-docs.northeastern.edu/en/latest/runningjobs/interactiveandbatch.html\n",
            "\n",
            "Content: \n",
            "Turing\n",
            "46\n",
            "576\n",
            "4,608\n",
            "0\n",
            "2(x3)\n",
            "\n",
            "A30\n",
            "Ampere\n",
            "24\n",
            "224\n",
            "3,804\n",
            "0\n",
            "1(x3)\n",
            "\n",
            "RTX A5000\n",
            "Ampere\n",
            "24\n",
            "256\n",
            "8,192\n",
            "0\n",
            "6(x8)\n",
            "\n",
            "RTX A6000\n",
            "Ampere\n",
            "49\n",
            "336\n",
            "10,752\n",
            "0\n",
            "3(x8)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "The gpu partition is the general GPU resource for HPC users looking to use a GPU; multigpu is the alternative, where more than one GPU are accessible.\n",
            "Anyone with a cluster account has access to the gpu partition. However, you must submit a ServiceNow ticket requesting temporary access to multigpu provided sufficient need and preparation.\n",
            "\n",
            "Note\n",
            "The multigpu partition is available for a limited time window to fulfill urgent needs. In addition, only instances that require multigpu will be granted access to this partition. As the partition is only accessible for a limited time (e.g., 48 hours), it is advisable to use it at full capacity. A member of the RC team will review your request to ensure that there is a genuine need for the partition. Please note that all user limits are subject to the availability of the multigpu resources at the time and will be allocated based on user needs.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Name\n",
            "Requires Approval?\n",
            "Time in Hours (Default/Max)\n",
            "Submitted Jobs\n",
            "GPU per Job Limit\n",
            "User Limit (No. GPUs)\n",
            "\n",
            "\n",
            "\n",
            "gpu\n",
            "No\n",
            "4/8\n",
            "50/100\n",
            "1\n",
            "8\n",
            "\n",
            "multigpu\n",
            "Yes\n",
            "4/24\n",
            "50/100\n",
            "12\n",
            "12\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Important\n",
            "Consider the compatibility of the GPU, as some programs do not work on the older k40m or k80 GPUs.\n",
            "Execute the following command to display the non-Kepler GPUs that are available:\n",
            "sinfo -p gpu --Format=nodes,cpus,memory,features,statecompact,nodelist,gres\n",
            "\n",
            "\n",
            "This indicates the state (idle or not) of gpu-types and could be helpful to find one that is idle. However, the command does not give real-time information of the state and should be used carefully.\n",
            "\n",
            "\n",
            "Requesting GPUs with Slurm#\n",
            "Use srun for interactive and\n",
            "Source: https://rc-docs.northeastern.edu/en/latest/runningjobs/workingwithgpus.html\n",
            "---------------------\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: What is Globus at northeaster?\n",
            "Assistant: I'm sorry, but there is no information available about \"Globus at northeaster\" in the provided sources.\n",
            "Human: How do i set up an account with Globus?\n",
            "Assistant: SOURCES:\n",
            "1. There is no information available about \"Globus at northeaster\" in the provided sources.\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Can i assign priorities for this?\"\n",
        "result = chain({\"question\": query})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLLL9c7tbgW1",
        "outputId": "766a6bf0-92f3-47ca-d56e-30cb719d395f"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: \n",
            "Below is a summary of the conversation so far, and a new question asked by the user that needs to be answered by searching in a knowledge base.\n",
            "Generate a search query based on the conversation and the new question.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: What is Globus at northeaster?\n",
            "Assistant: I'm sorry, but there is no information available about \"Globus at northeaster\" in the provided sources.\n",
            "Human: How do i set up an account with Globus?\n",
            "Assistant: SOURCES:\n",
            "1. There is no information available about \"Globus at northeaster\" in the provided sources.\n",
            "Human: What is the Scheduling Policies for HPC cluster?\n",
            "Assistant: I'm sorry, but there is no information available about setting up an account with Globus at Northeastern University in the provided sources.\n",
            "Human: Can i assign priorities for this?\n",
            "Assistant: The scheduling policies for an HPC cluster include FIFO (First-In-First-Out), Fair Share, and Priority-Based. FIFO executes jobs in the order they are submitted. Fair Share ensures that all users receive a fair share of cluster resources over time. Priority-Based assigns priorities to jobs based on user-defined criteria or system-wide rules, with higher-priority jobs executed before lower-priority ones. \n",
            "\n",
            "SOURCES:\n",
            "- https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\n",
            "\n",
            "Question:\n",
            "Can i assign priorities for this?\n",
            "\n",
            "Search query:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 50 is greater than number of elements in index 30, updating n_results = 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "<Instructions>\n",
            "Important:\n",
            "Answer with the facts listed in the list of sources below. If there isn't enough information below, say you don't know.\n",
            "If asking a clarifying question to the user would help, ask the question.\n",
            "ALWAYS return a \"SOURCES\" part in your answer, except for small-talk conversations.\n",
            "\n",
            "Question: Can i assign priorities for this?\n",
            "Sources:\n",
            "---------------------\n",
            "    Content:  Job Scripts\n",
            "Slurm Array Jobs and Dependencies\n",
            "Slurm Best Practices\n",
            "\n",
            "\n",
            "HPC for the ClassroomToggle child pages in navigation\n",
            "Classroom HPC: FAQ\n",
            "CPS Class Instructions\n",
            "\n",
            "\n",
            "Best PracticesToggle child pages in navigation\n",
            "Home Directory Storage Quota\n",
            "Checkpointing Jobs\n",
            "Optimizing Job Performance\n",
            "Best SW Practices\n",
            "\n",
            "\n",
            "Tutorials and TrainingToggle child pages in navigation\n",
            "Canvas and GitHub\n",
            "\n",
            "\n",
            "Frequently Asked Questions\n",
            "Glossary\n",
            "\n",
            "Contribution\n",
            "\n",
            "Change Log\n",
            "Report Docs Bug or Request\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "    v: latest\n",
            "  \n",
            "\n",
            "\n",
            "Versions\n",
            "latest\n",
            "2.0.0\n",
            "1.2.0\n",
            "v1.1.0\n",
            "\n",
            "\n",
            "Downloads\n",
            "\n",
            "\n",
            "On Read the Docs\n",
            "\n",
            "Project Home\n",
            "\n",
            "\n",
            "Builds\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Back to top\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Edit this page\n",
            "\n",
            "\n",
            "\n",
            "Toggle Light / Dark / Auto color theme\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Toggle table of contents sidebar\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Job Scheduling Policies and Priorities#\n",
            "In an HPC environment, efficient job scheduling is crucial for allocating computing resources and ensuring optimal cluster utilization. Job scheduling policies and priorities determine the order in which jobs are executed and the resources they receive. Understanding these policies is essential for maximizing job efficiency and minimizing wait times.\n",
            "\n",
            "Scheduling Policies#\n",
            "\n",
            "FIFO (First-In-First-Out)#\n",
            "Jobs are executed in the order they are submitted. Although simple, this policy may lead to long wait times for large, resource-intensive jobs if smaller jobs are constantly being submitted.\n",
            "\n",
            "\n",
            "Fair Share#\n",
            "This policy ensures that all users receive a fair share of cluster resources over time. Users with high resource usage may experience reduced priority, allowing others to access resources more regularly.\n",
            "\n",
            "\n",
            "Priority-Based#\n",
            "Jobs are assigned priorities based on user-defined criteria or system-wide rules. Higher-priority jobs are executed before lower-priority ones, allowing for resource allocation based on user requirements.\n",
            "\n",
            "\n",
            "\n",
            "Job Priorities#\n",
            "\n",
            "User Priority#\n",
            "Users can assign priority values to their jobs. Higher values result in increased job priority and faster access to resources.\n",
            "\n",
            "\n",
            "Resource Requirements#\n",
            "Jobs with larger resource requirements may be assigned higher priority, as they require more significant resources to execute efficiently.\n",
            "\n",
            "\n",
            "Walltime Limit#\n",
            "Source: https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\n",
            "\n",
            "Content:  larger resource requirements may be assigned higher priority, as they require more significant resources to execute efficiently.\n",
            "\n",
            "\n",
            "Walltime Limit#\n",
            "Jobs with shorter estimated execution times may receive higher priority, ensuring they are executed promptly and freeing up resources for other jobs.\n",
            "\n",
            "\n",
            "\n",
            "Balancing Policies#\n",
            "\n",
            "Backfilling#\n",
            "This policy allows smaller jobs to “backfill” into available resources ahead of larger jobs, optimizing resource utilization and reducing wait times.\n",
            "\n",
            "\n",
            "Preemption#\n",
            "Higher-priority jobs can preempt lower-priority ones, temporarily pausing the lower-priority job’s execution to make resources available for the higher-priority job.\n",
            "\n",
            "\n",
            "\n",
            "Best Practices#\n",
            "\n",
            "Set Realistic Priorities: Assign accurate priorities to your jobs to reflect their importance and resource requirements.\n",
            "Use Resource Quotas: Be mindful of the resources you request to prevent over- or underutilization.\n",
            "Leverage Backfilling: Submit smaller, shorter jobs that can backfill into available resources while waiting for larger jobs to start.\n",
            "\n",
            "Understanding these scheduling policies and priorities empowers you to make informed decisions when submitting jobs, ensuring that your computational tasks are executed efficiently and promptly. If you need further guidance on selecting the right scheduling policy for your job or optimizing your resource usage, our support team is available at rchelp@northeastern.edu or consult our Frequently Asked Questions (FAQs).\n",
            "Optimize your job execution by maximizing our cluster’s scheduling capabilities. Happy computing!\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Next\n",
            "\n",
            "Interactive and Batch Mode\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Previous\n",
            "\n",
            "Understanding the Queuing System\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                Copyright © 2023, RC\n",
            "            \n",
            "            Made with \n",
            "            Furo\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "            On this page\n",
            "          \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Job Scheduling Policies and Priorities\n",
            "Scheduling Policies\n",
            "FIFO (First-In-First-Out)\n",
            "Fair Share\n",
            "Priority-Based\n",
            "\n",
            "\n",
            "Job Priorities\n",
            "User Priority\n",
            "Resource Requirements\n",
            "Walltime Limit\n",
            "\n",
            "\n",
            "Balancing Policies\n",
            "Backfilling\n",
            "Preemption\n",
            "Source: https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\n",
            "\n",
            "Content: SBATCH --job-name=my_job\n",
            "#SBATCH --nodes=1\n",
            "#SBATCH --ntasks=4\n",
            "#SBATCH --time=01:00:00\n",
            "\n",
            "# Commands to execute\n",
            "module load my_program\n",
            "srun my_program.exe\n",
            "\n",
            "\n",
            "Save this script with a .sh extension, e.g., my_script.sh.\n",
            "\n",
            "\n",
            "Submitting Batch Jobs#\n",
            "You can submit your batch script using the sbatch command.\n",
            "sbatch my_script.sh\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Monitoring Batch Jobs#\n",
            "You can monitor the status of your batch job using the squeue command.\n",
            "squeue -u username\n",
            "\n",
            "\n",
            "Where username is your actual username.\n",
            "\n",
            "\n",
            "Use Cases#\n",
            "\n",
            "Long-Running Jobs: Suitable for extensive simulations or calculations.\n",
            "Scheduled Tasks: Execute jobs at specific times or under certain conditions.\n",
            "Automated Workflows: Manage complex workflows using multiple scripts.\n",
            "\n",
            "\n",
            "See also\n",
            "ADD LINK for More Examples and Guides for Batch Mode\n",
            "\n",
            "Interactive and Batch modes cater to different needs and scenarios in the HPC environment. You can explore both modes to choose the one that best aligns with your tasks. For more detailed guides and support, please consult the above guides or contact our support team at rchelp@northeastern.edu.\n",
            "Happy computing!\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Next\n",
            "\n",
            "Working with GPUs\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Previous\n",
            "\n",
            "Job Scheduling Policies and Priorities\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                Copyright © 2023, RC\n",
            "            \n",
            "            Made with \n",
            "            Furo\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "            On this page\n",
            "          \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Interactive and Batch Mode\n",
            "Interactive Mode\n",
            "Getting Started with Interactive Mode\n",
            "Interactive Mode Use Cases\n",
            "\n",
            "\n",
            "Batch Mode\n",
            "Creating Batch Scripts\n",
            "Submitting Batch Jobs\n",
            "Monitoring Batch Jobs\n",
            "Use Cases\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Source: https://rc-docs.northeastern.edu/en/latest/runningjobs/interactiveandbatch.html\n",
            "---------------------\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: What is Globus at northeaster?\n",
            "Assistant: I'm sorry, but there is no information available about \"Globus at northeaster\" in the provided sources.\n",
            "Human: How do i set up an account with Globus?\n",
            "Assistant: SOURCES:\n",
            "1. There is no information available about \"Globus at northeaster\" in the provided sources.\n",
            "Human: What is the Scheduling Policies for HPC cluster?\n",
            "Assistant: I'm sorry, but there is no information available about setting up an account with Globus at Northeastern University in the provided sources.\n",
            "Human: Can i assign priorities for this?\n",
            "Assistant: The scheduling policies for an HPC cluster include FIFO (First-In-First-Out), Fair Share, and Priority-Based. FIFO executes jobs in the order they are submitted. Fair Share ensures that all users receive a fair share of cluster resources over time. Priority-Based assigns priorities to jobs based on user-defined criteria or system-wide rules, with higher-priority jobs executed before lower-priority ones. \n",
            "\n",
            "SOURCES:\n",
            "- https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(memory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLT4uLaXZCR_",
        "outputId": "eb839c76-9585-4665-d251-64245f03f0ba"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain.memory.token_buffer.ConversationTokenBufferMemory"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoAgKYZNZJ1s",
        "outputId": "98ba51d6-d959-4f17-c2e0-72880d02d197"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConversationTokenBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='What is Globus at northeaster?'), AIMessage(content='I\\'m sorry, but there is no information available about \"Globus at northeaster\" in the provided sources.'), HumanMessage(content='How do i set up an account with Globus?'), AIMessage(content='SOURCES:\\n1. There is no information available about \"Globus at northeaster\" in the provided sources.'), HumanMessage(content='What is the Scheduling Policies for HPC cluster?'), AIMessage(content=\"I'm sorry, but there is no information available about setting up an account with Globus at Northeastern University in the provided sources.\"), HumanMessage(content='Can i assign priorities for this?'), AIMessage(content='The scheduling policies for an HPC cluster include FIFO (First-In-First-Out), Fair Share, and Priority-Based. FIFO executes jobs in the order they are submitted. Fair Share ensures that all users receive a fair share of cluster resources over time. Priority-Based assigns priorities to jobs based on user-defined criteria or system-wide rules, with higher-priority jobs executed before lower-priority ones. \\n\\nSOURCES:\\n- https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html'), HumanMessage(content='Can i assign priorities for this?'), AIMessage(content='SOURCES:\\n- https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html')]), input_key='question', return_messages=True, llm=ChatOpenAI(verbose=True, callbacks=[<langchain.callbacks.stdout.StdOutCallbackHandler object at 0x7bceb869e410>], client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', streaming=True), memory_key='chat_history', max_token_limit=1000)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Conversation Memory Buffer & Input Details:\n",
        "------------------------------------------\n",
        "- Input Key: question\n",
        "- Memory Key: chat_history\n",
        "- Maximum Token Limit: 1000\n",
        "- Return Messages: True\n",
        "\n",
        "ChatOpenAI Configuration:\n",
        "-------------------------\n",
        "- Verbose: True\n",
        "- Streaming: True\n",
        "- OpenAI API Key: [REDACTED]\n",
        "- OpenAI API Base: (No Value Provided)\n",
        "- OpenAI Organization: (No Value Provided)\n",
        "- OpenAI Proxy: (No Value Provided)\n",
        "- Client: ChatCompletion\n",
        "- Callbacks: FlyteCallbackHandler (Object Reference: 0x7832be39c9d0)\n",
        "\n",
        "Conversation History:\n",
        "---------------------\n",
        "1. User: What is Globus at Northeastern?\n",
        "\n",
        "   Assistant: Globus is a data management system that allows you to transfer and share files.\n",
        "   Northeastern has a subscription to Globus, and you can set up a Globus account using your Northeastern credentials.\n",
        "   You can also link your other accounts, whether they are personal or from another institution, with your Globus account.\n",
        "   To use Globus, you will need to set up an account and install Globus Connect on your local computer.\n",
        "   After completing these initial setup procedures, you can use the Globus web app to perform file transfers.\n",
        "\n",
        "   Sources:\n",
        "   - RCDocs (Using Globus)\n",
        "   - RCDocs (Globus Account Set Up)\n",
        "   - RCDocs (To use Globus)\n",
        "\n",
        "2. User: How do i set up an account with Globus?\n",
        "\n",
        "   Assistant: Globus is a data management system that allows you to transfer and share files.\n",
        "   Northeastern has a subscription to Globus, and you can set up an account with Globus using your Northeastern credentials.\n",
        "   You can also link your other accounts, whether they are personal or from another institution, with your Globus account.\n",
        "   To set up an account with Globus, follow these steps:\n",
        "   1. Go to the Globus website.\n",
        "   2. Click on \"Log In\".\n",
        "   3. Select \"Northeastern University\" from the options under \"Use your existing organizational login\" and click \"Continue\".\n",
        "   4. Enter your Northeastern username and password.\n",
        "   5. If you don't have a previous Globus account, click \"Continue\". If you have an existing account, click \"Link to an existing account\".\n",
        "   6. Check the agreement checkbox and click \"Continue\".\n",
        "   7. Click \"Allow\" to permit Globus to access your files.\n",
        "   After setting up your account, you can access the Globus File Manager app.\n",
        "\n",
        "   Sources:\n",
        "   - RCDocs (Using Globus)\n",
        "   - RCDocs (Globus Account Set Up)\n",
        "'''\n"
      ],
      "metadata": {
        "id": "5mrfHVdAZQer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jo16MuocEVXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OqU88pUbEVS1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}