{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d07e60c-79bf-4898-a746-a6e1456cf7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91b96f2d-6340-410d-a784-7dd50d6d5e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "900b2960-9d29-4a66-950e-d8c171fda643",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d43a98a2-94c3-4c46-82b2-83895f946d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_INSTRUCTOR_XL = \"hkunlp/instructor-xl\"\n",
    "EMB_SBERT_MPNET_BASE = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "039ca58e-65a5-4780-9c91-d2b3bc3f67a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_FLAN_T5_XXL = \"google/flan-t5-xxl\"\n",
    "LLM_FLAN_T5_XL = \"google/flan-t5-xl\"\n",
    "LLM_FASTCHAT_T5_XL = \"lmsys/fastchat-t5-3b-v1.0\"\n",
    "LLM_FLAN_T5_SMALL = \"google/flan-t5-small\"\n",
    "LLM_FLAN_T5_BASE = \"google/flan-t5-base\"\n",
    "LLM_FLAN_T5_LARGE = \"google/flan-t5-large\"\n",
    "LLM_FALCON_7B_INSTRUCT = \"tiiuae/falcon-7b-instruct\"\n",
    "LLM_FALCON_40B_INSTRUCT = \"tiiuae/falcon-40b-instruct\"\n",
    "LLM_FALCON_7B = \"tiiuae/falcon-7b\"\n",
    "LLM_FALCON_40B = \"tiiuae/falcon-40b\"\n",
    "LLM_LLAMA2_70B_INSTRUCT = \"upstage/Llama-2-70b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d839bbf-4377-4cdc-9c23-ff5e2cb79c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir='/work/rc/projects/chatbot/models'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae518ccf-4075-46f8-893e-5d715c43658a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"persist_directory\":None,\n",
    "          \"load_in_8bit\":False,\n",
    "          \"embedding\" : EMB_SBERT_MPNET_BASE,\n",
    "          \"llm\":LLM_FALCON_40B_INSTRUCT,\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78a52923-6b76-4a50-a4bd-9bc886af2a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/work/rc/projects/chatbot/models'\n",
    "#cache_folder=os.getenv('SENTENCE_TRANSFORMERS_HOME')\n",
    "os.environ['SENTENCE_TRANSFORMERS_HOME'] = '/work/rc/projects/chatbot/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e84c01f7-80cf-44cc-bb16-e973c4b7e2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-14 03:04:40.671121: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-14 03:04:42.090486: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c16a556c-29a4-4283-a14c-d6c5762227ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_llama2_70b_instruct(load_in_8bit=True):\n",
    "        model = LLM_LLAMA2_70B_INSTRUCT\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model , cache_dir=cache_dir)\n",
    "        hf_pipeline = pipeline(\n",
    "                task=\"text-generation\",\n",
    "                model = model,\n",
    "                do_sample=True, #Whether or not to use sampling ; use greedy decoding otherwise.\n",
    "                tokenizer = tokenizer,\n",
    "                #trust_remote_code = True,\n",
    "                max_new_tokens=500, #The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n",
    "                #cache_dir=cache_dir,\n",
    "                model_kwargs={\n",
    "                    \"device_map\": \"auto\", \n",
    "                    \"load_in_8bit\": load_in_8bit, \n",
    "                    \"max_length\": 512, \n",
    "                    \"temperature\": 0.01,\n",
    "                    \n",
    "                    \"torch_dtype\":torch.bfloat16,\n",
    "                    }\n",
    "            )\n",
    "        return hf_pipeline\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6ded4db-00ab-457d-9ad3-30417b6f4b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_falcon_40b(load_in_8bit=True):\n",
    "        model = LLM_FALCON_40B\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model , cache_dir=cache_dir)\n",
    "        hf_pipeline = pipeline(\n",
    "                task=\"text-generation\",\n",
    "                model = model,\n",
    "                do_sample=True, #Whether or not to use sampling ; use greedy decoding otherwise.\n",
    "                tokenizer = tokenizer,\n",
    "                #trust_remote_code = True,\n",
    "                max_new_tokens=500, #The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n",
    "                #cache_dir=cache_dir,\n",
    "                model_kwargs={\n",
    "                    \"device_map\": \"auto\", \n",
    "                    \"load_in_8bit\": load_in_8bit, \n",
    "                    \"max_length\": 512, \n",
    "                    \"temperature\": 0.01,\n",
    "                    \n",
    "                    \"torch_dtype\":torch.bfloat16,\n",
    "                    }\n",
    "            )\n",
    "        return hf_pipeline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca182968-c28f-41e5-a0ff-de1a03008a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_falcon_40b_instruct(load_in_8bit=True):\n",
    "        model = LLM_FALCON_40B_INSTRUCT\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model , cache_dir=cache_dir)\n",
    "        hf_pipeline = pipeline(\n",
    "                task=\"text-generation\",\n",
    "                model = model,\n",
    "                do_sample=True, #Whether or not to use sampling ; use greedy decoding otherwise.\n",
    "                tokenizer = tokenizer,\n",
    "                #trust_remote_code = True,\n",
    "                max_new_tokens=100, #The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n",
    "                #cache_dir=cache_dir,\n",
    "                model_kwargs={\n",
    "                    \"device_map\": \"auto\", \n",
    "                    \"load_in_8bit\": load_in_8bit, \n",
    "                    \"max_length\": 512, \n",
    "                    \"temperature\": 0.01,\n",
    "                    \n",
    "                    \"torch_dtype\":torch.bfloat16,\n",
    "                    }\n",
    "            )\n",
    "        return hf_pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c69bf69c-2e78-4690-b534-875648744310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_falcon_7b(load_in_8bit=True):\n",
    "        model = LLM_FALCON_7B\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model , cache_dir=cache_dir)\n",
    "        hf_pipeline = pipeline(\n",
    "                task=\"text-generation\",\n",
    "                model = model,\n",
    "                do_sample=True, #Whether or not to use sampling ; use greedy decoding otherwise.\n",
    "                tokenizer = tokenizer,\n",
    "                #trust_remote_code = True,\n",
    "                max_new_tokens=100, #The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n",
    "                #cache_dir=cache_dir,\n",
    "                model_kwargs={\n",
    "                    \"device_map\": \"auto\", \n",
    "                    \"load_in_8bit\": load_in_8bit, \n",
    "                    \"max_length\": 512, \n",
    "                    \"temperature\": 0.01,\n",
    "                    \n",
    "                    \"torch_dtype\":torch.bfloat16,\n",
    "                    }\n",
    "            )\n",
    "        return hf_pipeline\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfc19c2e-57f7-4bc4-9d1b-f20687fb0606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_falcon_7b_instruct(load_in_8bit=True):\n",
    "        model = LLM_FALCON_7B_INSTRUCT\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model , cache_dir=cache_dir)\n",
    "        hf_pipeline = pipeline(\n",
    "                task=\"text-generation\",\n",
    "                model = model,\n",
    "                do_sample=True, #Whether or not to use sampling ; use greedy decoding otherwise.\n",
    "                tokenizer = tokenizer,\n",
    "                #trust_remote_code = True,\n",
    "                max_new_tokens=500, #The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n",
    "                #cache_dir=cache_dir,\n",
    "                model_kwargs={\n",
    "                    \"device_map\": \"auto\", \n",
    "                    \"load_in_8bit\": load_in_8bit, \n",
    "                    \"max_length\": 512, \n",
    "                    \"temperature\": 0.01,\n",
    "                    \n",
    "                    \"torch_dtype\":torch.bfloat16,\n",
    "                    }\n",
    "            )\n",
    "        return hf_pipeline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b66995a-1b3d-4989-94e5-e96c848896a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_flan_t5_base(load_in_8bit=True):\n",
    "        # Wrap it in HF pipeline for use with LangChain\n",
    "        model=\"google/flan-t5-base\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model, cache_dir=cache_dir)\n",
    "        return pipeline(\n",
    "            task=\"text2text-generation\",\n",
    "            model=model,\n",
    "            tokenizer = tokenizer,\n",
    "            max_new_tokens=100,\n",
    "            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.}\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca743027-6ffc-40ae-b624-a05ad8b307ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_in_8bit = config[\"load_in_8bit\"]\n",
    "if config[\"llm\"] == LLM_FLAN_T5_BASE:\n",
    "    llm = create_flan_t5_base(load_in_8bit=load_in_8bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32bbeab2-1d7c-4aad-ba51-14a19da0d58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_in_8bit = config[\"load_in_8bit\"]\n",
    "\n",
    "if config[\"llm\"] == LLM_FALCON_40B:\n",
    "    llm = create_falcon_40b(load_in_8bit=load_in_8bit)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "810f0f3f-4222-4b8c-9039-14aceaf0c261",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/rc/projects/chatbot/conda_env/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.01` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c393b706b634433db41f9417905e0f5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_in_8bit = config[\"load_in_8bit\"]\n",
    "\n",
    "if config[\"llm\"] == LLM_FALCON_40B_INSTRUCT:\n",
    "    llm = create_falcon_40b_instruct(load_in_8bit=load_in_8bit)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a89e1adb-3d0d-43c2-bc65-2ba44c1a5d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_in_8bit = config[\"load_in_8bit\"]\n",
    "\n",
    "if config[\"llm\"] == LLM_FALCON_7B_INSTRUCT:\n",
    "    llm = create_falcon_7b_instruct(load_in_8bit=load_in_8bit)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8de42604-fa88-4a53-b969-867d64136198",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_in_8bit = config[\"load_in_8bit\"]\n",
    "\n",
    "if config[\"llm\"] == LLM_FALCON_7B:\n",
    "    llm = create_falcon_7b(load_in_8bit=load_in_8bit)\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4cc8d24e-778c-492d-937e-394c9743c538",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_in_8bit = config[\"load_in_8bit\"]\n",
    "\n",
    "if config[\"llm\"] == LLM_LLAMA2_70B_INSTRUCT:\n",
    "    llm = create_llama2_70b_instruct(load_in_8bit=load_in_8bit)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09546165-5f43-4dcf-8baa-7fa6b9058d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "\n",
    "\n",
    "hf_llm = HuggingFacePipeline(pipeline = llm, model_kwargs = {'temperature':0})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef3a6e21-c42e-4b85-bc64-553ddbb2e22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate,  LLMChain\n",
    "\n",
    "template = \"\"\"\n",
    "<Instructions>\n",
    "Important:\n",
    "You are an intelligent chatbot. Answer the question with the facts listed in Content below. If there isn't enough information below, say you don't know.\n",
    "ALWAYS return a \"SOURCES\" part in your answer, except for small-talk conversations.\n",
    "\n",
    "Question: {question}\n",
    "Content:   \n",
    "\n",
    "Job Scheduling Policies and Priorities#\n",
    "In an HPC environment, efficient job scheduling is crucial for allocating computing resources and ensuring optimal cluster utilization. Job scheduling policies and priorities determine the order in which jobs are executed and the resources they receive. Understanding these policies is essential for maximizing job efficiency and minimizing wait times.\n",
    "\n",
    "Scheduling Policies#\n",
    "\n",
    "FIFO (First-In-First-Out)#\n",
    "Jobs are executed in the order they are submitted. Although simple, this policy may lead to long wait times for large, resource-intensive jobs if smaller jobs are constantly being submitted.\n",
    "\n",
    "\n",
    "Fair Share#\n",
    "This policy ensures that all users receive a fair share of cluster resources over time. Users with high resource usage may experience reduced priority, allowing others to access resources more regularly.\n",
    "\n",
    "\n",
    "Priority-Based#\n",
    "Jobs are assigned priorities based on user-defined criteria or system-wide rules. Higher-priority jobs are executed before lower-priority ones, allowing for resource allocation based on user requirements.\n",
    "\n",
    "\n",
    "\n",
    "Job Priorities#\n",
    "\n",
    "User Priority#\n",
    "Users can assign priority values to their jobs. Higher values result in increased job priority and faster access to resources.\n",
    "\n",
    "\n",
    "Resource Requirements#\n",
    "Jobs with larger resource requirements may be assigned higher priority, as they require more significant resources to execute efficiently.\n",
    "\n",
    "\n",
    "Walltime Limit#\n",
    "Source: https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\n",
    "\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=hf_llm)\n",
    "\n",
    "question = \"What is the Scheduling Policies for HPC cluster?\" \n",
    "\n",
    "#print(llm_chain.run(question))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d0950e5-5692-42ba-9c10-554192d0bd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/rc/projects/chatbot/conda_env/lib/python3.9/site-packages/transformers/generation/utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "The current implementation of Falcon calls `torch.scaled_dot_product_attention` directly, this will be deprecated in the future in favor of the `BetterTransformer` API. Please install the latest optimum library with `pip install -U optimum` and call `model.to_bettertransformer()` to benefit from `torch.scaled_dot_product_attention` and future performance optimizations.\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "print(llm_chain.run(question))\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Time take : \" , elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fa5d38-eb87-4571-8efb-485534c7fcd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0812a12c-5c9d-4914-a403-62ba386041f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6744fcef-f08d-47aa-bfa9-f49d45f8d38c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15042891-1cd7-45bc-b597-b37733c9db7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Chatbot Environment",
   "language": "python",
   "name": "chatbot_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
