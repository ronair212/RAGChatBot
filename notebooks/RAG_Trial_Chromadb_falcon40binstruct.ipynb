{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51d19f7e-e096-49b1-9698-6a2958921a2b",
   "metadata": {},
   "source": [
    "\n",
    "# Retrieval Augmented Generation using ChromaDB and FALCON_40B\n",
    "\n",
    "## Overview\n",
    "\n",
    "Navigating through tokenization to intelligent query processing, this documentation unveils a structured approach to text management and model utilization. Initially, it leverages `tiktoken` for precise tokenization and employs strategic text splitters to ensure optimal text segmentation. Subsequently, a selection of pre-trained transformer models like `SBERT MPNet` and `FALCON_40B` are integrated and configured via meticulously crafted functions and a flexible configuration dictionary. The process culminates by intertwining embedding retrievers with initialized Language Models, establishing a retrieval-based Question Answering system that adeptly navigates user queries, showcasing a judicious amalgamation of structured text management and intelligent data querying in Natural Language Processing applications.\n",
    "\n",
    "\n",
    "\n",
    "## 1. Tokenization and Document Splitting\n",
    "\n",
    "### Token Counting\n",
    "A function named `num_tokens_from_string` utilizes `tiktoken` to calculate and return the number of tokens in a given string. It accepts the text and an encoding name as input arguments, using them to encode the text and return its token length.\n",
    "\n",
    "### Text Splitting\n",
    "- `TokenTextSplitter`: Splits texts into chunks with specified sizes and overlaps.\n",
    "- `RecursiveCharacterTextSplitter`: Further divides texts with considerations for character count, overlap, and potential additional metadata.\n",
    "  \n",
    "Both splitters aim to break down text into manageable sizes for subsequent processing, ensuring that models can handle them within their token limits.\n",
    "\n",
    "## 2. Model Definitions and Setup\n",
    "\n",
    "The code incorporates various pre-trained transformer models for embeddings and Language Model (LM) generation. Model identifiers and a caching directory are specified at the beginning of this section.\n",
    "\n",
    "### Models Used\n",
    "- `EMB_SBERT_MPNET_BASE`: Sentence transformer model for embeddings.\n",
    "- `EMB_INSTRUCTOR_XL`: Not utilized in the provided code.\n",
    "- `LLM_FALCON_7B` and `LLM_FALCON_40B`: Pre-trained transformer models for text generation.\n",
    "\n",
    "### Cache Directory Setup\n",
    "The cache directory (`/work/rc/projects/chatbot/models`) is set in the environment variables to store downloaded model weights, ensuring they are readily available for subsequent runs.\n",
    "\n",
    "## 3. Model Creators\n",
    "\n",
    "The code defines several functions to create models and pipelines, notably:\n",
    "- `create_sbert_mpnet()`: Initializes the SBERT MPNet model.\n",
    "- `create_falcon_40b_instruct()` and `create_falcon_7b_instruct()`: Set up models for text generation via Hugging Face’s pipeline, configuring tokenizers and various model arguments.\n",
    "- `create_flan_t5_base()`: Sets up a T5 model pipeline from Google for text-to-text generation.\n",
    "\n",
    "These functions handle the instantiation and configuration of the models, ensuring they are set up with the appropriate parameters and caching.\n",
    "\n",
    "## 4. Model and Pipeline Configuration \n",
    "\n",
    "A configuration dictionary `config` holds keys for adjusting model parameters and selection. Depending on this configuration:\n",
    "- The corresponding embedding model is initialized.\n",
    "- One of the LLM models (Falcon or T5) is chosen and instantiated based on the specified parameters.\n",
    "\n",
    "## 5. Data Processing and Question Answering Setup \n",
    "\n",
    "Here, a sample data string `data` is defined and split into documents using the earlier mentioned text splitter. Then, it sets up the embeddings and retrieval-based Question Answering (QA) system. \n",
    "\n",
    "### RetrievalQA Setup\n",
    "- An instance of `HuggingFacePipeline` is initialized using the previously created Language Models.\n",
    "- The embedding model's retriever is configured.\n",
    "- The QA model is built using the retriever and the pipeline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f97713d5-0e72-48b2-9d3a-2c5e883ded11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MNSwbz8svgCi",
    "outputId": "edbdd442-5caa-4c86-b902-da4a5f72f106"
   },
   "outputs": [],
   "source": [
    "#!pip install pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d545559d-fc07-4386-bf1f-c7ba2ffff1b3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MuNwBfb6BngZ",
    "outputId": "30409fdb-2773-411e-dcfc-170ab7fc63f7"
   },
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52313acc-a137-4988-8c60-b54bf031560e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nwJCPFhtCBmi",
    "outputId": "aa6b6914-32a8-4f4d-e68c-84e5077d0fad"
   },
   "outputs": [],
   "source": [
    "#!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f33747f5-5b69-4ec3-9fb0-4014e1b516d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vwqNT7T0CF-O",
    "outputId": "9fc123df-4163-4f34-f81f-610513bc1fe5"
   },
   "outputs": [],
   "source": [
    "#!pip install cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbb2a466-1414-48a1-92e5-aee7a157c1df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QQl_Bfl8CLL1",
    "outputId": "cc07de5a-2e7c-4b28-a66d-5906fea3a052"
   },
   "outputs": [],
   "source": [
    "#!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c4736a2-64e2-4100-9ec1-8293d9ba1b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c96bce9f-2e23-4b3c-b416-a8308202086a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cc597bf-9b24-460a-91bf-e303782fe92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "608c8ac9-1f4c-409e-91e5-c0ce1cf9ff87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04c91c45-ee77-42e4-84aa-6770a18c93c5",
   "metadata": {
    "id": "mq7rbJww6Cjo"
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "\n",
    "urls = [\"https://rc-docs.northeastern.edu/en/latest/runningjobs/understandingqueuing.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/interactiveandbatch.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/workingwithgpus.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/recurringjobs.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/debuggingjobs.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/../datamanagement/index.html\",\n",
    "]\n",
    "loader = WebBaseLoader(urls)\n",
    "data = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcdc2770-3a12-4506-9798-d7eee8501654",
   "metadata": {
    "id": "6twYZJo26GmB"
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "encoding_name = tiktoken.get_encoding(\"cl100k_base\")\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f98e16e5-1a1d-41ce-98cf-5432bb997295",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "9b3T5m1YBVrf",
    "outputId": "4f06dc21-756e-4dd9-f462-c503f4dd675b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\n\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    chunk_size = 700,\\n    chunk_overlap  = 70,\\n    length_function = len,\\n    add_start_index = True,\\n)\\ndocs = text_splitter.create_documents([data])\\n\\nfor idx, text in enumerate(docs):\\n    docs[idx].metadata[\\'source\\'] = \"RCDocs\"\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "text_splitter = TokenTextSplitter(chunk_size=500, chunk_overlap=25)\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "'''\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 700,\n",
    "    chunk_overlap  = 70,\n",
    "    length_function = len,\n",
    "    add_start_index = True,\n",
    ")\n",
    "docs = text_splitter.create_documents([data])\n",
    "\n",
    "for idx, text in enumerate(docs):\n",
    "    docs[idx].metadata['source'] = \"RCDocs\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34c44c26-868e-4d12-b739-aeb81ee76da8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_DfcNy9sT3jj",
    "outputId": "36618e1f-e92c-4ed3-f051-980b780bc57b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain.schema.document.Document"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d6b0f13-2dad-4e7e-b496-bd2193c5bfa2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PILXy2pdVC4v",
    "outputId": "99fdfd5c-a9f0-422a-dded-c4c33e642735"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='\\n\\n\\n\\n\\n\\n\\nUnderstanding the Queuing System - RC RTD\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nContents\\n\\n\\n\\n\\n\\nMenu\\n\\n\\n\\n\\n\\n\\n\\nExpand\\n\\n\\n\\n\\n\\nLight mode\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDark mode\\n\\n\\n\\n\\n\\n\\nAuto light/dark mode\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHide navigation sidebar\\n\\n\\nHide table of contents sidebar\\n\\n\\n\\n\\n\\nToggle site navigation sidebar\\n\\n\\n\\n\\nRC RTD\\n\\n\\n\\n\\nToggle Light / Dark / Auto color theme\\n\\n\\n\\n\\n\\n\\nToggle table of contents sidebar\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nResearch ComputingToggle child pages in navigation\\nWelcome\\nServices We Provide\\nGetting Help\\nIntroduction to HPC and Slurm\\nCase Studies and User Testimonials\\n\\n\\n\\n\\nGetting StartedToggle child pages in navigation\\nGetting Access\\nAccount Manager\\nConnecting To ClusterToggle child pages in navigation\\nMac\\nWindows\\n\\n\\n\\n\\nFirst StepsToggle child pages in navigation\\nPasswordless SSH\\nShell Environment on the Cluster\\nCluster via Command-Line\\n\\n\\n\\nUser Guides\\n\\nHardwareToggle child pages in navigation\\nOverview\\nPartitions\\n\\n\\nOpen OnDemand (OOD)Toggle child pages in navigation\\nIntroduction to OOD\\nAccessing Open OnDemand\\nInteractive Open OnDemand ApplicationsToggle child pages in navigation\\nDesktop App\\nOOD File Explorer\\nJupyterLab\\nStata\\n\\n\\n\\n\\nRunning JobsToggle child pages in navigation\\nUnderstanding the Queuing System\\nJob Scheduling Policies and Priorities\\nInteractive and Batch Mode\\nWorking with GPUs\\nRecurring Jobs\\nDebugging and Troubleshooting Jobs\\n\\n\\nData ManagementToggle child pages in navigation\\nData Storage Options\\nTransfer Data\\nUsing Globus\\nData Backup and Restore\\nSecurity and Compliance\\n\\n\\nSoftwareToggle child pages in navigation\\nSystem WideToggle child pages in navigation\\nModules\\nMPI\\nR\\nMatlab\\n\\n\\nPackage ManagersToggle child pages in navigation\\nConda\\nSpack\\n\\n\\nFrom SourceToggle child pages in navigation\\nMake\\nCMake\\n\\n\\n\\n\\nSlurmToggle child pages in navigation\\nIntroduction to Slurm\\nSlurm Commands\\nSlurm Running Jobs\\nMonitoring and Managing Jobs\\nSlurm Job Scripts\\nSlurm Array Jobs and Dependencies\\nSlurm Best Practices\\n\\n\\nHPC for the ClassroomT', metadata={'source': 'https://rc-docs.northeastern.edu/en/latest/runningjobs/understandingqueuing.html', 'title': 'Understanding the Queuing System - RC RTD', 'language': 'en'})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "592fd0b3-1578-4e80-ac09-a9ab6a8a7b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_INSTRUCTOR_XL = \"hkunlp/instructor-xl\"\n",
    "EMB_SBERT_MPNET_BASE = \"sentence-transformers/all-mpnet-base-v2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a9af797-f1bb-433a-b495-cc98ac36081e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_FLAN_T5_XXL = \"google/flan-t5-xxl\"\n",
    "LLM_FLAN_T5_XL = \"google/flan-t5-xl\"\n",
    "LLM_FASTCHAT_T5_XL = \"lmsys/fastchat-t5-3b-v1.0\"\n",
    "LLM_FLAN_T5_SMALL = \"google/flan-t5-small\"\n",
    "LLM_FLAN_T5_BASE = \"google/flan-t5-base\"\n",
    "LLM_FLAN_T5_LARGE = \"google/flan-t5-large\"\n",
    "LLM_FALCON_7B = \"tiiuae/falcon-7b-instruct\"\n",
    "LLM_FALCON_40b = \"tiiuae/falcon-40b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d274b67-365b-4fe8-8590-0a0d3f9b2dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir='/work/rc/projects/chatbot/models'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19979ba8-7a98-461d-abbe-54df6d71bcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"persist_directory\":None,\n",
    "          \"load_in_8bit\":False,\n",
    "          \"embedding\" : EMB_SBERT_MPNET_BASE,\n",
    "          \"llm\":LLM_FALCON_40b,\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47672542-7726-458d-8077-f02d45edd67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/work/rc/projects/chatbot/models'\n",
    "#cache_folder=os.getenv('SENTENCE_TRANSFORMERS_HOME')\n",
    "os.environ['SENTENCE_TRANSFORMERS_HOME'] = '/work/rc/projects/chatbot/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d07a707a-255f-41c7-b71e-76df7e75afab",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def create_sbert_mpnet():\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        return HuggingFaceEmbeddings(model_name=EMB_SBERT_MPNET_BASE, model_kwargs={\"device\": device})\n",
    "\n",
    "'''\n",
    "\n",
    "def create_sbert_mpnet():\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        return HuggingFaceEmbeddings(model_name=EMB_SBERT_MPNET_BASE, cache_folder=cache_dir, model_kwargs={\"device\": device})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\", cache_dir=\"new_cache_dir/\")\n",
    "\n",
    "#model = AutoModelForMaskedLM.from_pretrained(\"roberta-base\", cache_dir=\"new_cache_dir/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21f57029-387f-4901-ac8b-04b03ec03f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-10 11:30:31.803164: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-10 11:30:33.724973: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "216b63a5-6c81-415c-8bf5-e4092448c40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_falcon_40b_instruct(load_in_8bit=False):\n",
    "        model = LLM_FALCON_40b\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model , cache_dir=cache_dir)\n",
    "        hf_pipeline = pipeline(\n",
    "                task=\"text-generation\",\n",
    "                model = model,\n",
    "                do_sample=True,\n",
    "                tokenizer = tokenizer,\n",
    "                #trust_remote_code = True,\n",
    "                max_new_tokens=100,\n",
    "                #cache_dir=cache_dir,\n",
    "                model_kwargs={\n",
    "                    \"device_map\": \"auto\", \n",
    "                    \"load_in_8bit\": load_in_8bit, \n",
    "                    \"max_length\": 512, \n",
    "                    \"temperature\": 0.01,\n",
    "                    \n",
    "                    \"torch_dtype\":torch.bfloat16,\n",
    "                    }\n",
    "            )\n",
    "        return hf_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cac57891-9948-4517-86f0-c6bd141b5d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n WARNING: You are currently loading Falcon using legacy code contained in the model repository. \\n Falcon has now been fully ported into the Hugging Face transformers library. \\n For the most up-to-date and high-performance version of the Falcon model code, \\n please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def create_flan_t5_base(load_in_8bit=False):\n",
    "        # Wrap it in HF pipeline for use with LangChain\n",
    "        model=\"google/flan-t5-base\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model, cache_dir=cache_dir)\n",
    "        return pipeline(\n",
    "            task=\"text2text-generation\",\n",
    "            model=model,\n",
    "            tokenizer = tokenizer,\n",
    "            max_new_tokens=100,\n",
    "            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.}\n",
    "        )\n",
    "        \n",
    "'''\n",
    " WARNING: You are currently loading Falcon using legacy code contained in the model repository. \n",
    " Falcon has now been fully ported into the Hugging Face transformers library. \n",
    " For the most up-to-date and high-performance version of the Falcon model code, \n",
    " please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85552c12-701a-4984-8436-a7273d08e7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"embedding\"] == EMB_SBERT_MPNET_BASE:\n",
    "    embedding = create_sbert_mpnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22578aea-3ace-4378-bb16-c6b1ac86bb5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_in_8bit = config[\"load_in_8bit\"]\n",
    "if config[\"llm\"] == LLM_FLAN_T5_BASE:\n",
    "    llm = create_flan_t5_base(load_in_8bit=load_in_8bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "921d211e-6c86-4de8-afc2-09659c45487b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/rc/projects/chatbot/conda_env/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.01` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "016343a0317a4ee4b8543b4c8964a98e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_in_8bit = config[\"load_in_8bit\"]\n",
    "\n",
    "if config[\"llm\"] == LLM_FALCON_40b:\n",
    "    llm = create_falcon_40b_instruct(load_in_8bit=load_in_8bit)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc5c3520-4d1d-4b9f-b034-a5dfbca76a4b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"\"\"\n",
    "\n",
    "Introduction to OOD#\n",
    "Open OnDemand (OOD) is a web portal to the Discovery cluster. A Discovery account is necessary for you to access OOD. If you need an account, see Request an account. If you already have an account, in a web browser go to http://ood.discovery.neu.edu and sign in with your Northeastern username and password.\n",
    "OOD provides you with several resources for interacting with the Discovery cluster:\n",
    "\n",
    "Launch a terminal within your web browser without needing a separate terminal program. This is an advantage if you use Windows, as otherwise, you need to download and use a separately installed program, such as MobaXterm.\n",
    "Use software applications like SAS Studio that run in your browser without further configuration. See Interactive Open OnDemand Applications for more information.\n",
    "View, download, copy, and delete files using the OOD File Explorer feature.\n",
    "\n",
    "Note\n",
    "OOD is a web-based application. You access it by using a web browser. Like many web-based applications, it has compatibility issues with specific web browsers. Use OOD with newer Chrome, Firefox, or Internet Explorer versions for optimal results. OOD does not currently support Safari or mobile devices (phones and tablets).\n",
    "\n",
    "https://rc-docs.northeastern.edu/en/latest/using-ood/accessingood.html\n",
    "\n",
    "Accessing Open OnDemand#\n",
    "Open OnDemand (OOD) is a web portal to the HPC cluster.\n",
    "This topic is for connecting to the HPC cluster through the browser application Open OnDemand. If you want to access the HPC directly on your system rather than through a browser, please see Connecting To Cluster, whether Mac or Windows.\n",
    "A cluster account is necessary for you to access OOD. If you need an account, see Getting Access. After you have created a cluster account, access the cluster through Open OnDemand (OOD) via the following steps:\n",
    "\n",
    "In a web browser, go to http://ood.discovery.neu.edu.\n",
    "At the prompt, enter your Northeastern username and password. Note that your username is the first part of your email without the @northeastern, such as j.smith.\n",
    "Press Enter or click Sign in.\n",
    "\n",
    "Watch the following video for a short tutorial. If you do not see any controls on the video,\n",
    "right-click on the video to see viewing options.\n",
    "\n",
    "  Your browser does not support the video tag.\n",
    "\n",
    "https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/index.html\n",
    "\n",
    "Interactive Open OnDemand Applications#\n",
    "\n",
    "Desktop App\n",
    "\n",
    "OOD File Explorer\n",
    "\n",
    "JupyterLab\n",
    "\n",
    "The OOD web portal provides a range of applications. Upon clicking launch, the Slurm scheduler assigns a compute node with a specified number of cores and memory. By default, applications run for one hour. If you require more than an hour, you may have to wait for Slurm to allocate resources for the duration of your request.\n",
    "\n",
    "Applications on OOD#\n",
    "\n",
    "The Open OnDemand interface offers several applications, which as of June 2023, include:\n",
    "\n",
    "JupyterLab\n",
    "RStudio (Rocker)\n",
    "Matlab\n",
    "Schrodinger (Maestro)\n",
    "Desktop\n",
    "Gaussian (GaussView)\n",
    "KNIME\n",
    "TensorBoard\n",
    "SAS\n",
    "\n",
    "These applications can be accessed from the OOD web interface’s Interactive Apps drop-down menu.\n",
    "\n",
    "Note\n",
    "Specific applications in the Interactive Apps section, particularly those with graphical user interfaces (GUIs), may require X11 forwarding and the setup of passwordless SSH. For tips and troubleshooting information on X11 forwarding setup and usage, please look at the [Using X11] section of our documentation.\n",
    "\n",
    "Additionally, we offer a selection of modified standard applications intended to support specific coursework. These applications are under the Courses menu on the OOD web interface. Please note that these course-specific applications are only accessible to students enrolled in the respective courses.\n",
    "\n",
    "Note\n",
    "Certain apps are reserved for specific research groups and are not publicly accessible, as indicated by the “Restricted” label next to the application name. If you receive an access error when attempting to open a restricted app, and you believe you should have access to it, please email rchelp@northeastern.edu with the following information: your username, research group, the app you are trying to access, and a screenshot of the error message. We will investigate and address the issue.\n",
    "\n",
    "Go to [Open On Demand] in a web browser. If prompted, enter your MyNortheastern username and password.\n",
    "Select Interactive Apps, then select the application you want to use.\n",
    "Keep the default options for most apps, then click Launch. You might have to wait a minute or two for a compute node to be available for your requested time and resource.\n",
    "\n",
    "https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/desktopood.html\n",
    "\n",
    "Desktop App#\n",
    "Open OnDemand provides a containerized desktop to run on the HPC cluster.\n",
    "The following tools and programs are accessible on our Desktop App:\n",
    "\n",
    "Slurm (for running Slurm commands via the terminal in the desktop and interacting on compute nodes)\n",
    "Module command (for loading and running HPC-ready modules)\n",
    "File explorer (able to traverse and view files that you have access to on the HPC)\n",
    "Firefox web browser\n",
    "VLC media player\n",
    "LibreOffice suite of applications (word, spreadsheet, and presentation processing)\n",
    "\n",
    "Note\n",
    "The desktop application is a Singularity container; a Singularity container cannot run inside the desktop application. It fails if users run a container-based module or program via the desktop application.\n",
    "\n",
    "https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/fileexplore.html\n",
    "\n",
    "OOD File Explorer#\n",
    "When working with the resources in OOD, your files are stored in your home directory on the storage space on the Discovery cluster. Like any file navigation system, you can work with your files and directories through the OOD Files feature, as detailed below. For example, you can download a Jupyter Notebook file in OOD that you have been working on to your local hard drive, rename a file, or delete a file you no longer need.\n",
    "\n",
    "Note\n",
    "Your home directory has a file size limit of 75GB. Please check your home directory regularly, and remove any files you do not need to make sure you have enough space.\n",
    "\n",
    "In a web browser, go to ood.discovery.neu.edu. If prompted, enter your MyNortheastern username and password.\n",
    "Select Files > Home Directory. The contents of your home directory display in a new tab.\n",
    "To download a file to your hard drive, navigate to the file you want to download,\n",
    "select the file, and click Download. If prompted by your browser,\n",
    "click OK to save your file to your hard drive.\n",
    "To navigate to another folder on the Discovery file system, click Go To,\n",
    "enter the path to the folder you want to access and click OK.\n",
    "\n",
    "Note\n",
    "From the Files > Home Directory view, the Edit button will not launch your .ipynb file in a Jupyter Notebook. It will open the file in a text editor. You must be in Jupyter Notebook to launch a .ipynb file from your /home directory. See Interactive Open OnDemand Applications to access a Jupyter Notebook through OOD.\n",
    "\n",
    "https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/jupyterlab.html\n",
    "\n",
    "JupyterLab#\n",
    "JupyterLab Notebook is one of the interactive apps on OOD. This section will provide a walk through of setting up and using this app. The general workflow is to create a virtual Python environment, ensure that JupyterLab Notebook uses your virtual environment, and reference this environment when you start the JupyterLab Notebook OOD interactive app.\n",
    "To find the JupyterLab Notebook on OOD, follow these steps:\n",
    "\n",
    "Go to [Open On Demand].\n",
    "Click on Interactive Apps.\n",
    "Select JupyterLab Notebook from the drop-down list.\n",
    "\n",
    "The OOD form for launching JupyterLab Notebook will appear.\n",
    "\n",
    "Conda virtual environment\n",
    "You can import Python packages in your JupyterLab Notebook session by creating a conda virtual environment and activating that environment when starting a JupyterLab Notebook instance.\n",
    "\n",
    "First, set up a virtual Python environment. See Creating Environments for how to set up a virtual Python environment on the HPC using the terminal.\n",
    "Type source activate <yourenvironmentname> where <yourenvironmentname> is the name of your custom environment.\n",
    "Type conda install jupyterlab -y to install JupyterLab in your environment.\n",
    "\n",
    "Using OOD to launch JupyterLab Notebook#\n",
    "\n",
    "Go to [Open On Demand].\n",
    "Click Interactive Apps, then select JupyterLab Notebook.\n",
    "Enter your Working Directory (e.g., /home/<username> or /work/<project>) that you want JupyterLab Notebook to launch in.\n",
    "Select from the Partition drop-down menu the partition you want to use for your session. Refer to Partitions for the resource restrictions for the different partitions. If you need a GPU, select the gpu partition.\n",
    "Select the compute node features for the job:\n",
    "\n",
    "In the Time field, enter the number of hour(s) needed for the job.\n",
    "Enter the memory you need for the job in the Memory (in Gb) field.\n",
    "If you selected the gpu partition from the drop-down menu, select the GPU you would like to use and the version of CUDA that you would like to use for your session under the respective drop-down menus.\n",
    "\n",
    "Select the Anaconda version you used to create your virtual Python environment in the System-wide Conda Module field.\n",
    "Check the Custom Anaconda Environment box, and enter the name of your custom virtual Python environment in the Name of Custom Conda Environment field.\n",
    "Click Launch to join the queue for a compute node. This might take a few minutes, depending on what you asked for.\n",
    "When allocated a compute node, click Connect to Jupyter.\n",
    "\n",
    "When your JupyterLab Notebook is running and open, type conda list in a cell and run the cell to confirm that the environment is your custom conda environment (you should see this on the first line). This command will also list all of your available packages.\n",
    "\n",
    "\n",
    "Understanding the Queuing System#\n",
    "The queuing system in a high-performance computing (HPC) environment manages and schedules computing tasks. Our HPC cluster uses the Slurm Workload Manager as our queuing system. This section aims to help you understand how the queuing system works and how to interact effectively.\n",
    "\n",
    "Introduction to Queuing Systems#\n",
    "The Slurm scheduler manages jobs in the queue. When you submit a job, it gets placed in the queue. The scheduler then assigns resources to the job when they become available, according to the job’s priority and the available resources.\n",
    "\n",
    "Job Submission and Scheduling#\n",
    "Jobs are submitted to the queue via a script specifying the resources required (e.g., number of CPUs, memory, and GPUs) and the commands to be executed. Once submitted, the queuing system schedules the job based on the resources requested, the current system load, and scheduling policies.\n",
    "\n",
    "Scheduling Policies**#\n",
    "Our cluster uses a fair-share scheduling policy. This means that usage is tracked for each user or group, and the system attempts to balance resource allocation over time. If a user or group has been using many resources, their job priority may be temporarily reduced to allow others to use the system. Conversely, users or groups that have used fewer resources will have their jobs prioritized.\n",
    "The following policies ensure fair use of the cluster resources:\n",
    "\n",
    "Single job size: The maximum number of nodes a single job depends on the partition (see Partitions).\n",
    "Run time limit: The maximum run time for a job depends on the partition (see Partitions).\n",
    "Priority decay: If a job remains in the queue without running for an extended period, its priority may slowly decrease.\n",
    "\n",
    "Job Priority**#\n",
    "Several factors determine job priority:\n",
    "\n",
    "Fair-share: This is based on the historical resource usage of your group. The more resources your group has used, the lower your job’s priority becomes, and vice versa.\n",
    "Job size: Smaller jobs (regarding requested nodes) typically have higher priority.\n",
    "Queue wait time: The longer a job has been in the queue, the higher its priority becomes.\n",
    "\n",
    "Job States#\n",
    "Each job in the queue has a state. The main job states are:\n",
    "\n",
    "Pending (PD): The job is waiting for resources to become available.\n",
    "Running (R): The job is currently running.\n",
    "Completed (CG): The job has been completed successfully.\n",
    "\n",
    "A complete list of job states can be found in the Slurm documentation.\n",
    "\n",
    "Monitoring the Queue**#\n",
    "You can use the following commands to interact with the queue:\n",
    "\n",
    "squeue: Displays the state of jobs or job steps. It has a wide variety of filtering, sorting, and formatting options. For example, to display your jobs:\n",
    "\n",
    "squeue -u your_username\n",
    "\n",
    "scontrol: Used to view and modify Slurm configuration and state. For example, to show the details of a specific job:\n",
    "\n",
    "scontrol show job your_job_id\n",
    "\n",
    "Tips for Efficient Queue Usage**#\n",
    "\n",
    "Request only the resources you need: Overestimating your job’s requirements can result in longer queue times.\n",
    "Break up large jobs: Large jobs tend to wait in the queue longer than small jobs. Break up large jobs into smaller ones.\n",
    "Use idle resources: Sometimes, idle resources can be used. If your job is flexible regarding start time and duration, you can use the --begin and --time options to take advantage of these idle resources.\n",
    "\n",
    "https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\n",
    "\n",
    "Job Scheduling Policies and Priorities#\n",
    "In an HPC environment, efficient job scheduling is crucial for allocating computing resources and ensuring optimal cluster utilization. Job scheduling policies and priorities determine the order in which jobs are executed and the resources they receive. Understanding these policies is essential for maximizing job efficiency and minimizing wait times.\n",
    "\n",
    "Scheduling Policies#\n",
    "\n",
    "FIFO (First-In-First-Out)#\n",
    "Jobs are executed in the order they are submitted. Although simple, this policy may lead to long wait times for large, resource-intensive jobs if smaller jobs are constantly being submitted.\n",
    "\n",
    "Fair Share#\n",
    "This policy ensures that all users receive a fair share of cluster resources over time. Users with high resource usage may experience reduced priority, allowing others to access resources more regularly.\n",
    "\n",
    "Priority-Based#\n",
    "Jobs are assigned priorities based on user-defined criteria or system-wide rules. Higher-priority jobs are executed before lower-priority ones, allowing for resource allocation based on user requirements.\n",
    "\n",
    "Job Priorities#\n",
    "\n",
    "User Priority#\n",
    "Users can assign priority values to their jobs. Higher values result in increased job priority and faster access to resources.\n",
    "\n",
    "Resource Requirements#\n",
    "Jobs with larger resource requirements may be assigned higher priority, as they require more significant resources to execute efficiently.\n",
    "\n",
    "Walltime Limit#\n",
    "Jobs with shorter estimated execution times may receive higher priority, ensuring they are executed promptly and freeing up resources for other jobs.\n",
    "\n",
    "Balancing Policies#\n",
    "\n",
    "Backfilling#\n",
    "This policy allows smaller jobs to “backfill” into available resources ahead of larger jobs, optimizing resource utilization and reducing wait times.\n",
    "\n",
    "Preemption#\n",
    "Higher-priority jobs can preempt lower-priority ones, temporarily pausing the lower-priority job’s execution to make resources available for the higher-priority job.\n",
    "\n",
    "Best Practices#\n",
    "\n",
    "Set Realistic Priorities: Assign accurate priorities to your jobs to reflect their importance and resource requirements.\n",
    "Use Resource Quotas: Be mindful of the resources you request to prevent over- or underutilization.\n",
    "Leverage Backfilling: Submit smaller, shorter jobs that can backfill into available resources while waiting for larger jobs to start.\n",
    "\n",
    "Understanding these scheduling policies and priorities empowers you to make informed decisions when submitting jobs, ensuring that your computational tasks are executed efficiently and promptly. If you need further guidance on selecting the right scheduling policy for your job or optimizing your resource usage, our support team is available at rchelp@northeastern.edu or consult our Frequently Asked Questions (FAQs).\n",
    "Optimize your job execution by maximizing our cluster’s scheduling capabilities. Happy computing!\n",
    "\n",
    "https://rc-docs.northeastern.edu/en/latest/runningjobs/interactiveandbatch.html\n",
    "\n",
    "Interactive and Batch Mode#\n",
    "In our High-Performance Computing (HPC) environment, users can run jobs in two primary modes: Interactive and Batch. This page provides an in-depth guide to both, assisting users in selecting the appropriate mode for their specific tasks.\n",
    "\n",
    "Interactive Mode#\n",
    "Interactive mode allows users to run jobs that need immediate execution and feedback.\n",
    "\n",
    "Getting Started with Interactive Mode#\n",
    "To launch an interactive session, use the following command:\n",
    "# Request an interactive session\n",
    "srun --pty /bin/bash\n",
    "\n",
    "This command allocates resources and gives you a shell prompt on the allocated node.\n",
    "\n",
    "Interactive Mode Use Cases#\n",
    "\n",
    "Development and Testing: Ideal for code development and testing.\n",
    "Short Tasks: Best for tasks that require less time and immediate results.\n",
    "\n",
    "See also\n",
    "ADD LINK for More Examples and Guides for Interactive Mode\n",
    "\n",
    "Batch Mode#\n",
    "Batch mode enables users to write scripts that manage job execution, making it suitable for more complex or longer-running jobs.\n",
    "\n",
    "Creating Batch Scripts#\n",
    "A typical batch script includes directives for resource allocation, job names, and commands. Here is an example:\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=my_job\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=4\n",
    "#SBATCH --time=01:00:00\n",
    "\n",
    "# Commands to execute\n",
    "module load my_program\n",
    "srun my_program.exe\n",
    "\n",
    "Save this script with a .sh extension, e.g., my_script.sh.\n",
    "\n",
    "Submitting Batch Jobs#\n",
    "You can submit your batch script using the sbatch command.\n",
    "sbatch my_script.sh\n",
    "\n",
    "Monitoring Batch Jobs#\n",
    "You can monitor the status of your batch job using the squeue command.\n",
    "squeue -u username\n",
    "\n",
    "Where username is your actual username.\n",
    "\n",
    "Use Cases#\n",
    "\n",
    "Long-Running Jobs: Suitable for extensive simulations or calculations.\n",
    "Scheduled Tasks: Execute jobs at specific times or under certain conditions.\n",
    "Automated Workflows: Manage complex workflows using multiple scripts.\n",
    "\n",
    "\n",
    "\n",
    "Transfer Data#\n",
    "The HPC has a dedicated transfer node that you must use to transfer data to and from the cluster. You cannot transfer data from any other node or the HPC to your local machine. The node name is <username>@xfer.discovery.neu.edu: where <username> is your Northeastern username to login into the transfer node.\n",
    "You can also transfer files using Globus. This is highly recommended if you need to transfer large amounts of data. See Using Globus for more information.\n",
    "If you are transferring data from different directories on the HPC, you need to use a compute node (see Interactive Jobs: srun Command or Batch Jobs: sbatch) with SCP, rsync, or the copy command to complete these tasks. You should use the --constraint=ib flag (see Hardware Overview) to ensure the fastest data transfer rate.\n",
    "\n",
    "Caution\n",
    "The /scratch space is for temporary file storage only. It is not backed up. If you have directed your output files to /scratch, you should transfer your data from /scratch to another location as soon as possible. See Data Storage Options for more information.\n",
    "\n",
    "Transfer via Terminal#\n",
    "\n",
    "SCP\n",
    "You can use scp to transfer files/directories to and from your local machine and the HPC. As an example, you can use this command to transfer a file to your /scratch space on the HPC from your local machine:\n",
    "scp <filename> <username>@xfer.discovery.neu.edu:/scratch/<username>\n",
    "\n",
    "where <filename> is the name of the file in your current directory you want to transfer, and <username> is your Northeastern username. So that you know, this command is run on your local machine.\n",
    "If you want to transfer a directory in your /scratch called test-data from the HPC to your local machine’s current working directory, an example of that command would be:\n",
    "scp -r <username>@xfer.discovery.neu.edu:/scratch/<username>/test-data .\n",
    "\n",
    "where -r flag is for the recursive transfer because it is a directory. So that you know, this command is run on your local machine.\n",
    "\n",
    "Rsync\n",
    "You can use the rsync command to transfer data to and from the HPC and local machine. You can also use rsync to transfer data from different directories on the cluster.\n",
    "The syntex of rsync is\n",
    "rsync [options] <source> <destination>\n",
    "\n",
    "An example of using rsync to transfer a directory called test-data in your current working directory on your local machine to your /scratch on the HPC is\n",
    "rsync -av test-data/ <username>@xfer.discovery.neu.edu:/scratch/<username>\n",
    "\n",
    "where this command is run on your local machine in the directory that contains test-data.\n",
    "Similarly, rsync can be used to copy from the current working directory on the HPC to your current working directory on your local machine:\n",
    "rsync -av <username>@xfer.discovery.neu.edu:/scratch/<username>/test-data .\n",
    "\n",
    "where this command is run on your local machine in the current directory that you want to save the directory test-data.\n",
    "You can also use rsync to copy data from different directories on the HPC:\n",
    "srun --partition=short --nodes=1 --ntasks=1 --time=01:05:00 --constraint=ib --pty /bin/bash\n",
    "rsync -av /scratch/<username>/source_folder /home/<username>/destination_folder\n",
    "\n",
    "sbatch\n",
    "You can use a sbatch job to complete data transfers by submitting the job to the HPC queue. An example of using rsync through a sbatch script is as follows:\n",
    "#!/bin/bash\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=2\n",
    "#SBATCH --time=0:05:00\n",
    "#SBATCH --job-name=DataTransfer\n",
    "#SBATCH --mem=2G\n",
    "#SBATCH --partition=short\n",
    "#SBATCH --constraint=ib\n",
    "#SBATCH -o %j.out\n",
    "#SBATCH -e %j.err\n",
    "\n",
    "rsync -av /scratch/<username>/source_folder /home/<username>/destination_folder\n",
    "\n",
    "where we are transferring the data from source_folder to the destination_folder.\n",
    "\n",
    "SSHFS\n",
    "If you want to use sshfs, use it with the dedicated transfer node xfer.discovery.neu.edu. It will not work on the login or compute nodes. On a Mac, you will also have to install macFUSE and sshfs (please refer to macFUSE) to use the sshfs command.\n",
    "Use this syntax to perform file transfers with sshfs:\n",
    "sshfs <username>@xfer.discovery.neu.edu:</your/remote/path> <your/local/path> -<options>\n",
    "\n",
    "For example, this will mount a directory in your /scratch named test-data to a local directory on your machine ~/mount_point:\n",
    "sshfs <username>@xfer.discovery.neu.edu:/scratch/<username>/test-data ~/mount_point\n",
    "\n",
    "You can interact with the directory from your GUI or use the terminal to perform tasks on it.\n",
    "\n",
    "Transfer via GUI Application#\n",
    "\n",
    "OOD’s File Explorer\n",
    "You can use OOD’s File Explorer application to transfer data from different directories on the HPC and also to transfer data to and from your local machine to the HPC. For more information to complete this please see OOD File Explorer.\n",
    "\n",
    "MobaXterm\n",
    "You can use MobaXterm to transfer data to and from the HPC. Please check out MobaXterm to download MobaXterm.\n",
    "\n",
    "Open MobaXterm.\n",
    "Click Session, then select SFTP.\n",
    "In the Remote host field, type xfer.discovery.neu.edu\n",
    "In the Username field, type your Northeastern username.\n",
    "In the Port field, type 22.\n",
    "In the Password box, type your Northeastern password and click OK. Click No if prompted to save your password.\n",
    "\n",
    "You will now be connected to the transfer node and can transfer files through MobaXterm. Please refer to MobaXterm for further information.\n",
    "\n",
    "FileZilla\n",
    "You can use FileZilla to transfer data to and from the HPC. Please check out FileZilla to download.\n",
    "\n",
    "Open FileZilla.\n",
    "In the Host field, type sftp://xfer.discovery.neu.edu\n",
    "In the Username field, type your Northeastern username.\n",
    "In the Password field, type your Northeastern password.\n",
    "In the Port field, type 22.\n",
    "\n",
    "You will now be connected to the transfer node and can transfer files through FileZilla. Please refer to FileZilla for further information.\n",
    "\n",
    "https://rc-docs.northeastern.edu/en/latest/datamanagement/globus.html\n",
    "\n",
    "Using Globus#\n",
    "Globus is a data management system that you can use to transfer and share files. Northeastern has a subscription to Globus, and you can set up a Globus account with your Northeastern credentials. You can link your accounts if you have another account, either personal or through another institution.\n",
    "To use Globus, you will need to set up an account, as detailed below. Then, as detailed below, you will need to install Globus Connect to create an endpoint on your local computer. After completing these two initial setup procedures, you can use the Globus web app to perform file transfers. See Using the Northeastern endpoint for a walkthrough of using the Northeastern endpoint on Globus.\n",
    "\n",
    "Globus Account Set Up#\n",
    "You can use the following instructions to set up an account with Globus using your Northeastern credentials.\n",
    "\n",
    "Go to Globus.\n",
    "Click Log In.\n",
    "From the Use your existing organizational login, select Northeastern University, and then click Continue.\n",
    "Enter your Northeastern username and password.\n",
    "If you do not have a previous Globus account, click Continue. If you have a previous account, click the Link to an existing account.\n",
    "Check the agreement checkbox, and then click Continue.\n",
    "Click Allow to permit Globus to access your files.\n",
    "\n",
    "You can then access the Globus File Manager app.\n",
    "\n",
    "Tip\n",
    "If you received an account identity that includes your NUID number (for example, 000123456@northeastern.edu), you can follow the “Creating and linking a new account identity” instructions below to get a different account identity if you want a more user-friendly account identity. You can then link the two accounts together.\n",
    "\n",
    "Creating and linking a new account identity (Optional)#\n",
    "If you created an account through Northeastern University’s existing organizational login and received a username that included your NUID, you can create a new identity with a different username and link the two accounts together. A username you select instead of one with your NUID can make it easier to remember your login credentials.\n",
    "\n",
    "Go to Globus.\n",
    "Click Log In.\n",
    "Click Globus ID to sign in.\n",
    "Click Need a Globus ID? Sign up.\n",
    "Enter your Globus ID information.\n",
    "Enter the verification code that Globus sends to your email.\n",
    "Click Link to an existing account to link this new account with your primary account.\n",
    "Select Northeastern University from the drop-down box and click Continue to be taken to the Northeastern University single sign-on page.\n",
    "Enter your Northeastern username and password.\n",
    "\n",
    "You should now see your two accounts linked in the Account section on the Globus web app.\n",
    "\n",
    "Install Globus Connect Personal (GCP)#\n",
    "Use Globus Connect Personal (GCP) as an endpoint for your laptop. You first need to install GCP using the following procedure and be logged in to Globus before you can install GCP.\n",
    "\n",
    "Go to Globus File Manager.\n",
    "Enter a name for your endpoint in the Endpoint Display Name field.\n",
    "Click Generate Setup Key to generate a setup key for your endpoint.\n",
    "Click the Copy icon next to the generated setup key to copy the key to your clipboard. You will need this key during the installation of GCP in step 6.\n",
    "Click the appropriate OS icon for your computer to download the installation file.\n",
    "After downloading the installation file to your computer, double-click on the file to launch the installer.\n",
    "\n",
    "Accept the defaults on the install wizard. After the installation, you can use your laptop as an endpoint within Globus.\n",
    "\n",
    "Note\n",
    "You cannot modify an endpoint after you have created it. If you need an endpoint with different options, you must delete and recreate it. Follow the instructions on the Globus website for deleting and recreating an endpoint.\n",
    "\n",
    "Working with Globus#\n",
    "After you have an account and set up a personal endpoint using Globus Connect personal, you can perform basic file management tasks using the Globus File Manager interface, such as transferring files, renaming files, and creating new folders. You can also download and use the Globus Command Line Interface (CLI) tool. Globus also has extensive documentation and training files for you to practice with.\n",
    "\n",
    "Using the Northeastern endpoint#\n",
    "To access the Northeastern endpoint on Globus, on the Globus web app, click File Manager, then in the Collection text box, type Northeastern. The endpoints owned by Northeastern University are displayed in the collection area. The general Northeastern endpoint is northeastern#discovery. Using the File Manager interface, you can easily change directories, switch the direction of transferring to and from, and specify options such as transferring only new or changed files. Below is a procedure for transferring files from Discovery to your personal computer, but with the flexibility of the File Manager interface, you can adjust the endpoints, file view, direction of the transfer, and many other options.\n",
    "To transfer files from Discovery to your personal computer, do the following\n",
    "\n",
    "Create an endpoint on your computer using the procedure above “Install Globus Connect,” if you have not done so already.\n",
    "In the File Manager on the Globus web app, in the Collections textbox, type Northeastern, then in the collection list, click the northeastern#discovery endpoint.\n",
    "click Transfer or Sync to in the right-pane menu.\n",
    "Click in the Search text box, and then click the name of your endpoint on the Your Collections tab. You can now see the list of your files on Discovery on the left and on your personal computer on the right.\n",
    "Select the file or files from the right-side list of Discovery files that you want to transfer to your personal computer.\n",
    "Select the destination folder from the left-side list of the files on your computer.\n",
    "(Optional) Click Transfer & Sync Options and select the transfer options you need.\n",
    "Click Start.\n",
    "\n",
    "Connecting to Google Drive#\n",
    "The version of Globus currently on Discovery allows you to connect to Google Drive by first setting up the connection in GCP. This will add your Google Drive to your current personal endpoint.\n",
    "Just so you know, you will first need a personal endpoint, as outlined in the procedure above. This procedure is slightly different from using the Google Drive Connector with\n",
    "Globus version 5.5. You will need your Google Drive downloaded to your local computer.\n",
    "To add Google Drive to your endpoint, do the following\n",
    "\n",
    "Open the GCP app. Right-click the G icon in your taskbar on Windows and select Options. Click the G icon in the menu bar on Mac and select Preferences.\n",
    "On the Access tab, click the + button to open the Choose a directory dialog box.\n",
    "Navigate to your Google Drive on your computer and click Choose.\n",
    "Click the Shareable checkbox to make this a shareable folder in Globus File Manager, and then click Save.\n",
    "\n",
    "You can now go to Globus File Manager and see that your Google Drive is available as a folder on your endpoint.\n",
    "\n",
    "Command Line Interface (CLI)#\n",
    "The Globus Command Line Interface (CLI) tool allows you to access Globus from the command line. It is a stand-alone app that requires a separate download\n",
    "and installation. Please refer to the Globus CLI documentation for working with this app.\n",
    "\n",
    "Globus documentation and test files#\n",
    "Globus provides detailed instructions on using Globus and has test files for you to practice with. These are free for you to access and use. We would like to encourage you to use the test files to become familiar with the Globus interface. You can access the Globus documentation and training files on the Globus How To website.\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e62473d1-107c-4f39-a519-9ac4c6cec48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "encoding_name = tiktoken.get_encoding(\"cl100k_base\")\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b4ff27da-5e0e-498b-91b5-39e178921ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 700,\n",
    "    chunk_overlap  = 70,\n",
    "    length_function = len,\n",
    "    add_start_index = True,\n",
    "    #metadata={\"source\": \"RCDocs\"},\n",
    ")\n",
    "texts = text_splitter.create_documents([data])\n",
    "\n",
    "#text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "#texts = text_splitter.split_documents(text_documents)\n",
    "for idx, text in enumerate(docs):\n",
    "    docs[idx].metadata['source'] = \"RCDocs\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e266f829-1490-44a1-a4dc-d46a87ea3b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe3ee27e-6219-4f83-aafa-5d7ee7a5da50",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = config[\"persist_directory\"]\n",
    "vectordb = Chroma.from_documents(documents=texts, embedding=embedding, persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce193d59-2091-45e3-a547-37547b14862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    PromptTemplate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c43df85-1686-4de9-89e4-286cdcacc912",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_llm = HuggingFacePipeline(pipeline=llm)\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\":4})\n",
    "qa = RetrievalQA.from_chain_type(llm=hf_llm, chain_type=\"stuff\",retriever=retriever)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4be4651-b6a4-457e-8e6c-b6e301704d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining a default prompt for flan models\n",
    "if config[\"llm\"] == LLM_FLAN_T5_SMALL or config[\"llm\"] == LLM_FLAN_T5_BASE or config[\"llm\"] == LLM_FLAN_T5_LARGE or config[\"llm\"] == LLM_FALCON_40b:\n",
    "    question_t5_template = \"\"\"\n",
    "    context: {context}\n",
    "    question: {question}\n",
    "    answer: \n",
    "    \"\"\"\n",
    "    QUESTION_T5_PROMPT = PromptTemplate(\n",
    "        template=question_t5_template, input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    qa.combine_documents_chain.llm_chain.prompt = QUESTION_T5_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d9875352-c548-4d81-9c24-5749ea431b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the steps to install GCP?\"\n",
    "qa.combine_documents_chain.verbose = True\n",
    "qa.return_source_documents = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8678c09-f909-4e71-9983-3b7f467dff49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/rc/projects/chatbot/conda_env/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.01` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/work/rc/projects/chatbot/conda_env/lib/python3.9/site-packages/transformers/generation/utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "The current implementation of Falcon calls `torch.scaled_dot_product_attention` directly, this will be deprecated in the future in favor of the `BetterTransformer` API. Please install the latest optimum library with `pip install -U optimum` and call `model.to_bettertransformer()` to benefit from `torch.scaled_dot_product_attention` and future performance optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Time take :  94.88298988342285\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "results = qa({\"query\":question,})\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Time take : \" , elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "25d3d9c3-ec17-4cc1-96bc-16815db03152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time take :  94.88298988342285\n"
     ]
    }
   ],
   "source": [
    "print(\"Time take : \" , elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0447e523-852a-4587-8ef3-b1c76d053edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['query', 'result', 'source_documents'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "08dd48e3-3f2e-4b57-97f9-c6bc91fae865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- Go to Globus File Manager.\\n    - Enter a name for your endpoint in the Endpoint Display Name field.\\n    - Click Generate Setup Key to generate a setup key for your endpoint.\\n    - Click the Copy icon next to the generated setup key to copy the key to your clipboard. You will need this key during the installation of GCP in step 6.\\n    - Click the appropriate OS icon for your computer to download the installation file.\\n    - After downloading the installation file to your'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0618cf2e-8219-496e-88ba-0b95eff6977d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### result from falcon 40b (time taken 837 sec)\n",
    "- Go to Globus File Manager.\n",
    "- Enter a name for your endpoint in the Endpoint Display Name field.\n",
    "- Click Generate Setup Key to generate a setup key for your endpoint.\n",
    "- Click the Copy icon next to the generated setup key to copy the key to your clipboard. You will need this key during the installation of GCP in step 6.\n",
    "- Click the appropriate OS icon for your computer to download the installation file.\n",
    "- After downloading the installation file to your'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14df70bc-189b-4d0f-8271-8deb39d0e641",
   "metadata": {},
   "source": [
    "### result from falcon 7b (time taken 5 sec)\n",
    "- Go to Globus File Manager.\n",
    "- Click on the G icon in the taskbar.\n",
    "- Click on the G icon in the menu bar.\n",
    "- Click on the + icon in the Choose a directory dialog box.\n",
    "- Navigate to your Google Drive on your computer and click Choose.\n",
    "- Click the Shareable checkbox to make this a shareable folder in Globus File Manager.\n",
    "- Click Save.\n",
    "\n",
    "Now you can go to Globus File Manager and see'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536a2e65-92aa-4b1f-8b4b-4722bb6435ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Install Globus Connect Personal (GCP) https://rc-docs.northeastern.edu/en/latest/datamanagement/globus.html\n",
    "\n",
    "Use Globus Connect Personal (GCP) as an endpoint for your laptop. You first need to install GCP using the following procedure and be logged in to Globus before you can install GCP.\n",
    "\n",
    "- Go to Globus File Manager.\n",
    "- Enter a name for your endpoint in the Endpoint Display Name field.\n",
    "- Click Generate Setup Key to generate a setup key for your endpoint.\n",
    "- Click the Copy icon next to the generated setup key to copy the key to your clipboard. You will need this key during the installation of GCP in step 6.\n",
    "- Click the appropriate OS icon for your computer to download the installation file.\n",
    "- After downloading the installation file to your computer, double-click on the file to launch the installer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffbf64d-4128-4e8f-9530-9cac91ecf56b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Chatbot Environment",
   "language": "python",
   "name": "chatbot_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
