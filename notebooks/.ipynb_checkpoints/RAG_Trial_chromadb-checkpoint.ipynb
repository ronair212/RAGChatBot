{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MNSwbz8svgCi",
    "outputId": "edbdd442-5caa-4c86-b902-da4a5f72f106"
   },
   "outputs": [],
   "source": [
    "#!pip install pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MuNwBfb6BngZ",
    "outputId": "30409fdb-2773-411e-dcfc-170ab7fc63f7"
   },
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nwJCPFhtCBmi",
    "outputId": "aa6b6914-32a8-4f4d-e68c-84e5077d0fad"
   },
   "outputs": [],
   "source": [
    "#!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vwqNT7T0CF-O",
    "outputId": "9fc123df-4163-4f34-f81f-610513bc1fe5"
   },
   "outputs": [],
   "source": [
    "#!pip install cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QQl_Bfl8CLL1",
    "outputId": "cc07de5a-2e7c-4b28-a66d-5906fea3a052"
   },
   "outputs": [],
   "source": [
    "#!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3oMYj3f-vmG7"
   },
   "outputs": [],
   "source": [
    "#import pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "njgYMfbQvnjn"
   },
   "outputs": [],
   "source": [
    "#pinecone.init(api_key=\"b360318b-4fc8-4580-bf6c-d88959179985\",\n",
    "#              environment=\"us-west1-gcp-free\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "drKj9gcYvx36"
   },
   "outputs": [],
   "source": [
    "#pinecone.whoami()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "B979WTowvyGz"
   },
   "outputs": [],
   "source": [
    "#pinecone.list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "71U2rh7xwQIy"
   },
   "outputs": [],
   "source": [
    "#pinecone.list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Cohere API Key: ········································\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nCohereAPIError: You are using a Trial key, which is limited to 2 API calls / minute.\\nYou can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at\\n'https://dashboard.cohere.ai/api-keys'. \\nContact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "CohereAPIError: You are using a Trial key, which is limited to 2 API calls / minute.\n",
    "You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at\n",
    "'https://dashboard.cohere.ai/api-keys'. \n",
    "Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OPENAI API Key: ···················································\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OPENAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "mq7rbJww6Cjo"
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "\n",
    "urls = [\"https://rc-docs.northeastern.edu/en/latest/runningjobs/understandingqueuing.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/interactiveandbatch.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/workingwithgpus.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/recurringjobs.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/debuggingjobs.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/../datamanagement/index.html\",\n",
    "]\n",
    "loader = WebBaseLoader(urls)\n",
    "data = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6twYZJo26GmB"
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "encoding_name = tiktoken.get_encoding(\"cl100k_base\")\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "9b3T5m1YBVrf",
    "outputId": "4f06dc21-756e-4dd9-f462-c503f4dd675b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\n\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    chunk_size = 700,\\n    chunk_overlap  = 70,\\n    length_function = len,\\n    add_start_index = True,\\n)\\ndocs = text_splitter.create_documents([data])\\n\\nfor idx, text in enumerate(docs):\\n    docs[idx].metadata[\\'source\\'] = \"RCDocs\"\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "text_splitter = TokenTextSplitter(chunk_size=500, chunk_overlap=25)\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "'''\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 700,\n",
    "    chunk_overlap  = 70,\n",
    "    length_function = len,\n",
    "    add_start_index = True,\n",
    ")\n",
    "docs = text_splitter.create_documents([data])\n",
    "\n",
    "for idx, text in enumerate(docs):\n",
    "    docs[idx].metadata['source'] = \"RCDocs\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_DfcNy9sT3jj",
    "outputId": "36618e1f-e92c-4ed3-f051-980b780bc57b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain.schema.document.Document"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PILXy2pdVC4v",
    "outputId": "99fdfd5c-a9f0-422a-dded-c4c33e642735"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='\\n\\n\\n\\n\\n\\n\\nUnderstanding the Queuing System - RC RTD\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nContents\\n\\n\\n\\n\\n\\nMenu\\n\\n\\n\\n\\n\\n\\n\\nExpand\\n\\n\\n\\n\\n\\nLight mode\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDark mode\\n\\n\\n\\n\\n\\n\\nAuto light/dark mode\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHide navigation sidebar\\n\\n\\nHide table of contents sidebar\\n\\n\\n\\n\\n\\nToggle site navigation sidebar\\n\\n\\n\\n\\nRC RTD\\n\\n\\n\\n\\nToggle Light / Dark / Auto color theme\\n\\n\\n\\n\\n\\n\\nToggle table of contents sidebar\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nResearch ComputingToggle child pages in navigation\\nWelcome\\nServices We Provide\\nGetting Help\\nIntroduction to HPC and Slurm\\nCase Studies and User Testimonials\\n\\n\\n\\n\\nGetting StartedToggle child pages in navigation\\nGetting Access\\nAccount Manager\\nConnecting To ClusterToggle child pages in navigation\\nMac\\nWindows\\n\\n\\n\\n\\nFirst StepsToggle child pages in navigation\\nPasswordless SSH\\nShell Environment on the Cluster\\nCluster via Command-Line\\n\\n\\n\\nUser Guides\\n\\nHardwareToggle child pages in navigation\\nOverview\\nPartitions\\n\\n\\nOpen OnDemand (OOD)Toggle child pages in navigation\\nIntroduction to OOD\\nAccessing Open OnDemand\\nInteractive Open OnDemand ApplicationsToggle child pages in navigation\\nDesktop App\\nOOD File Explorer\\nJupyterLab\\nStata\\n\\n\\n\\n\\nRunning JobsToggle child pages in navigation\\nUnderstanding the Queuing System\\nJob Scheduling Policies and Priorities\\nInteractive and Batch Mode\\nWorking with GPUs\\nRecurring Jobs\\nDebugging and Troubleshooting Jobs\\n\\n\\nData ManagementToggle child pages in navigation\\nData Storage Options\\nTransfer Data\\nUsing Globus\\nData Backup and Restore\\nSecurity and Compliance\\n\\n\\nSoftwareToggle child pages in navigation\\nSystem WideToggle child pages in navigation\\nModules\\nMPI\\nR\\nMatlab\\n\\n\\nPackage ManagersToggle child pages in navigation\\nConda\\nSpack\\n\\n\\nFrom SourceToggle child pages in navigation\\nMake\\nCMake\\n\\n\\n\\n\\nSlurmToggle child pages in navigation\\nIntroduction to Slurm\\nSlurm Commands\\nSlurm Running Jobs\\nMonitoring and Managing Jobs\\nSlurm Job Scripts\\nSlurm Array Jobs and Dependencies\\nSlurm Best Practices\\n\\n\\nHPC for the ClassroomT', metadata={'source': 'https://rc-docs.northeastern.edu/en/latest/runningjobs/understandingqueuing.html', 'title': 'Understanding the Queuing System - RC RTD', 'language': 'en'})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "kh1JnImJ6Mt2"
   },
   "outputs": [],
   "source": [
    "#from langchain.vectorstores import Pinecone\n",
    "#import pinecone\n",
    "from langchain.embeddings import CohereEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "yZiQ1hc_6PYl"
   },
   "outputs": [],
   "source": [
    "embeddings = CohereEmbeddings(model='embed-english-light-v2.0',cohere_api_key=os.environ.get(\"COHERE_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bumy27EP6Q7F",
    "outputId": "19048b6b-2076-4af7-b351-78053cf34a9c"
   },
   "outputs": [],
   "source": [
    "#pinecone.init(\n",
    "#\tapi_key='b360318b-4fc8-4580-bf6c-d88959179985',\n",
    "#\tenvironment='us-west1-gcp-free'\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "8U4W4qHt_m79"
   },
   "outputs": [],
   "source": [
    "#pinecone.delete_index(\"chatbot1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "XYqGH05t_nXW"
   },
   "outputs": [],
   "source": [
    "#pinecone.create_index(\"chatbot1\", dimension=1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "yHrQSGHpWSMT"
   },
   "outputs": [],
   "source": [
    "#index = pinecone.Index('chatbot1')\n",
    "\n",
    "#index_name = \"chatbot1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "tLU7yDoS6Stw"
   },
   "outputs": [],
   "source": [
    "#docsearch = Pinecone.from_documents(docs, embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "5oEHdLS1mfma"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#You can configure Chroma to save and load from your local machine. Data will be persisted automatically and loaded on start (if it exists).\\n\\nimport chromadb\\n\\nfrom chromadb.config import Settings\\n\\n\\nclient = chromadb.Client(Settings(chroma_db_impl=\"duckdb+parquet\", \\n                                    persist_directory=\"db/\"\\n                                ))\\n#DuckDB on the backend\\n#stored in db folder\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#You can configure Chroma to save and load from your local machine. Data will be persisted automatically and loaded on start (if it exists).\n",
    "\n",
    "import chromadb\n",
    "\n",
    "from chromadb.config import Settings\n",
    "\n",
    "\n",
    "client = chromadb.Client(Settings(chroma_db_impl=\"duckdb+parquet\", \n",
    "                                    persist_directory=\"db/\"\n",
    "                                ))\n",
    "#DuckDB on the backend\n",
    "#stored in db folder\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "39b7bDIInas5"
   },
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(docs, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "CF8AHNQm6Ucf"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "#from langchain.vectorstores import Pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "aeEbBgCS6dHH"
   },
   "outputs": [],
   "source": [
    "# load index\n",
    "#docsearch = Pinecone.from_existing_index(index_name, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "zp19GFSp6ed2",
    "outputId": "b8cf41d0-4243-43bc-a35d-08a2eb34aba8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nVectorStoreRetriever\\n\\nReturn VectorStoreRetriever initialized from this VectorStore.\\n\\nArgs:\\n    search_type (Optional[str]): Defines the type of search that\\n        the Retriever should perform.\\nCan be \"similarity\" (default), \"mmr\", or\\n\"similarity_score_threshold\".\\n    search_kwargs (Optional[Dict]): Keyword arguments to pass to the\\n        search function. Can include things like:\\n            k: Amount of documents to return (Default: 4)\\n            score_threshold: Minimum relevance threshold\\n                for similarity_score_threshold\\n            fetch_k: Amount of documents to pass to MMR algorithm (Default: 20)\\n            lambda_mult: Diversity of results returned by MMR;\\n                1 for minimum diversity and 0 for maximum. (Default: 0.5)\\n            filter: Filter by document metadata\\n\\nReturns:\\n    VectorStoreRetriever: Retriever class for VectorStore.\\n\\nExamples:\\n\\n# Retrieve more documents with higher diversity\\n# Useful if your dataset has many similar documents\\ndocsearch.as_retriever(\\n    search_type=\"mmr\",\\n    search_kwargs={\\'k\\': 6, \\'lambda_mult\\': 0.25}\\n)\\n\\n# Fetch more documents for the MMR algorithm to consider\\n# But only return the top 5\\ndocsearch.as_retriever(\\n    search_type=\"mmr\",\\n    search_kwargs={\\'k\\': 5, \\'fetch_k\\': 50}\\n)\\n\\n# Only retrieve documents that have a relevance score\\n# Above a certain threshold\\ndocsearch.as_retriever(\\n    search_type=\"similarity_score_threshold\",\\n    search_kwargs={\\'score_threshold\\': 0.8}\\n)\\n\\n# Only get the single most similar document from the dataset\\ndocsearch.as_retriever(search_kwargs={\\'k\\': 1})\\n\\n# Use a filter to only retrieve documents from a specific paper\\ndocsearch.as_retriever(\\n    search_kwargs={\\'filter\\': {\\'paper_title\\':\\'GPT-4 Technical Report\\'}}\\n)\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize base retriever\n",
    "#retriever = docsearch.as_retriever(search_kwargs={\"k\": 4})\n",
    "#retriever = Chroma.as_retriever(search_kwargs={\"k\": 4})\n",
    "#retriever = Chroma.as_retriever(\n",
    "#    search_type=\"mmr\",\n",
    "#    search_kwargs={'k': 4, 'fetch_k': 50} )\n",
    "\n",
    "from langchain.schema.vectorstore import VectorStoreRetriever\n",
    "retriever = VectorStoreRetriever(vectorstore=db, search_type=\"mmr\", search_kwargs={'k': 4, 'fetch_k': 10},)\n",
    "\n",
    "'''\n",
    "\n",
    "VectorStoreRetriever\n",
    "\n",
    "Return VectorStoreRetriever initialized from this VectorStore.\n",
    "\n",
    "Args:\n",
    "    search_type (Optional[str]): Defines the type of search that\n",
    "        the Retriever should perform.\n",
    "Can be \"similarity\" (default), \"mmr\", or\n",
    "\"similarity_score_threshold\".\n",
    "    search_kwargs (Optional[Dict]): Keyword arguments to pass to the\n",
    "        search function. Can include things like:\n",
    "            k: Amount of documents to return (Default: 4)\n",
    "            score_threshold: Minimum relevance threshold\n",
    "                for similarity_score_threshold\n",
    "            fetch_k: Amount of documents to pass to MMR algorithm (Default: 20)\n",
    "            lambda_mult: Diversity of results returned by MMR;\n",
    "                1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
    "            filter: Filter by document metadata\n",
    "\n",
    "Returns:\n",
    "    VectorStoreRetriever: Retriever class for VectorStore.\n",
    "\n",
    "Examples:\n",
    "\n",
    "# Retrieve more documents with higher diversity\n",
    "# Useful if your dataset has many similar documents\n",
    "docsearch.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={'k': 6, 'lambda_mult': 0.25}\n",
    ")\n",
    "\n",
    "# Fetch more documents for the MMR algorithm to consider\n",
    "# But only return the top 5\n",
    "docsearch.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={'k': 5, 'fetch_k': 50}\n",
    ")\n",
    "\n",
    "# Only retrieve documents that have a relevance score\n",
    "# Above a certain threshold\n",
    "docsearch.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={'score_threshold': 0.8}\n",
    ")\n",
    "\n",
    "# Only get the single most similar document from the dataset\n",
    "docsearch.as_retriever(search_kwargs={'k': 1})\n",
    "\n",
    "# Use a filter to only retrieve documents from a specific paper\n",
    "docsearch.as_retriever(\n",
    "    search_kwargs={'filter': {'paper_title':'GPT-4 Technical Report'}}\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "LcR414tm7UAd"
   },
   "outputs": [],
   "source": [
    "compressor = CohereRerank()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "dmf4wxNb7Xr0"
   },
   "outputs": [],
   "source": [
    "# Set up cohere's reranker\n",
    "reranker = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "E6L9I9sU7Yy7"
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "EQuyfiGR7aO5"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "lUJilunXIox7"
   },
   "outputs": [],
   "source": [
    "#from langchain.callbacks import ContextCallbackHandler\n",
    "#from langchain.callbacks import FlyteCallbackHandler\n",
    "from langchain.callbacks import StdOutCallbackHandler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "M18gm_YBHKE8"
   },
   "outputs": [],
   "source": [
    "#context_callback = ContextCallbackHandler(token=\"T1gM1n4RzGWLFSsJnQ5ziLUW\")\n",
    "#context_callback = FlyteCallbackHandler()\n",
    "context_callback = StdOutCallbackHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "-BFTmJkK-na4"
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.7, verbose=True, openai_api_key = os.environ.get(\"OPENAI_API_KEY\"), streaming=True, callbacks=[context_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "4_aSlIy6F1e8"
   },
   "outputs": [],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=llm,memory_key=\"chat_history\", return_messages=True,input_key='question',max_token_limit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "BAbvsetCTEIL"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "CONDENSE_QUESTION_PROMPT = '''\n",
    "Below is a summary of the conversation so far, and a new question asked by the user that needs to be answered by searching in a knowledge base.\n",
    "Generate a search query based on the conversation and the new question.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Search query:\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "PromptTemplates = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"question\"],\n",
    "    template=\"\"\"\n",
    "Below is a summary of the conversation so far, and a new question asked by the user that needs to be answered by searching in a knowledge base.\n",
    "Generate a search query based on the conversation and the new question.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Search query:\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "system_message_prompt = SystemMessagePromptTemplate(prompt=PromptTemplates)\n",
    "\n",
    "chat_prompt_for_ques = ChatPromptTemplate.from_messages(\n",
    "    [system_message_prompt])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "wJiRbW27TaZp"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "WiKNdmX3S1Zj"
   },
   "outputs": [],
   "source": [
    "question_generator = LLMChain(llm=llm, prompt=chat_prompt_for_ques, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "CzcMctULXMY9"
   },
   "outputs": [],
   "source": [
    "Answer_Generator_Prompt= '''\n",
    "<Instructions>\n",
    "Important:\n",
    "Answer with the facts listed in the list of sources below. If there isn't enough information below, say you don't know.\n",
    "If asking a clarifying question to the user would help, ask the question.\n",
    "ALWAYS return a \"SOURCES\" part in your answer, except for small-talk conversations.\n",
    "\n",
    "Question: {question}\n",
    "Sources:\n",
    "---------------------\n",
    "    {summaries}\n",
    "---------------------\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "C2yZpFpRVJPD"
   },
   "outputs": [],
   "source": [
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "\n",
    "chat_prompt = PromptTemplate(template=Answer_Generator_Prompt, input_variables=[\"question\", \"summaries\",\"chat_history\"])\n",
    "\n",
    "answer_chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\", verbose=True,prompt=chat_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "WgdFwotlZtyJ"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "\n",
    "chain = ConversationalRetrievalChain(\n",
    "            retriever=reranker,\n",
    "            question_generator=question_generator,\n",
    "            combine_docs_chain=answer_chain,\n",
    "            verbose=True,\n",
    "            memory=memory,\n",
    "            rephrase_question=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ClKEHA56ZxqI",
    "outputId": "d6d1b3c3-fa3d-4e9b-aef7-1894e9d2f630"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "<Instructions>\n",
      "Important:\n",
      "Answer with the facts listed in the list of sources below. If there isn't enough information below, say you don't know.\n",
      "If asking a clarifying question to the user would help, ask the question.\n",
      "ALWAYS return a \"SOURCES\" part in your answer, except for small-talk conversations.\n",
      "\n",
      "Question: What is the Scheduling Policies for HPC cluster?\n",
      "Sources:\n",
      "---------------------\n",
      "    Content:  Job Scripts\n",
      "Slurm Array Jobs and Dependencies\n",
      "Slurm Best Practices\n",
      "\n",
      "\n",
      "HPC for the ClassroomToggle child pages in navigation\n",
      "Classroom HPC: FAQ\n",
      "CPS Class Instructions\n",
      "\n",
      "\n",
      "Best PracticesToggle child pages in navigation\n",
      "Home Directory Storage Quota\n",
      "Checkpointing Jobs\n",
      "Optimizing Job Performance\n",
      "Best SW Practices\n",
      "\n",
      "\n",
      "Tutorials and TrainingToggle child pages in navigation\n",
      "Canvas and GitHub\n",
      "\n",
      "\n",
      "Frequently Asked Questions\n",
      "Glossary\n",
      "\n",
      "Contribution\n",
      "\n",
      "Change Log\n",
      "Report Docs Bug or Request\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "    v: latest\n",
      "  \n",
      "\n",
      "\n",
      "Versions\n",
      "latest\n",
      "2.0.0\n",
      "1.2.0\n",
      "v1.1.0\n",
      "\n",
      "\n",
      "Downloads\n",
      "\n",
      "\n",
      "On Read the Docs\n",
      "\n",
      "Project Home\n",
      "\n",
      "\n",
      "Builds\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Back to top\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Edit this page\n",
      "\n",
      "\n",
      "\n",
      "Toggle Light / Dark / Auto color theme\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Toggle table of contents sidebar\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Job Scheduling Policies and Priorities#\n",
      "In an HPC environment, efficient job scheduling is crucial for allocating computing resources and ensuring optimal cluster utilization. Job scheduling policies and priorities determine the order in which jobs are executed and the resources they receive. Understanding these policies is essential for maximizing job efficiency and minimizing wait times.\n",
      "\n",
      "Scheduling Policies#\n",
      "\n",
      "FIFO (First-In-First-Out)#\n",
      "Jobs are executed in the order they are submitted. Although simple, this policy may lead to long wait times for large, resource-intensive jobs if smaller jobs are constantly being submitted.\n",
      "\n",
      "\n",
      "Fair Share#\n",
      "This policy ensures that all users receive a fair share of cluster resources over time. Users with high resource usage may experience reduced priority, allowing others to access resources more regularly.\n",
      "\n",
      "\n",
      "Priority-Based#\n",
      "Jobs are assigned priorities based on user-defined criteria or system-wide rules. Higher-priority jobs are executed before lower-priority ones, allowing for resource allocation based on user requirements.\n",
      "\n",
      "\n",
      "\n",
      "Job Priorities#\n",
      "\n",
      "User Priority#\n",
      "Users can assign priority values to their jobs. Higher values result in increased job priority and faster access to resources.\n",
      "\n",
      "\n",
      "Resource Requirements#\n",
      "Jobs with larger resource requirements may be assigned higher priority, as they require more significant resources to execute efficiently.\n",
      "\n",
      "\n",
      "Walltime Limit#\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\n",
      "\n",
      "Content:  larger resource requirements may be assigned higher priority, as they require more significant resources to execute efficiently.\n",
      "\n",
      "\n",
      "Walltime Limit#\n",
      "Jobs with shorter estimated execution times may receive higher priority, ensuring they are executed promptly and freeing up resources for other jobs.\n",
      "\n",
      "\n",
      "\n",
      "Balancing Policies#\n",
      "\n",
      "Backfilling#\n",
      "This policy allows smaller jobs to “backfill” into available resources ahead of larger jobs, optimizing resource utilization and reducing wait times.\n",
      "\n",
      "\n",
      "Preemption#\n",
      "Higher-priority jobs can preempt lower-priority ones, temporarily pausing the lower-priority job’s execution to make resources available for the higher-priority job.\n",
      "\n",
      "\n",
      "\n",
      "Best Practices#\n",
      "\n",
      "Set Realistic Priorities: Assign accurate priorities to your jobs to reflect their importance and resource requirements.\n",
      "Use Resource Quotas: Be mindful of the resources you request to prevent over- or underutilization.\n",
      "Leverage Backfilling: Submit smaller, shorter jobs that can backfill into available resources while waiting for larger jobs to start.\n",
      "\n",
      "Understanding these scheduling policies and priorities empowers you to make informed decisions when submitting jobs, ensuring that your computational tasks are executed efficiently and promptly. If you need further guidance on selecting the right scheduling policy for your job or optimizing your resource usage, our support team is available at rchelp@northeastern.edu or consult our Frequently Asked Questions (FAQs).\n",
      "Optimize your job execution by maximizing our cluster’s scheduling capabilities. Happy computing!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Next\n",
      "\n",
      "Interactive and Batch Mode\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Previous\n",
      "\n",
      "Understanding the Queuing System\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                Copyright © 2023, RC\n",
      "            \n",
      "            Made with \n",
      "            Furo\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            On this page\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Job Scheduling Policies and Priorities\n",
      "Scheduling Policies\n",
      "FIFO (First-In-First-Out)\n",
      "Fair Share\n",
      "Priority-Based\n",
      "\n",
      "\n",
      "Job Priorities\n",
      "User Priority\n",
      "Resource Requirements\n",
      "Walltime Limit\n",
      "\n",
      "\n",
      "Balancing Policies\n",
      "Backfilling\n",
      "Preemption\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\n",
      "\n",
      "Content:  allow others to use the system. Conversely, users or groups that have used fewer resources will have their jobs prioritized.\n",
      "The following policies ensure fair use of the cluster resources:\n",
      "\n",
      "Single job size: The maximum number of nodes a single job depends on the partition (see Partitions).\n",
      "Run time limit: The maximum run time for a job depends on the partition (see Partitions).\n",
      "Priority decay: If a job remains in the queue without running for an extended period, its priority may slowly decrease.\n",
      "\n",
      "\n",
      "\n",
      "Job Priority**#\n",
      "Several factors determine job priority:\n",
      "\n",
      "Fair-share: This is based on the historical resource usage of your group. The more resources your group has used, the lower your job’s priority becomes, and vice versa.\n",
      "Job size: Smaller jobs (regarding requested nodes) typically have higher priority.\n",
      "Queue wait time: The longer a job has been in the queue, the higher its priority becomes.\n",
      "\n",
      "\n",
      "\n",
      "Job States#\n",
      "Each job in the queue has a state. The main job states are:\n",
      "\n",
      "Pending (PD): The job is waiting for resources to become available.\n",
      "Running (R): The job is currently running.\n",
      "Completed (CG): The job has been completed successfully.\n",
      "\n",
      "A complete list of job states can be found in the Slurm documentation.\n",
      "\n",
      "\n",
      "Monitoring the Queue**#\n",
      "You can use the following commands to interact with the queue:\n",
      "\n",
      "squeue: Displays the state of jobs or job steps. It has a wide variety of filtering, sorting, and formatting options. For example, to display your jobs:\n",
      "\n",
      "squeue -u your_username\n",
      "\n",
      "\n",
      "\n",
      "scontrol: Used to view and modify Slurm configuration and state. For example, to show the details of a specific job:\n",
      "\n",
      "scontrol show job your_job_id\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tips for Efficient Queue Usage**#\n",
      "\n",
      "Request only the resources you need: Overestimating your job’s requirements can result in longer queue times.\n",
      "Break up large jobs: Large jobs tend to wait in the queue longer than small jobs. Break up large jobs into smaller ones.\n",
      "Use idle resources: Sometimes, idle resources can be used. If your job is flexible regarding start time and duration, you can use the --begin and --time options to take advantage of these idle resources.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/runningjobs/understandingqueuing.html\n",
      "---------------------\n",
      "\n",
      "Chat History:\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Question from user :  What is the Scheduling Policies for HPC cluster? \n",
      "\n",
      "Reply from ChatBot :  The scheduling policies for an HPC cluster include FIFO (First-In-First-Out), Fair Share, and Priority-Based. In the FIFO policy, jobs are executed in the order they are submitted, which may result in long wait times for resource-intensive jobs. The Fair Share policy ensures that all users receive a fair share of cluster resources over time, with users who have high resource usage experiencing reduced priority. The Priority-Based policy assigns priorities to jobs based on user-defined criteria or system-wide rules, with higher-priority jobs being executed before lower-priority ones. \n",
      "\n",
      "Job priorities in an HPC cluster are determined by factors such as user priority, resource requirements, and walltime limit. Users can assign priority values to their jobs, with higher values resulting in increased job priority and faster access to resources. Jobs with larger resource requirements may be assigned higher priority, as they require more significant resources to execute efficiently. Jobs with shorter estimated execution times may also receive higher priority, ensuring prompt execution and freeing up resources for other jobs.\n",
      "\n",
      "Balancing policies in an HPC cluster include backfilling and preemption. Backfilling allows smaller jobs to fill in available resources ahead of larger jobs, optimizing resource utilization and reducing wait times. Preemption allows higher-priority jobs to temporarily pause the execution of lower-priority jobs, making resources available for the higher-priority jobs.\n",
      "\n",
      "Some best practices for job scheduling in an HPC cluster include setting realistic priorities, using resource quotas to prevent over- or underutilization, and leveraging backfilling by submitting smaller jobs that can fill in available resources while waiting for larger jobs to start.\n",
      "\n",
      "Sources:\n",
      "- https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\n",
      "- https://rc-docs.northeastern.edu/en/latest/runningjobs/understandingqueuing.html\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the Scheduling Policies for HPC cluster?\"\n",
    "result = chain({\"question\": query})\n",
    "\n",
    "\n",
    "print(\"Question from user : \" , query ,\"\\n\")\n",
    "print(\"Reply from ChatBot : \" , result['answer'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gLLL9c7tbgW1",
    "outputId": "766a6bf0-92f3-47ca-d56e-30cb719d395f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "Below is a summary of the conversation so far, and a new question asked by the user that needs to be answered by searching in a knowledge base.\n",
      "Generate a search query based on the conversation and the new question.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: What is the Scheduling Policies for HPC cluster?\n",
      "Assistant: The scheduling policies for an HPC cluster include FIFO (First-In-First-Out), Fair Share, and Priority-Based. In the FIFO policy, jobs are executed in the order they are submitted, which may result in long wait times for resource-intensive jobs. The Fair Share policy ensures that all users receive a fair share of cluster resources over time, with users who have high resource usage experiencing reduced priority. The Priority-Based policy assigns priorities to jobs based on user-defined criteria or system-wide rules, with higher-priority jobs being executed before lower-priority ones. \n",
      "\n",
      "Job priorities in an HPC cluster are determined by factors such as user priority, resource requirements, and walltime limit. Users can assign priority values to their jobs, with higher values resulting in increased job priority and faster access to resources. Jobs with larger resource requirements may be assigned higher priority, as they require more significant resources to execute efficiently. Jobs with shorter estimated execution times may also receive higher priority, ensuring prompt execution and freeing up resources for other jobs.\n",
      "\n",
      "Balancing policies in an HPC cluster include backfilling and preemption. Backfilling allows smaller jobs to fill in available resources ahead of larger jobs, optimizing resource utilization and reducing wait times. Preemption allows higher-priority jobs to temporarily pause the execution of lower-priority jobs, making resources available for the higher-priority jobs.\n",
      "\n",
      "Some best practices for job scheduling in an HPC cluster include setting realistic priorities, using resource quotas to prevent over- or underutilization, and leveraging backfilling by submitting smaller jobs that can fill in available resources while waiting for larger jobs to start.\n",
      "\n",
      "Sources:\n",
      "- https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\n",
      "- https://rc-docs.northeastern.edu/en/latest/runningjobs/understandingqueuing.html\n",
      "Human: How do I check Job Status?\n",
      "Assistant: SOURCES:\n",
      "- Source 1: https://rc-docs.northeastern.edu/en/latest/runningjobs/understandingqueuing.html\n",
      "- Source 2: https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\n",
      "\n",
      "Question:\n",
      "How do I check Job Status?\n",
      "\n",
      "Search query:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "<Instructions>\n",
      "Important:\n",
      "Answer with the facts listed in the list of sources below. If there isn't enough information below, say you don't know.\n",
      "If asking a clarifying question to the user would help, ask the question.\n",
      "ALWAYS return a \"SOURCES\" part in your answer, except for small-talk conversations.\n",
      "\n",
      "Question: How do I check Job Status?\n",
      "Sources:\n",
      "---------------------\n",
      "    Content: SBATCH --job-name=my_job\n",
      "#SBATCH --nodes=1\n",
      "#SBATCH --ntasks=4\n",
      "#SBATCH --time=01:00:00\n",
      "\n",
      "# Commands to execute\n",
      "module load my_program\n",
      "srun my_program.exe\n",
      "\n",
      "\n",
      "Save this script with a .sh extension, e.g., my_script.sh.\n",
      "\n",
      "\n",
      "Submitting Batch Jobs#\n",
      "You can submit your batch script using the sbatch command.\n",
      "sbatch my_script.sh\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Monitoring Batch Jobs#\n",
      "You can monitor the status of your batch job using the squeue command.\n",
      "squeue -u username\n",
      "\n",
      "\n",
      "Where username is your actual username.\n",
      "\n",
      "\n",
      "Use Cases#\n",
      "\n",
      "Long-Running Jobs: Suitable for extensive simulations or calculations.\n",
      "Scheduled Tasks: Execute jobs at specific times or under certain conditions.\n",
      "Automated Workflows: Manage complex workflows using multiple scripts.\n",
      "\n",
      "\n",
      "See also\n",
      "ADD LINK for More Examples and Guides for Batch Mode\n",
      "\n",
      "Interactive and Batch modes cater to different needs and scenarios in the HPC environment. You can explore both modes to choose the one that best aligns with your tasks. For more detailed guides and support, please consult the above guides or contact our support team at rchelp@northeastern.edu.\n",
      "Happy computing!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Next\n",
      "\n",
      "Working with GPUs\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Previous\n",
      "\n",
      "Job Scheduling Policies and Priorities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                Copyright © 2023, RC\n",
      "            \n",
      "            Made with \n",
      "            Furo\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            On this page\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Interactive and Batch Mode\n",
      "Interactive Mode\n",
      "Getting Started with Interactive Mode\n",
      "Interactive Mode Use Cases\n",
      "\n",
      "\n",
      "Batch Mode\n",
      "Creating Batch Scripts\n",
      "Submitting Batch Jobs\n",
      "Monitoring Batch Jobs\n",
      "Use Cases\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/runningjobs/interactiveandbatch.html\n",
      "\n",
      "Content:  allow others to use the system. Conversely, users or groups that have used fewer resources will have their jobs prioritized.\n",
      "The following policies ensure fair use of the cluster resources:\n",
      "\n",
      "Single job size: The maximum number of nodes a single job depends on the partition (see Partitions).\n",
      "Run time limit: The maximum run time for a job depends on the partition (see Partitions).\n",
      "Priority decay: If a job remains in the queue without running for an extended period, its priority may slowly decrease.\n",
      "\n",
      "\n",
      "\n",
      "Job Priority**#\n",
      "Several factors determine job priority:\n",
      "\n",
      "Fair-share: This is based on the historical resource usage of your group. The more resources your group has used, the lower your job’s priority becomes, and vice versa.\n",
      "Job size: Smaller jobs (regarding requested nodes) typically have higher priority.\n",
      "Queue wait time: The longer a job has been in the queue, the higher its priority becomes.\n",
      "\n",
      "\n",
      "\n",
      "Job States#\n",
      "Each job in the queue has a state. The main job states are:\n",
      "\n",
      "Pending (PD): The job is waiting for resources to become available.\n",
      "Running (R): The job is currently running.\n",
      "Completed (CG): The job has been completed successfully.\n",
      "\n",
      "A complete list of job states can be found in the Slurm documentation.\n",
      "\n",
      "\n",
      "Monitoring the Queue**#\n",
      "You can use the following commands to interact with the queue:\n",
      "\n",
      "squeue: Displays the state of jobs or job steps. It has a wide variety of filtering, sorting, and formatting options. For example, to display your jobs:\n",
      "\n",
      "squeue -u your_username\n",
      "\n",
      "\n",
      "\n",
      "scontrol: Used to view and modify Slurm configuration and state. For example, to show the details of a specific job:\n",
      "\n",
      "scontrol show job your_job_id\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tips for Efficient Queue Usage**#\n",
      "\n",
      "Request only the resources you need: Overestimating your job’s requirements can result in longer queue times.\n",
      "Break up large jobs: Large jobs tend to wait in the queue longer than small jobs. Break up large jobs into smaller ones.\n",
      "Use idle resources: Sometimes, idle resources can be used. If your job is flexible regarding start time and duration, you can use the --begin and --time options to take advantage of these idle resources.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/runningjobs/understandingqueuing.html\n",
      "\n",
      "Content:  the state and should be used carefully.\n",
      "\n",
      "\n",
      "Requesting GPUs with Slurm#\n",
      "Use srun for interactive and sbatch for batch mode. The srun example below is requesting 1 node and 1 GPU with 4GB of memory in the gpu partition. You must use the --gres= option to request a gpu:\n",
      "srun --partition=gpu --nodes=1 --pty --gres=gpu:1 --ntasks=1 --mem=4GB --time=01:00:00 /bin/bash\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "On the gpu partition, requesting more than 1 GPU (--gres=gpu:1) will cause your request to fail. Additionally, one cannot request all the CPUs on that gpu node as they are reserved for other GPUs.\n",
      "\n",
      "The sbatch example below is similar to the srun example above, but it submits the job in the background, gives it a name, and directs the output to a file:\n",
      "#!/bin/bash\n",
      "#SBATCH --partition=gpu\n",
      "#SBATCH --nodes=1\n",
      "#SBATCH --gres=gpu:1\n",
      "#SBATCH --time=01:00:00\n",
      "#SBATCH --job-name=gpu_run\n",
      "#SBATCH --mem=4GB\n",
      "#SBATCH --ntasks=1\n",
      "#SBATCH --output=myjob.%j.out\n",
      "#SBATCH --error=myjob.%j.err\n",
      "\n",
      "## <your code>\n",
      "\n",
      "\n",
      "\n",
      "Specifying a GPU type#\n",
      "You can add a specific type of GPU to the --gres= option (with either srun or sbatch). For a list of available GPU types, refer to the GPU Types column in the table, at the top of this page, that are listed as Public.\n",
      "\n",
      "Command to request one p100 GPU.#\n",
      "--gres=gpu:p100:1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "Requesting a specific type of GPU could result in longer wait times, based on GPU availability at that time.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Using CUDA#\n",
      "There are several versions of CUDA Toolkits available on the HPC, including. Use the module avail command to check for the latest software versions on the cluster.\n",
      "$ module avail cuda\n",
      "\n",
      "------------------------------- /shared/centos7/modulefiles -------------------------------\n",
      "cuda/10.0    cuda/\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/runningjobs/workingwithgpus.html\n",
      "---------------------\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: What is the Scheduling Policies for HPC cluster?\n",
      "Assistant: The scheduling policies for an HPC cluster include FIFO (First-In-First-Out), Fair Share, and Priority-Based. In the FIFO policy, jobs are executed in the order they are submitted, which may result in long wait times for resource-intensive jobs. The Fair Share policy ensures that all users receive a fair share of cluster resources over time, with users who have high resource usage experiencing reduced priority. The Priority-Based policy assigns priorities to jobs based on user-defined criteria or system-wide rules, with higher-priority jobs being executed before lower-priority ones. \n",
      "\n",
      "Job priorities in an HPC cluster are determined by factors such as user priority, resource requirements, and walltime limit. Users can assign priority values to their jobs, with higher values resulting in increased job priority and faster access to resources. Jobs with larger resource requirements may be assigned higher priority, as they require more significant resources to execute efficiently. Jobs with shorter estimated execution times may also receive higher priority, ensuring prompt execution and freeing up resources for other jobs.\n",
      "\n",
      "Balancing policies in an HPC cluster include backfilling and preemption. Backfilling allows smaller jobs to fill in available resources ahead of larger jobs, optimizing resource utilization and reducing wait times. Preemption allows higher-priority jobs to temporarily pause the execution of lower-priority jobs, making resources available for the higher-priority jobs.\n",
      "\n",
      "Some best practices for job scheduling in an HPC cluster include setting realistic priorities, using resource quotas to prevent over- or underutilization, and leveraging backfilling by submitting smaller jobs that can fill in available resources while waiting for larger jobs to start.\n",
      "\n",
      "Sources:\n",
      "- https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\n",
      "- https://rc-docs.northeastern.edu/en/latest/runningjobs/understandingqueuing.html\n",
      "Human: How do I check Job Status?\n",
      "Assistant: SOURCES:\n",
      "- Source 1: https://rc-docs.northeastern.edu/en/latest/runningjobs/understandingqueuing.html\n",
      "- Source 2: https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Question from user :  How do I check Job Status? \n",
      "\n",
      "Reply from ChatBot :  To check the job status in an HPC cluster, you can use the `squeue` command. Here are the steps:\n",
      "\n",
      "1. Open the terminal or command prompt.\n",
      "2. Enter the following command: `squeue -u username`\n",
      "   Replace \"username\" with your actual username.\n",
      "3. The command will display the status of your jobs or job steps, including information such as the job ID, job name, state (e.g., pending, running, completed), and other details.\n",
      "\n",
      "Make sure to replace \"username\" with your actual username when using the `squeue` command.\n",
      "\n",
      "SOURCES:\n",
      "- Source 1: https://rc-docs.northeastern.edu/en/latest/runningjobs/understandingqueuing.html\n",
      "- Source 2: https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\n"
     ]
    }
   ],
   "source": [
    "query = \"How do I check Job Status?\"\n",
    "result = chain({\"question\": query})\n",
    "\n",
    "\n",
    "print(\"Question from user : \" , query ,\"\\n\")\n",
    "print(\"Reply from ChatBot : \" , result['answer'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nLT4uLaXZCR_",
    "outputId": "eb839c76-9585-4665-d251-64245f03f0ba"
   },
   "outputs": [],
   "source": [
    "type(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xoAgKYZNZJ1s",
    "outputId": "98ba51d6-d959-4f17-c2e0-72880d02d197"
   },
   "outputs": [],
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5mrfHVdAZQer"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Conversation Memory Buffer & Input Details:\n",
    "------------------------------------------\n",
    "- Input Key: question\n",
    "- Memory Key: chat_history\n",
    "- Maximum Token Limit: 1000\n",
    "- Return Messages: True\n",
    "\n",
    "ChatOpenAI Configuration:\n",
    "-------------------------\n",
    "- Verbose: True\n",
    "- Streaming: True\n",
    "- OpenAI API Key: [REDACTED]\n",
    "- OpenAI API Base: (No Value Provided)\n",
    "- OpenAI Organization: (No Value Provided)\n",
    "- OpenAI Proxy: (No Value Provided)\n",
    "- Client: ChatCompletion\n",
    "- Callbacks: FlyteCallbackHandler (Object Reference: 0x7832be39c9d0)\n",
    "\n",
    "Conversation History:\n",
    "---------------------\n",
    "1. User: What is Globus at Northeastern?\n",
    "\n",
    "   Assistant: Globus is a data management system that allows you to transfer and share files.\n",
    "   Northeastern has a subscription to Globus, and you can set up a Globus account using your Northeastern credentials.\n",
    "   You can also link your other accounts, whether they are personal or from another institution, with your Globus account.\n",
    "   To use Globus, you will need to set up an account and install Globus Connect on your local computer.\n",
    "   After completing these initial setup procedures, you can use the Globus web app to perform file transfers.\n",
    "\n",
    "   Sources:\n",
    "   - RCDocs (Using Globus)\n",
    "   - RCDocs (Globus Account Set Up)\n",
    "   - RCDocs (To use Globus)\n",
    "\n",
    "2. User: How do i set up an account with Globus?\n",
    "\n",
    "   Assistant: Globus is a data management system that allows you to transfer and share files.\n",
    "   Northeastern has a subscription to Globus, and you can set up an account with Globus using your Northeastern credentials.\n",
    "   You can also link your other accounts, whether they are personal or from another institution, with your Globus account.\n",
    "   To set up an account with Globus, follow these steps:\n",
    "   1. Go to the Globus website.\n",
    "   2. Click on \"Log In\".\n",
    "   3. Select \"Northeastern University\" from the options under \"Use your existing organizational login\" and click \"Continue\".\n",
    "   4. Enter your Northeastern username and password.\n",
    "   5. If you don't have a previous Globus account, click \"Continue\". If you have an existing account, click \"Link to an existing account\".\n",
    "   6. Check the agreement checkbox and click \"Continue\".\n",
    "   7. Click \"Allow\" to permit Globus to access your files.\n",
    "   After setting up your account, you can access the Globus File Manager app.\n",
    "\n",
    "   Sources:\n",
    "   - RCDocs (Using Globus)\n",
    "   - RCDocs (Globus Account Set Up)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jo16MuocEVXD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OqU88pUbEVS1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Chatbot Environment",
   "language": "python",
   "name": "chatbot_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
