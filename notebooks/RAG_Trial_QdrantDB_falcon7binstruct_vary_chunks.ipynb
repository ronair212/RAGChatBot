{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c96bce9f-2e23-4b3c-b416-a8308202086a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cc597bf-9b24-460a-91bf-e303782fe92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1263a695-a3ce-48b5-97ba-42e6d5581a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"COHERE_API_KEY\"] = \"Aj7fRPV0FBm1u6baUBuAZc5yMOvOs6krkrqVppam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "608c8ac9-1f4c-409e-91e5-c0ce1cf9ff87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04c91c45-ee77-42e4-84aa-6770a18c93c5",
   "metadata": {
    "id": "mq7rbJww6Cjo"
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "\n",
    "urls = [\"https://rc-docs.northeastern.edu/en/latest/welcome/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/welcome/welcome.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/welcome/services.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/welcome/gettinghelp.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/welcome/introtocluster.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/welcome/casestudiesandtestimonials.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/get_access.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/accountmanager.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/connectingtocluster/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/connectingtocluster/mac.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/connectingtocluster/windows.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/first_steps/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/first_steps/passwordlessssh.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/first_steps/shellenvironment.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/first_steps/usingbash.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/hardware/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/hardware/hardware_overview.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/hardware/partitions.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/introduction.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/accessingood.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/desktopood.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/fileexplore.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/jupyterlab.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/understandingqueuing.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/workingwithgpus.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/recurringjobs.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/debuggingjobs.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/discovery_storage.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/transferringdata.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/globus.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/databackup.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/securityandcompliance.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/modules.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/mpi.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/r.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/matlab.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/packagemanagers/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/packagemanagers/conda.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/packagemanagers/spack.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/fromsource/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/fromsource/makefile.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/fromsource/cmake.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/introductiontoslurm.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmcommands.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmrunningjobs.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmmonitoringandmanaging.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmscripts.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmarray.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmbestpractices.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/classroom/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/classroom/class_use.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/classroom/cps_ood.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/classroom/classroomexamples.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/best-practices/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/best-practices/homequota.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/best-practices/checkpointing.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/best-practices/optimizingperformance.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/best-practices/software.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/tutorialsandtraining/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/tutorialsandtraining/canvasandgithub.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/faq.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/glossary.html\",\n",
    "]\n",
    "loader = WebBaseLoader(urls)\n",
    "data = loader.load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2deeb4df-8b64-4986-a026-c81860aa5fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 02:08:58.571868: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-26 02:08:58.626598: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-26 02:09:01.480050: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "#Local mode, without using the Qdrant server, may also store your vectors on disk so they're persisted between runs.\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "\n",
    "\n",
    "from langchain.schema.vectorstore import VectorStoreRetriever\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import pipeline\n",
    "\n",
    "from langchain import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#from langchain.vectorstores import Pinecone\n",
    "\n",
    "\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "592fd0b3-1578-4e80-ac09-a9ab6a8a7b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_INSTRUCTOR_XL = \"hkunlp/instructor-xl\"\n",
    "EMB_SBERT_MPNET_BASE = \"sentence-transformers/all-mpnet-base-v2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a9af797-f1bb-433a-b495-cc98ac36081e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LLM_FALCON_7B_INSTRUCT = \"tiiuae/falcon-7b-instruct\"\n",
    "LLM_FALCON_40B_INSTRUCT = \"tiiuae/falcon-40b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d274b67-365b-4fe8-8590-0a0d3f9b2dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir='/work/rc/projects/chatbot/models'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19979ba8-7a98-461d-abbe-54df6d71bcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"persist_directory\":None,\n",
    "          \"load_in_8bit\":False,\n",
    "          \"embedding\" : EMB_SBERT_MPNET_BASE,\n",
    "          \"llm\":LLM_FALCON_40B_INSTRUCT,\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47672542-7726-458d-8077-f02d45edd67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/work/rc/projects/chatbot/models'\n",
    "#cache_folder=os.getenv('SENTENCE_TRANSFORMERS_HOME')\n",
    "os.environ['SENTENCE_TRANSFORMERS_HOME'] = '/work/rc/projects/chatbot/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d07a707a-255f-41c7-b71e-76df7e75afab",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def create_sbert_mpnet():\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        return HuggingFaceEmbeddings(model_name=EMB_SBERT_MPNET_BASE, model_kwargs={\"device\": device})\n",
    "\n",
    "'''\n",
    "\n",
    "def create_sbert_mpnet():\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        return HuggingFaceEmbeddings(model_name=EMB_SBERT_MPNET_BASE, cache_folder=cache_dir, model_kwargs={\"device\": device})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\", cache_dir=\"new_cache_dir/\")\n",
    "\n",
    "#model = AutoModelForMaskedLM.from_pretrained(\"roberta-base\", cache_dir=\"new_cache_dir/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "216b63a5-6c81-415c-8bf5-e4092448c40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_falcon_40b_instruct(load_in_8bit=False):\n",
    "        model_name = \"tiiuae/falcon-40b-instruct\"\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=\"/work/rc/projects/chatbot/models\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name , cache_dir=\"/work/rc/projects/chatbot/models\")\n",
    "        hf_pipeline = pipeline(\n",
    "                task=\"text-generation\",\n",
    "                model = model,\n",
    "                do_sample=True,\n",
    "                tokenizer = tokenizer,\n",
    "                #trust_remote_code = True,\n",
    "                max_new_tokens=100,\n",
    "                #cache_dir=cache_dir,\n",
    "                model_kwargs={\n",
    "                    \"device_map\": \"auto\", \n",
    "                    \"load_in_8bit\": load_in_8bit, \n",
    "                    \"max_length\": 512, \n",
    "                    \"temperature\": 0.01,\n",
    "                    \n",
    "                    \"torch_dtype\":torch.bfloat16,\n",
    "                    }\n",
    "            )\n",
    "        return hf_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8499ec1f-2405-42d5-a990-3eec2eb06baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_falcon_7b_instruct(load_in_8bit=False):\n",
    "        model_name = \"tiiuae/falcon-7b-instruct\"\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=\"/work/rc/projects/chatbot/models\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name , cache_dir=\"/work/rc/projects/chatbot/models\")\n",
    "        hf_pipeline = pipeline(\n",
    "                task=\"text-generation\",\n",
    "                model = model,\n",
    "                do_sample=True,\n",
    "                tokenizer = tokenizer,\n",
    "                #trust_remote_code = True,\n",
    "                max_new_tokens=100,\n",
    "                #cache_dir=cache_dir,\n",
    "                model_kwargs={\n",
    "                    \"device_map\": \"auto\", \n",
    "                    \"load_in_8bit\": load_in_8bit, \n",
    "                    \"max_length\": 512, \n",
    "                    \"temperature\": 0.01,\n",
    "                    \n",
    "                    \"torch_dtype\":torch.bfloat16,\n",
    "                    }\n",
    "            )\n",
    "        return hf_pipeline\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cac57891-9948-4517-86f0-c6bd141b5d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_flan_t5_base(load_in_8bit=False):\n",
    "        # Wrap it in HF pipeline for use with LangChain\n",
    "        model=\"google/flan-t5-base\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model, cache_dir=cache_dir)\n",
    "        return pipeline(\n",
    "            task=\"text2text-generation\",\n",
    "            model=model,\n",
    "            tokenizer = tokenizer,\n",
    "            max_new_tokens=100,\n",
    "            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.}\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85552c12-701a-4984-8436-a7273d08e7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"embedding\"] == EMB_SBERT_MPNET_BASE:\n",
    "    embedding = create_sbert_mpnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "921d211e-6c86-4de8-afc2-09659c45487b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dacf28f94cd4191a1bad1e8012fc107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_in_8bit = config[\"load_in_8bit\"]\n",
    "\n",
    "if config[\"llm\"] == LLM_FALCON_40B_INSTRUCT:\n",
    "    llm = create_falcon_40b_instruct(load_in_8bit=load_in_8bit)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35c3860e-fc8d-4998-bcfb-08add0ace2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_in_8bit = config[\"load_in_8bit\"]\n",
    "\n",
    "if config[\"llm\"] == LLM_FALCON_7B_INSTRUCT:\n",
    "    llm = create_falcon_7b_instruct(load_in_8bit=load_in_8bit)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c94184dc-a2be-48ae-90db-b1536633167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "CONDENSE_QUESTION_PROMPT = '''\n",
    "Below is a summary of the conversation so far, and a new question asked by the user that needs to be answered by searching in a knowledge base.\n",
    "Generate a search query based on the conversation and the new question.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Search query:\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "PromptTemplates = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"question\"],\n",
    "    template=\"\"\"\n",
    "Below is a summary of the conversation so far, and a new question asked by the user that needs to be answered by searching in a knowledge base.\n",
    "Generate a search query based on the conversation and the new question.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Search query:\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc4619a9-31f6-480b-bd1c-6a67fa5f1679",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer_Generator_Prompt= '''\n",
    "<Instructions>\n",
    "Important:\n",
    "Answer with the facts listed in the list of sources below. If there isn't enough information below, say you don't know.\n",
    "If asking a clarifying question to the user would help, ask the question.\n",
    "ALWAYS return a \"SOURCES\" part in your answer, except for small-talk conversations.\n",
    "\n",
    "Question: {question}\n",
    "Sources:\n",
    "---------------------\n",
    "    {summaries}\n",
    "---------------------\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cef8762-3816-4ffe-bd2f-757590f018db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "chunk_sizes = [200,400,600,800]\n",
    "chunk_overlaps = [40,60 , 80]\n",
    "\n",
    "input_path = 'sample_questions.xlsx'\n",
    "df_input = pd.read_excel(input_path)\n",
    "queries = df_input['queries'].tolist()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1bd38e-40d7-42c1-a47f-e0813f17d462",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current values are =  200 40\n",
      "Qdrantdb with  RC_documents_f7bi1  completed\n",
      "running query  What is the Scheduling Policies for HPC cluster?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "<Instructions>\n",
      "Important:\n",
      "Answer with the facts listed in the list of sources below. If there isn't enough information below, say you don't know.\n",
      "If asking a clarifying question to the user would help, ask the question.\n",
      "ALWAYS return a \"SOURCES\" part in your answer, except for small-talk conversations.\n",
      "\n",
      "Question: What is the Scheduling Policies for HPC cluster?\n",
      "Sources:\n",
      "---------------------\n",
      "    Content:  priorities.\n",
      "Fair use policies typically utilize a scheduler that allocates resources to jobs based on job priority, resource requirements, and historical resource usage. This approach prevents any single user or job type from monopolizing the resources and ensures a balanced and efficient allocation of resources across different users and projects.\n",
      "This policy promotes fairness and efficient resource utilization and contributes to an inclusive and productive computing environment for all users.\n",
      "\n",
      "See also\n",
      "Our page on the Queuing System and Job Scheduling Policies and Priorities.\n",
      "\n",
      "\n",
      "Back to the top\n",
      "\n",
      "\n",
      "Accounts#\n",
      "\n",
      "\n",
      "How do I get an account for the HPC cluster?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "See Request An Account, and complete the ServiceNow RC Access Request form with your details and submit it. Your request will be processed within a few business days.\n",
      "\n",
      "\n",
      "\n",
      "How can I reset my password?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Please use our website’s ‘Account Management’ tool for password reset. Follow the instructions to reset your password\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/faq.html\n",
      "\n",
      "Content: A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.\n",
      "\n",
      "Parallel Computing#A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.\n",
      "\n",
      "Partition#A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.\n",
      "\n",
      "Quota#A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.\n",
      "\n",
      "Queue#A waiting line for jobs ready to be executed but waiting for resources to become available.\n",
      "\n",
      "Resource Reservation#The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.\n",
      "\n",
      "Scheduling Policy#A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/glossary.html\n",
      "\n",
      "Content: s scheduling system. Job priority determines the order in which jobs are executed and the allocation of computing resources.\n",
      "In an HPC environment, different jobs may have varying degrees of importance or resource requirements. Job priority allows the cluster’s scheduler to make decisions on which job to execute next, considering factors such as:\n",
      "\n",
      "User-defined Priority: Users can often assign priority values to their jobs, indicating the relative importance of their tasks. Higher priority values typically result in faster job execution.\n",
      "Resource Requirements: Jobs with greater resource demands, such as more CPUs, memory, or GPUs, may receive higher priority, ensuring they receive the necessary resources for efficient execution.\n",
      "Walltime Limit: Jobs with shorter estimated execution times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.\n",
      "\n",
      "By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/glossary.html\n",
      "---------------------\n",
      "\n",
      "Chat History:\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The current implementation of Falcon calls `torch.scaled_dot_product_attention` directly, this will be deprecated in the future in favor of the `BetterTransformer` API. Please install the latest optimum library with `pip install -U optimum` and call `model.to_bettertransformer()` to benefit from `torch.scaled_dot_product_attention` and future performance optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer is  <Bots> We can't save chat history on Google Cloud Platform and can't reply to private messages.\n",
      "running query  How do I check Job Status?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "Below is a summary of the conversation so far, and a new question asked by the user that needs to be answered by searching in a knowledge base.\n",
      "Generate a search query based on the conversation and the new question.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: What is the Scheduling Policies for HPC cluster?\n",
      "Assistant: <Bots> We can't save chat history on Google Cloud Platform and can't reply to private messages.\n",
      "\n",
      "Question:\n",
      "How do I check Job Status?\n",
      "\n",
      "Search query:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "<Instructions>\n",
      "Important:\n",
      "Answer with the facts listed in the list of sources below. If there isn't enough information below, say you don't know.\n",
      "If asking a clarifying question to the user would help, ask the question.\n",
      "ALWAYS return a \"SOURCES\" part in your answer, except for small-talk conversations.\n",
      "\n",
      "Question: How do I check Job Status?\n",
      "Sources:\n",
      "---------------------\n",
      "    Content: \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Job Scheduling Policies and Priorities - RC RTD\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Menu\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Expand\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Light mode\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dark mode\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Auto light/dark mode\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Hide navigation sidebar\n",
      "\n",
      "\n",
      "Hide table of contents sidebar\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Toggle site navigation sidebar\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RC RTD\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Toggle Light / Dark / Auto color theme\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Toggle table of contents sidebar\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Research ComputingToggle child pages in navigation\n",
      "Welcome\n",
      "Services We Provide\n",
      "Getting Help\n",
      "Introduction to HPC and Slurm\n",
      "Case Studies and User Testimonials\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Getting StartedToggle child pages in navigation\n",
      "Getting Access\n",
      "Account Manager\n",
      "Connecting To ClusterToggle child pages in navigation\n",
      "Mac\n",
      "Windows\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "First\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\n",
      "\n",
      "Content:  Docs\n",
      "\n",
      "Project Home\n",
      "\n",
      "\n",
      "Builds\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Back to top\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Edit this page\n",
      "\n",
      "\n",
      "\n",
      "Toggle Light / Dark / Auto color theme\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Toggle table of contents sidebar\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Job Scheduling Policies and Priorities#\n",
      "In an HPC environment, efficient job scheduling is crucial for allocating computing resources and ensuring optimal cluster utilization. Job scheduling policies and priorities determine the order in which jobs are executed and the resources they receive. Understanding these policies is essential for maximizing job efficiency and minimizing wait times.\n",
      "\n",
      "Scheduling Policies#\n",
      "\n",
      "FIFO (First-In-First-Out)#\n",
      "Jobs are executed in the order they are submitted. Although simple, this policy may lead to long wait times for large, resource-intensive jobs if smaller jobs are constantly being submitted.\n",
      "\n",
      "\n",
      "Fair Share#\n",
      "This policy ensures that all users receive a fair share of cluster resources over\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\n",
      "\n",
      "Content:  the job when they become available, according to the job’s priority and the available resources.\n",
      "\n",
      "\n",
      "Job Submission and Scheduling#\n",
      "Jobs are submitted to the queue via a script specifying the resources required (e.g., number of CPUs, memory, and GPUs) and the commands to be executed. Once submitted, the queuing system schedules the job based on the resources requested, the current system load, and scheduling policies.\n",
      "\n",
      "\n",
      "Scheduling Policies**#\n",
      "Our cluster uses a fair-share scheduling policy. This means that usage is tracked for each user or group, and the system attempts to balance resource allocation over time. If a user or group has been using many resources, their job priority may be temporarily reduced to allow others to use the system. Conversely, users or groups that have used fewer resources will have their jobs prioritized.\n",
      "The following policies ensure fair use of the cluster resources:\n",
      "\n",
      "Single job size: The maximum number of nodes a single job\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/runningjobs/understandingqueuing.html\n",
      "---------------------\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: What is the Scheduling Policies for HPC cluster?\n",
      "Assistant: <Bots> We can't save chat history on Google Cloud Platform and can't reply to private messages.\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "answer is  \n",
      "running query  What is a cluster?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "Below is a summary of the conversation so far, and a new question asked by the user that needs to be answered by searching in a knowledge base.\n",
      "Generate a search query based on the conversation and the new question.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: What is the Scheduling Policies for HPC cluster?\n",
      "Assistant: <Bots> We can't save chat history on Google Cloud Platform and can't reply to private messages.\n",
      "Human: How do I check Job Status?\n",
      "Assistant: \n",
      "\n",
      "Question:\n",
      "What is a cluster?\n",
      "\n",
      "Search query:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "<Instructions>\n",
      "Important:\n",
      "Answer with the facts listed in the list of sources below. If there isn't enough information below, say you don't know.\n",
      "If asking a clarifying question to the user would help, ask the question.\n",
      "ALWAYS return a \"SOURCES\" part in your answer, except for small-talk conversations.\n",
      "\n",
      "Question: What is a cluster?\n",
      "Sources:\n",
      "---------------------\n",
      "    Content:  Docs\n",
      "\n",
      "Project Home\n",
      "\n",
      "\n",
      "Builds\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Back to top\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Edit this page\n",
      "\n",
      "\n",
      "\n",
      "Toggle Light / Dark / Auto color theme\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Toggle table of contents sidebar\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Job Scheduling Policies and Priorities#\n",
      "In an HPC environment, efficient job scheduling is crucial for allocating computing resources and ensuring optimal cluster utilization. Job scheduling policies and priorities determine the order in which jobs are executed and the resources they receive. Understanding these policies is essential for maximizing job efficiency and minimizing wait times.\n",
      "\n",
      "Scheduling Policies#\n",
      "\n",
      "FIFO (First-In-First-Out)#\n",
      "Jobs are executed in the order they are submitted. Although simple, this policy may lead to long wait times for large, resource-intensive jobs if smaller jobs are constantly being submitted.\n",
      "\n",
      "\n",
      "Fair Share#\n",
      "This policy ensures that all users receive a fair share of cluster resources over\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\n",
      "\n",
      "Content: A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.\n",
      "\n",
      "Parallel Computing#A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.\n",
      "\n",
      "Partition#A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.\n",
      "\n",
      "Quota#A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.\n",
      "\n",
      "Queue#A waiting line for jobs ready to be executed but waiting for resources to become available.\n",
      "\n",
      "Resource Reservation#The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.\n",
      "\n",
      "Scheduling Policy#A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/glossary.html\n",
      "\n",
      "Content:  among nodes in a cluster.\n",
      "\n",
      "Compute Node#A component within a cluster that performs the actual computational tasks. It typically consists of multiple processors, memory, and storage resources, where the primary computation and data processing occur.\n",
      "\n",
      "Concurrency#The ability of the system to handle multiple tasks or jobs simultaneously, without waiting for each task to complete before starting another.\n",
      "\n",
      "Container#A lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Containers provide consistency and portability across different computing environments.\n",
      "\n",
      "Core#A processor within a CPU. Each core can execute its tasks.\n",
      "\n",
      "Central Processing Unit (CPU)#The primary component of a computer that performs most processing inside the computer. CPUs can have multiple cores.\n",
      "\n",
      "Fair Share Allocation#A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/glossary.html\n",
      "---------------------\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: What is the Scheduling Policies for HPC cluster?\n",
      "Assistant: <Bots> We can't save chat history on Google Cloud Platform and can't reply to private messages.\n",
      "Human: How do I check Job Status?\n",
      "Assistant: \n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "answer is  **Can you provide more context on what specifically you want to know?\n",
      "running query  How do I submit an interactive job?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "Below is a summary of the conversation so far, and a new question asked by the user that needs to be answered by searching in a knowledge base.\n",
      "Generate a search query based on the conversation and the new question.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: What is the Scheduling Policies for HPC cluster?\n",
      "Assistant: <Bots> We can't save chat history on Google Cloud Platform and can't reply to private messages.\n",
      "Human: How do I check Job Status?\n",
      "Assistant: \n",
      "Human: What is a cluster?\n",
      "Assistant: **Can you provide more context on what specifically you want to know?\n",
      "\n",
      "Question:\n",
      "How do I submit an interactive job?\n",
      "\n",
      "Search query:\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "\n",
    "for chunk_size in chunk_sizes:\n",
    "    for chunk_overlap in chunk_overlaps:\n",
    "        print(\"current values are = \" , chunk_size ,chunk_overlap )\n",
    "        i = i + 1 \n",
    "        \n",
    "        text_splitter = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        docs = text_splitter.split_documents(data)\n",
    "        name  = \"RC_documents_f7bi\" + str(i)\n",
    "        Qdrantdb = Qdrant.from_documents(\n",
    "            docs,\n",
    "            embeddings,\n",
    "            path=\"/work/rc/projects/chatbot/chatbotrc/notebooks/RAG/tmp/local_qdrant\"+name,\n",
    "            collection_name=name ,\n",
    "        )\n",
    "        print(\"Qdrantdb with \" , name , \" completed\" )\n",
    "        \n",
    "\n",
    "        retriever = VectorStoreRetriever(vectorstore=Qdrantdb, search_type=\"mmr\", search_kwargs={'k': 4, 'fetch_k': 10},)\n",
    "        compressor = CohereRerank()\n",
    "        \n",
    "        reranker = ContextualCompressionRetriever(\n",
    "            base_compressor=compressor, base_retriever=retriever)\n",
    "        \n",
    "        context_callback = StdOutCallbackHandler()\n",
    "        \n",
    "        hf_llm = HuggingFacePipeline(pipeline=llm)\n",
    "\n",
    "        memory = ConversationTokenBufferMemory(llm=hf_llm,memory_key=\"chat_history\", return_messages=True,input_key='question',max_token_limit=1000)\n",
    "        system_message_prompt = SystemMessagePromptTemplate(prompt=PromptTemplates)\n",
    "\n",
    "        chat_prompt_for_ques = ChatPromptTemplate.from_messages([system_message_prompt])\n",
    "\n",
    "        question_generator = LLMChain(llm=hf_llm, prompt=chat_prompt_for_ques, verbose=True)\n",
    "        \n",
    "        chat_prompt = PromptTemplate(template=Answer_Generator_Prompt, input_variables=[\"question\", \"summaries\",\"chat_history\"])\n",
    "\n",
    "        answer_chain = load_qa_with_sources_chain(hf_llm, chain_type=\"stuff\", verbose=True,prompt=chat_prompt)\n",
    "        chain = ConversationalRetrievalChain(\n",
    "            retriever=reranker,\n",
    "            question_generator=question_generator,\n",
    "            combine_docs_chain=answer_chain,\n",
    "            verbose=True,\n",
    "            memory=memory,\n",
    "            rephrase_question=False )\n",
    "        \n",
    "        # Iterate over queries and save results\n",
    "        answers = []\n",
    "        for query in queries:\n",
    "            print(\"running query \" , query)\n",
    "            result = chain({\"question\": query})\n",
    "            print(\"answer is \" , result['answer'])\n",
    "            answers.append(result['answer'])\n",
    "            \n",
    "        df_output = pd.DataFrame({'queries': queries, 'answers': answers})\n",
    "\n",
    "        # Save the output to an Excel sheet\n",
    "        output_filename = f'{name}_output_chunk_size_{chunk_size}_overlap_{chunk_overlap}.xlsx'\n",
    "        df_output.to_excel(output_filename, index=False)\n",
    "        \n",
    "print(\"Finished processing all combinations!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74c607f-c08c-4c88-a2fa-242400e81f66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Chatbot Environment",
   "language": "python",
   "name": "chatbot_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
