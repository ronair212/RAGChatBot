{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation using ChromaDB and GPT\n",
    "\n",
    "## Overview\n",
    "\n",
    "This code snippet explores the implementation of a conversational chatbot using various APIs and the `langchain` Python library. Specifically, it initiates API interactions with `Cohere` and `OpenAI`, utilizes web scraping for document loading, applies token-based text splitting, and leverages embedding models for semantic understanding. Additionally, it applies a memory buffer to retain chat history and structures conversational chains for query and answer generation. \n",
    "\n",
    "### Setting Up API Keys\n",
    "\n",
    "```python\n",
    "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OPENAI API Key:\")\n",
    "```\n",
    "\n",
    "The above code securely retrieves API keys for Cohere and OpenAI, prompting the user to input them without displaying them on-screen.\n",
    "\n",
    "### Loading and Pre-processing Data\n",
    "\n",
    "```python\n",
    "urls = [...]\n",
    "loader = WebBaseLoader(urls)\n",
    "data = loader.load()\n",
    "```\n",
    "\n",
    "A predefined set of URLs is loaded and parsed using `WebBaseLoader`, which essentially fetches documents/web pages for further interaction.\n",
    "\n",
    "### Embeddings and Document Representation\n",
    "\n",
    "```python\n",
    "embeddings = CohereEmbeddings(model='...', cohere_api_key=..(\"COHERE_API_KEY\"))\n",
    "db = Chroma.from_documents(docs, embeddings)\n",
    "```\n",
    "This segment prepares document embeddings using the `CohereEmbeddings` and stores them in a format suitable for quick retrieval and similarity search using `Chroma`.\n",
    "\n",
    "### Retrievers and Compressors\n",
    "\n",
    "```python\n",
    "from langchain.retrievers import ContextualCompressionRetriever, VectorStoreRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "\n",
    "#... [various retriever and compressor setup]\n",
    "```\n",
    "Retrievers and compressors manage the extraction and summarization of information from the loaded documents.\n",
    "\n",
    "### Conversational Interface\n",
    "\n",
    "```python\n",
    "memory = ConversationTokenBufferMemory(llm=llm, memory_key=\"chat_history\", return_messages=True, input_key='question', max_token_limit=1000)\n",
    "```\n",
    "The conversation’s state is managed through memory objects. This setup helps retain and reference past interactions to manage coherent dialogue.\n",
    "\n",
    "### Question and Answer Generation\n",
    "\n",
    "```python\n",
    "#... [prompt template set up]\n",
    "\n",
    "question_generator = LLMChain(llm=llm, prompt=chat_prompt_for_ques, verbose=True)\n",
    "answer_chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\", verbose=True, prompt=chat_prompt)\n",
    "```\n",
    "Using predefined prompt templates, the system generates questions and answers. These questions derive from user inputs and historical chat data, while answers reference and cite embedded documents.\n",
    "\n",
    "### Conversational Chain and User Interaction\n",
    "\n",
    "```python\n",
    "chain = ConversationalRetrievalChain(\n",
    "            retriever=reranker,\n",
    "            question_generator=question_generator,\n",
    "            combine_docs_chain=answer_chain,\n",
    "            verbose=True,\n",
    "            memory=memory,\n",
    "            rephrase_question=False\n",
    ")\n",
    "\n",
    "query = \"What is the Scheduling Policies for HPC cluster?\"\n",
    "result = chain({\"question\": query})\n",
    "\n",
    "print(\"Question from user : \" , query ,\"\\n\")\n",
    "print(\"Reply from ChatBot : \" , result['answer'])\n",
    "```\n",
    "This conversational chain consolidates the previously defined components to facilitate user interaction. It retrieves and generates responses dynamically, considering prior conversational history and the loaded document base. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MNSwbz8svgCi",
    "outputId": "edbdd442-5caa-4c86-b902-da4a5f72f106"
   },
   "outputs": [],
   "source": [
    "#!pip install pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MuNwBfb6BngZ",
    "outputId": "30409fdb-2773-411e-dcfc-170ab7fc63f7"
   },
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nwJCPFhtCBmi",
    "outputId": "aa6b6914-32a8-4f4d-e68c-84e5077d0fad"
   },
   "outputs": [],
   "source": [
    "#!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vwqNT7T0CF-O",
    "outputId": "9fc123df-4163-4f34-f81f-610513bc1fe5"
   },
   "outputs": [],
   "source": [
    "#!pip install cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QQl_Bfl8CLL1",
    "outputId": "cc07de5a-2e7c-4b28-a66d-5906fea3a052"
   },
   "outputs": [],
   "source": [
    "#!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3oMYj3f-vmG7"
   },
   "outputs": [],
   "source": [
    "#import pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "njgYMfbQvnjn"
   },
   "outputs": [],
   "source": [
    "#pinecone.init(api_key=\"b360318b-4fc8-4580-bf6c-d88959179985\",\n",
    "#              environment=\"us-west1-gcp-free\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "drKj9gcYvx36"
   },
   "outputs": [],
   "source": [
    "#pinecone.whoami()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "B979WTowvyGz"
   },
   "outputs": [],
   "source": [
    "#pinecone.list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "71U2rh7xwQIy"
   },
   "outputs": [],
   "source": [
    "#pinecone.list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Cohere API Key: ········································\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nCohereAPIError: You are using a Trial key, which is limited to 2 API calls / minute.\\nYou can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at\\n'https://dashboard.cohere.ai/api-keys'. \\nContact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "CohereAPIError: You are using a Trial key, which is limited to 2 API calls / minute.\n",
    "You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at\n",
    "'https://dashboard.cohere.ai/api-keys'. \n",
    "Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OPENAI API Key: ···················································\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OPENAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "mq7rbJww6Cjo"
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "\n",
    "urls = [\"https://rc-docs.northeastern.edu/en/latest/welcome/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/welcome/welcome.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/welcome/services.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/welcome/gettinghelp.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/welcome/introtocluster.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/welcome/casestudiesandtestimonials.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/get_access.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/accountmanager.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/connectingtocluster/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/connectingtocluster/mac.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/connectingtocluster/windows.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/first_steps/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/first_steps/passwordlessssh.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/first_steps/shellenvironment.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/first_steps/usingbash.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/hardware/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/hardware/hardware_overview.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/hardware/partitions.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/introduction.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/accessingood.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/desktopood.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/fileexplore.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/jupyterlab.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/understandingqueuing.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/workingwithgpus.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/recurringjobs.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/debuggingjobs.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/discovery_storage.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/transferringdata.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/globus.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/databackup.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/securityandcompliance.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/modules.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/mpi.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/r.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/matlab.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/packagemanagers/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/packagemanagers/conda.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/packagemanagers/spack.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/fromsource/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/fromsource/makefile.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/fromsource/cmake.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/introductiontoslurm.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmcommands.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmrunningjobs.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmmonitoringandmanaging.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmscripts.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmarray.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmbestpractices.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/classroom/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/classroom/class_use.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/classroom/cps_ood.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/classroom/classroomexamples.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/best-practices/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/best-practices/homequota.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/best-practices/checkpointing.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/best-practices/optimizingperformance.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/best-practices/software.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/tutorialsandtraining/index.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/tutorialsandtraining/canvasandgithub.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/faq.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/glossary.html\",\n",
    "]\n",
    "loader = WebBaseLoader(urls)\n",
    "data = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6twYZJo26GmB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "encoding_name = tiktoken.get_encoding(\"cl100k_base\")\n",
    "#tiktoken package to count the total tokens in our corpus\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "9b3T5m1YBVrf",
    "outputId": "4f06dc21-756e-4dd9-f462-c503f4dd675b",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\n\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    chunk_size = 700,\\n    chunk_overlap  = 70,\\n    length_function = len,\\n    add_start_index = True,\\n)\\ndocs = text_splitter.create_documents([data])\\n\\nfor idx, text in enumerate(docs):\\n    docs[idx].metadata[\\'source\\'] = \"RCDocs\"\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "text_splitter = TokenTextSplitter(chunk_size=500, chunk_overlap=25)\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "'''\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 700,\n",
    "    chunk_overlap  = 70,\n",
    "    length_function = len,\n",
    "    add_start_index = True,\n",
    ")\n",
    "docs = text_splitter.create_documents([data])\n",
    "\n",
    "for idx, text in enumerate(docs):\n",
    "    docs[idx].metadata['source'] = \"RCDocs\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_DfcNy9sT3jj",
    "outputId": "36618e1f-e92c-4ed3-f051-980b780bc57b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain.schema.document.Document"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PILXy2pdVC4v",
    "outputId": "99fdfd5c-a9f0-422a-dded-c4c33e642735"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='\\n\\n\\n\\n\\n\\n\\nResearch Computing - RC RTD\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nContents\\n\\n\\n\\n\\n\\nMenu\\n\\n\\n\\n\\n\\n\\n\\nExpand\\n\\n\\n\\n\\n\\nLight mode\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDark mode\\n\\n\\n\\n\\n\\n\\nAuto light/dark mode\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHide navigation sidebar\\n\\n\\nHide table of contents sidebar\\n\\n\\n\\n\\n\\nToggle site navigation sidebar\\n\\n\\n\\n\\nRC RTD\\n\\n\\n\\n\\nToggle Light / Dark / Auto color theme\\n\\n\\n\\n\\n\\n\\nToggle table of contents sidebar\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nResearch ComputingToggle child pages in navigation\\nWelcome\\nServices We Provide\\nGetting Help\\nIntroduction to HPC and Slurm\\nCase Studies and User Testimonials\\n\\n\\n\\n\\nGetting StartedToggle child pages in navigation\\nGetting Access\\nAccount Manager\\nConnecting To ClusterToggle child pages in navigation\\nMac\\nWindows\\n\\n\\n\\n\\nFirst StepsToggle child pages in navigation\\nPasswordless SSH\\nShell Environment on the Cluster\\nCluster via Command-Line\\n\\n\\n\\nUser Guides\\n\\nHardwareToggle child pages in navigation\\nOverview\\nPartitions\\n\\n\\nOpen OnDemand (OOD)Toggle child pages in navigation\\nIntroduction to OOD\\nAccessing Open OnDemand\\nInteractive Open OnDemand ApplicationsToggle child pages in navigation\\nDesktop App\\nOOD File Explorer\\nJupyterLab\\nStata\\n\\n\\n\\n\\nRunning JobsToggle child pages in navigation\\nUnderstanding the Queuing System\\nJob Scheduling Policies and Priorities\\nInteractive and Batch Mode\\nWorking with GPUs\\nRecurring Jobs\\nDebugging and Troubleshooting Jobs\\n\\n\\nData ManagementToggle child pages in navigation\\nData Storage Options\\nTransfer Data\\nUsing Globus\\nData Backup and Restore\\nSecurity and Compliance\\n\\n\\nSoftwareToggle child pages in navigation\\nSystem WideToggle child pages in navigation\\nModules\\nMPI\\nR\\nMatlab\\n\\n\\nPackage ManagersToggle child pages in navigation\\nConda\\nSpack\\n\\n\\nFrom SourceToggle child pages in navigation\\nMake\\nCMake\\n\\n\\n\\n\\nSlurmToggle child pages in navigation\\nIntroduction to Slurm\\nSlurm Commands\\nSlurm Running Jobs\\nMonitoring and Managing Jobs\\nSlurm Job Scripts\\nSlurm Array Jobs and Dependencies\\nSlurm Best Practices\\n\\n\\nHPC for the ClassroomToggle child pages', metadata={'source': 'https://rc-docs.northeastern.edu/en/latest/welcome/index.html', 'title': 'Research Computing - RC RTD', 'language': 'en'})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "kh1JnImJ6Mt2"
   },
   "outputs": [],
   "source": [
    "#from langchain.vectorstores import Pinecone\n",
    "#import pinecone\n",
    "from langchain.embeddings import CohereEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "yZiQ1hc_6PYl"
   },
   "outputs": [],
   "source": [
    "embeddings = CohereEmbeddings(model='embed-english-light-v2.0',cohere_api_key=os.environ.get(\"COHERE_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bumy27EP6Q7F",
    "outputId": "19048b6b-2076-4af7-b351-78053cf34a9c"
   },
   "outputs": [],
   "source": [
    "#pinecone.init(\n",
    "#\tapi_key='b360318b-4fc8-4580-bf6c-d88959179985',\n",
    "#\tenvironment='us-west1-gcp-free'\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "8U4W4qHt_m79"
   },
   "outputs": [],
   "source": [
    "#pinecone.delete_index(\"chatbot1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "XYqGH05t_nXW"
   },
   "outputs": [],
   "source": [
    "#pinecone.create_index(\"chatbot1\", dimension=1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "yHrQSGHpWSMT"
   },
   "outputs": [],
   "source": [
    "#index = pinecone.Index('chatbot1')\n",
    "\n",
    "#index_name = \"chatbot1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "tLU7yDoS6Stw"
   },
   "outputs": [],
   "source": [
    "#docsearch = Pinecone.from_documents(docs, embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "5oEHdLS1mfma"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#You can configure Chroma to save and load from your local machine. \\n#Data will be persisted automatically and loaded on start (if it exists).\\n\\nimport chromadb\\n\\nfrom chromadb.config import Settings\\n\\n\\nclient = chromadb.Client(Settings(chroma_db_impl=\"duckdb+parquet\", \\n                                    persist_directory=\"db/\"\\n                                ))\\n#DuckDB on the backend\\n#stored in db folder\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#You can configure Chroma to save and load from your local machine. \n",
    "#Data will be persisted automatically and loaded on start (if it exists).\n",
    "\n",
    "# save to disk\n",
    "db2 = Chroma.from_documents(docs, embedding_function, persist_directory=\"./chroma_db\")\n",
    "docs = db2.similarity_search(query)\n",
    "\n",
    "# load from disk\n",
    "db3 = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding_function)\n",
    "docs = db3.similarity_search(query)\n",
    "print(docs[0].page_content)\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "39b7bDIInas5"
   },
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(docs, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "CF8AHNQm6Ucf"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "#from langchain.vectorstores import Pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "aeEbBgCS6dHH"
   },
   "outputs": [],
   "source": [
    "# load index\n",
    "#docsearch = Pinecone.from_existing_index(index_name, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "zp19GFSp6ed2",
    "outputId": "b8cf41d0-4243-43bc-a35d-08a2eb34aba8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nVectorStoreRetriever\\n\\nReturn VectorStoreRetriever initialized from this VectorStore.\\n\\nArgs:\\n    search_type (Optional[str]): Defines the type of search that\\n        the Retriever should perform.\\nCan be \"similarity\" (default), \"mmr\", or\\n\"similarity_score_threshold\".\\n    search_kwargs (Optional[Dict]): Keyword arguments to pass to the\\n        search function. Can include things like:\\n            k: Amount of documents to return (Default: 4)\\n            score_threshold: Minimum relevance threshold\\n                for similarity_score_threshold\\n            fetch_k: Amount of documents to pass to MMR algorithm (Default: 20)\\n            lambda_mult: Diversity of results returned by MMR;\\n                1 for minimum diversity and 0 for maximum. (Default: 0.5)\\n            filter: Filter by document metadata\\n\\nReturns:\\n    VectorStoreRetriever: Retriever class for VectorStore.\\n\\nExamples:\\n\\n# Retrieve more documents with higher diversity\\n# Useful if your dataset has many similar documents\\ndocsearch.as_retriever(\\n    search_type=\"mmr\",\\n    search_kwargs={\\'k\\': 6, \\'lambda_mult\\': 0.25}\\n)\\n\\n# Fetch more documents for the MMR algorithm to consider\\n# But only return the top 5\\ndocsearch.as_retriever(\\n    search_type=\"mmr\",\\n    search_kwargs={\\'k\\': 5, \\'fetch_k\\': 50}\\n)\\n\\n# Only retrieve documents that have a relevance score\\n# Above a certain threshold\\ndocsearch.as_retriever(\\n    search_type=\"similarity_score_threshold\",\\n    search_kwargs={\\'score_threshold\\': 0.8}\\n)\\n\\n# Only get the single most similar document from the dataset\\ndocsearch.as_retriever(search_kwargs={\\'k\\': 1})\\n\\n# Use a filter to only retrieve documents from a specific paper\\ndocsearch.as_retriever(\\n    search_kwargs={\\'filter\\': {\\'paper_title\\':\\'GPT-4 Technical Report\\'}}\\n)\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize base retriever\n",
    "#retriever = docsearch.as_retriever(search_kwargs={\"k\": 4})\n",
    "#retriever = Chroma.as_retriever(search_kwargs={\"k\": 4})\n",
    "#retriever = Chroma.as_retriever(\n",
    "#    search_type=\"mmr\",\n",
    "#    search_kwargs={'k': 4, 'fetch_k': 50} )\n",
    "\n",
    "from langchain.schema.vectorstore import VectorStoreRetriever\n",
    "retriever = VectorStoreRetriever(vectorstore=db, search_type=\"mmr\", search_kwargs={'k': 4, 'fetch_k': 10},)\n",
    "\n",
    "'''\n",
    "\n",
    "VectorStoreRetriever\n",
    "\n",
    "Return VectorStoreRetriever initialized from this VectorStore.\n",
    "\n",
    "Args:\n",
    "    search_type (Optional[str]): Defines the type of search that\n",
    "        the Retriever should perform.\n",
    "Can be \"similarity\" (default), \"mmr\", or\n",
    "\"similarity_score_threshold\".\n",
    "    search_kwargs (Optional[Dict]): Keyword arguments to pass to the\n",
    "        search function. Can include things like:\n",
    "            k: Amount of documents to return (Default: 4)\n",
    "            score_threshold: Minimum relevance threshold\n",
    "                for similarity_score_threshold\n",
    "            fetch_k: Amount of documents to pass to MMR algorithm (Default: 20)\n",
    "            lambda_mult: Diversity of results returned by MMR;\n",
    "                1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
    "            filter: Filter by document metadata\n",
    "\n",
    "Returns:\n",
    "    VectorStoreRetriever: Retriever class for VectorStore.\n",
    "\n",
    "Examples:\n",
    "\n",
    "# Retrieve more documents with higher diversity\n",
    "# Useful if your dataset has many similar documents\n",
    "docsearch.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={'k': 6, 'lambda_mult': 0.25}\n",
    ")\n",
    "\n",
    "# Fetch more documents for the MMR algorithm to consider\n",
    "# But only return the top 5\n",
    "docsearch.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={'k': 5, 'fetch_k': 50}\n",
    ")\n",
    "\n",
    "# Only retrieve documents that have a relevance score\n",
    "# Above a certain threshold\n",
    "docsearch.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={'score_threshold': 0.8}\n",
    ")\n",
    "\n",
    "# Only get the single most similar document from the dataset\n",
    "docsearch.as_retriever(search_kwargs={'k': 1})\n",
    "\n",
    "# Use a filter to only retrieve documents from a specific paper\n",
    "docsearch.as_retriever(\n",
    "    search_kwargs={'filter': {'paper_title':'GPT-4 Technical Report'}}\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "LcR414tm7UAd"
   },
   "outputs": [],
   "source": [
    "compressor = CohereRerank() #LLMChainExtractor,LLMChainFilter,EmbeddingsFilter\n",
    "# will iterate over the initially returned documents and extract from each only the content that is relevant to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "dmf4wxNb7Xr0"
   },
   "outputs": [],
   "source": [
    "# Set up cohere's reranker\n",
    "''' instead of immediately returning retrieved documents as-is, \n",
    "you can compress them using the context of the given query, so that only the relevant information is returned. '''\n",
    "reranker = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "E6L9I9sU7Yy7"
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "EQuyfiGR7aO5"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "lUJilunXIox7"
   },
   "outputs": [],
   "source": [
    "#from langchain.callbacks import ContextCallbackHandler\n",
    "#from langchain.callbacks import FlyteCallbackHandler\n",
    "from langchain.callbacks import StdOutCallbackHandler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "M18gm_YBHKE8"
   },
   "outputs": [],
   "source": [
    "#context_callback = ContextCallbackHandler(token=\"T1gM1n4RzGWLFSsJnQ5ziLUW\")\n",
    "#context_callback = FlyteCallbackHandler()\n",
    "context_callback = StdOutCallbackHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "-BFTmJkK-na4"
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.7, verbose=True, openai_api_key = os.environ.get(\"OPENAI_API_KEY\"), streaming=True, callbacks=[context_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "4_aSlIy6F1e8"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "ConversationTokenBufferMemory keeps a buffer of recent interactions in memory,\n",
    "and uses token length rather than number of interactions to determine when to flush interactions.\n",
    "'''\n",
    "memory = ConversationTokenBufferMemory(llm=llm,memory_key=\"chat_history\", return_messages=True,input_key='question',max_token_limit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "BAbvsetCTEIL"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "CONDENSE_QUESTION_PROMPT = '''\n",
    "Below is a summary of the conversation so far, and a new question asked by the user that needs to be answered by searching in a knowledge base.\n",
    "Generate a search query based on the conversation and the new question.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Search query:\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "PromptTemplates = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"question\"],\n",
    "    template=\"\"\"\n",
    "Below is a summary of the conversation so far, and a new question asked by the user that needs to be answered by searching in a knowledge base.\n",
    "Generate a search query based on the conversation and the new question.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Search query:\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "system_message_prompt = SystemMessagePromptTemplate(prompt=PromptTemplates)\n",
    "\n",
    "chat_prompt_for_ques = ChatPromptTemplate.from_messages(\n",
    "    [system_message_prompt])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "wJiRbW27TaZp"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "WiKNdmX3S1Zj"
   },
   "outputs": [],
   "source": [
    "question_generator = LLMChain(llm=llm, prompt=chat_prompt_for_ques, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "CzcMctULXMY9"
   },
   "outputs": [],
   "source": [
    "Answer_Generator_Prompt= '''\n",
    "<Instructions>\n",
    "Important:\n",
    "Answer with the facts listed in the list of sources below. If there isn't enough information below, say you don't know.\n",
    "If asking a clarifying question to the user would help, ask the question.\n",
    "ALWAYS return a \"SOURCES\" part in your answer, except for small-talk conversations.\n",
    "\n",
    "Question: {question}\n",
    "Sources:\n",
    "---------------------\n",
    "    {summaries}\n",
    "---------------------\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "C2yZpFpRVJPD"
   },
   "outputs": [],
   "source": [
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "\n",
    "chat_prompt = PromptTemplate(template=Answer_Generator_Prompt, input_variables=[\"question\", \"summaries\",\"chat_history\"])\n",
    "\n",
    "answer_chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\", verbose=True,prompt=chat_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "WgdFwotlZtyJ"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "\n",
    "chain = ConversationalRetrievalChain(\n",
    "            retriever=reranker,\n",
    "            question_generator=question_generator,\n",
    "            combine_docs_chain=answer_chain,\n",
    "            verbose=True,\n",
    "            memory=memory,\n",
    "            rephrase_question=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ClKEHA56ZxqI",
    "outputId": "d6d1b3c3-fa3d-4e9b-aef7-1894e9d2f630"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "<Instructions>\n",
      "Important:\n",
      "Answer with the facts listed in the list of sources below. If there isn't enough information below, say you don't know.\n",
      "If asking a clarifying question to the user would help, ask the question.\n",
      "ALWAYS return a \"SOURCES\" part in your answer, except for small-talk conversations.\n",
      "\n",
      "Question: What is the Scheduling Policies for HPC cluster?\n",
      "Sources:\n",
      "---------------------\n",
      "    Content:  Job Scripts\n",
      "Slurm Array Jobs and Dependencies\n",
      "Slurm Best Practices\n",
      "\n",
      "\n",
      "HPC for the ClassroomToggle child pages in navigation\n",
      "Classroom HPC: FAQ\n",
      "CPS Class Instructions\n",
      "\n",
      "\n",
      "Best PracticesToggle child pages in navigation\n",
      "Home Directory Storage Quota\n",
      "Checkpointing Jobs\n",
      "Optimizing Job Performance\n",
      "Best SW Practices\n",
      "\n",
      "\n",
      "Tutorials and TrainingToggle child pages in navigation\n",
      "Canvas and GitHub\n",
      "\n",
      "\n",
      "Frequently Asked Questions\n",
      "Glossary\n",
      "\n",
      "Contribution\n",
      "\n",
      "Change Log\n",
      "Report Docs Bug or Request\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "    v: latest\n",
      "  \n",
      "\n",
      "\n",
      "Versions\n",
      "latest\n",
      "2.0.0\n",
      "1.2.0\n",
      "v1.1.0\n",
      "\n",
      "\n",
      "Downloads\n",
      "\n",
      "\n",
      "On Read the Docs\n",
      "\n",
      "Project Home\n",
      "\n",
      "\n",
      "Builds\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Back to top\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Edit this page\n",
      "\n",
      "\n",
      "\n",
      "Toggle Light / Dark / Auto color theme\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Toggle table of contents sidebar\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Job Scheduling Policies and Priorities#\n",
      "In an HPC environment, efficient job scheduling is crucial for allocating computing resources and ensuring optimal cluster utilization. Job scheduling policies and priorities determine the order in which jobs are executed and the resources they receive. Understanding these policies is essential for maximizing job efficiency and minimizing wait times.\n",
      "\n",
      "Scheduling Policies#\n",
      "\n",
      "FIFO (First-In-First-Out)#\n",
      "Jobs are executed in the order they are submitted. Although simple, this policy may lead to long wait times for large, resource-intensive jobs if smaller jobs are constantly being submitted.\n",
      "\n",
      "\n",
      "Fair Share#\n",
      "This policy ensures that all users receive a fair share of cluster resources over time. Users with high resource usage may experience reduced priority, allowing others to access resources more regularly.\n",
      "\n",
      "\n",
      "Priority-Based#\n",
      "Jobs are assigned priorities based on user-defined criteria or system-wide rules. Higher-priority jobs are executed before lower-priority ones, allowing for resource allocation based on user requirements.\n",
      "\n",
      "\n",
      "\n",
      "Job Priorities#\n",
      "\n",
      "User Priority#\n",
      "Users can assign priority values to their jobs. Higher values result in increased job priority and faster access to resources.\n",
      "\n",
      "\n",
      "Resource Requirements#\n",
      "Jobs with larger resource requirements may be assigned higher priority, as they require more significant resources to execute efficiently.\n",
      "\n",
      "\n",
      "Walltime Limit#\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\n",
      "\n",
      "Content: m Job Scripts\n",
      "Slurm Array Jobs and Dependencies\n",
      "Slurm Best Practices\n",
      "\n",
      "\n",
      "HPC for the ClassroomToggle child pages in navigation\n",
      "Classroom HPC: FAQ\n",
      "CPS Class Instructions\n",
      "\n",
      "\n",
      "Best PracticesToggle child pages in navigation\n",
      "Home Directory Storage Quota\n",
      "Checkpointing Jobs\n",
      "Optimizing Job Performance\n",
      "Best SW Practices\n",
      "\n",
      "\n",
      "Tutorials and TrainingToggle child pages in navigation\n",
      "Canvas and GitHub\n",
      "\n",
      "\n",
      "Frequently Asked Questions\n",
      "Glossary\n",
      "\n",
      "Contribution\n",
      "\n",
      "Change Log\n",
      "Report Docs Bug or Request\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "    v: latest\n",
      "  \n",
      "\n",
      "\n",
      "Versions\n",
      "latest\n",
      "2.0.0\n",
      "1.2.0\n",
      "v1.1.0\n",
      "\n",
      "\n",
      "Downloads\n",
      "\n",
      "\n",
      "On Read the Docs\n",
      "\n",
      "Project Home\n",
      "\n",
      "\n",
      "Builds\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Back to top\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Edit this page\n",
      "\n",
      "\n",
      "\n",
      "Toggle Light / Dark / Auto color theme\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Toggle table of contents sidebar\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Frequently Asked Questions (FAQs)#\n",
      "Below are some common questions and answers regarding our HPC cluster and its operation.\n",
      "\n",
      "Cluster Details#\n",
      "\n",
      "\n",
      "What hardware is in the cluster?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The cluster compute and storage capabilities are changing on a regular basis.\n",
      "\n",
      "See also\n",
      "Hardware Overview for detailed information on the HPCs hardware components, along with the partitions comprising subsets of these components.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "What is a cluster?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A cluster is a network of interconnected computers designed to work together as a unified and robust system. These computers, also known as nodes, collaborate to collectively handle complex computational tasks more efficiently than a single machine could achieve. By distributing workloads across multiple nodes, a cluster harnesses parallel computing capabilities, enabling researchers and professionals to solve intricate problems, analyze large datasets, and perform simulations with incredible speed and precision. Clusters are widely used in high-performance computing (HPC) environments, scientific research, data analysis, and other fields that demand significant computing resources.\n",
      "\n",
      "\n",
      "\n",
      "What is the policy on fair use of resources?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The fair use policy for computing resources aims to provide equitable access and distribution within the cluster. This policy ensures that all users have an equal opportunity to utilize the cluster resources based on their needs and priorities.\n",
      "Fair use policies typically utilize a scheduler that allocates resources\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/faq.html\n",
      "\n",
      "Content:  times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.\n",
      "\n",
      "By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.\n",
      "\n",
      "Job Script#A file that contains a series of commands that the HPC cluster will execute.\n",
      "\n",
      "Login Node#A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.\n",
      "\n",
      "Module#In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.\n",
      "\n",
      "Message Passing Interface (MPI)#A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.\n",
      "\n",
      "Node#A single machine within a cluster. A node can have multiple processors and its memory and storage.\n",
      "\n",
      "Node Allocation#The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.\n",
      "\n",
      "Open OnDemand (OOD)#A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.\n",
      "\n",
      "Overcommitment#Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.\n",
      "\n",
      "Package Manager#A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.\n",
      "\n",
      "Parallel Computing#A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.\n",
      "\n",
      "Partition#A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.\n",
      "\n",
      "Quota#A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.\n",
      "\n",
      "Queue#A waiting line for jobs ready to be executed but waiting for resources to become available.\n",
      "\n",
      "Resource Reservation#The process of specifying resources required for a job in advance to ensure availability\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/glossary.html\n",
      "---------------------\n",
      "\n",
      "Chat History:\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Question from user :  What is the Scheduling Policies for HPC cluster? \n",
      "\n",
      "Reply from ChatBot :  The scheduling policies for an HPC (High-Performance Computing) cluster include:\n",
      "1. FIFO (First-In-First-Out): Jobs are executed in the order they are submitted, which may result in long wait times for large jobs if smaller jobs are constantly being submitted.\n",
      "2. Fair Share: This policy ensures that all users receive a fair share of cluster resources over time, even if some users have high resource usage.\n",
      "3. Priority-Based: Jobs are assigned priorities based on user-defined criteria or system-wide rules. Higher-priority jobs are executed before lower-priority ones, allowing for resource allocation based on user requirements.\n",
      "\n",
      "Job priorities in an HPC cluster can be determined by factors such as user priority, resource requirements, and walltime limit.\n",
      "\n",
      "Sources:\n",
      "- \"Job Scheduling Policies and Priorities\" from https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\n",
      "- \"Frequently Asked Questions (FAQs)\" from https://rc-docs.northeastern.edu/en/latest/faq.html\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the Scheduling Policies for HPC cluster?\"\n",
    "result = chain({\"question\": query})\n",
    "\n",
    "\n",
    "print(\"Question from user : \" , query ,\"\\n\")\n",
    "print(\"Reply from ChatBot : \" , result['answer'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gLLL9c7tbgW1",
    "outputId": "766a6bf0-92f3-47ca-d56e-30cb719d395f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "Below is a summary of the conversation so far, and a new question asked by the user that needs to be answered by searching in a knowledge base.\n",
      "Generate a search query based on the conversation and the new question.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: What is the Scheduling Policies for HPC cluster?\n",
      "Assistant: The scheduling policies for an HPC (High-Performance Computing) cluster include:\n",
      "1. FIFO (First-In-First-Out): Jobs are executed in the order they are submitted, which may result in long wait times for large jobs if smaller jobs are constantly being submitted.\n",
      "2. Fair Share: This policy ensures that all users receive a fair share of cluster resources over time, even if some users have high resource usage.\n",
      "3. Priority-Based: Jobs are assigned priorities based on user-defined criteria or system-wide rules. Higher-priority jobs are executed before lower-priority ones, allowing for resource allocation based on user requirements.\n",
      "\n",
      "Job priorities in an HPC cluster can be determined by factors such as user priority, resource requirements, and walltime limit.\n",
      "\n",
      "Sources:\n",
      "- \"Job Scheduling Policies and Priorities\" from https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\n",
      "- \"Frequently Asked Questions (FAQs)\" from https://rc-docs.northeastern.edu/en/latest/faq.html\n",
      "Human: How do I check Job Status?\n",
      "Assistant: SOURCES:\n",
      "- \"Job Scheduling Policies and Priorities\" from https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\n",
      "- \"Frequently Asked Questions (FAQs)\" from https://rc-docs.northeastern.edu/en/latest/faq.html\n",
      "\n",
      "Question:\n",
      "How do I check Job Status?\n",
      "\n",
      "Search query:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "<Instructions>\n",
      "Important:\n",
      "Answer with the facts listed in the list of sources below. If there isn't enough information below, say you don't know.\n",
      "If asking a clarifying question to the user would help, ask the question.\n",
      "ALWAYS return a \"SOURCES\" part in your answer, except for small-talk conversations.\n",
      "\n",
      "Question: How do I check Job Status?\n",
      "Sources:\n",
      "---------------------\n",
      "    Content:  reasons include using too much memory, too many cores, or running past a job’s timelimit.\n",
      "You can run sacct:\n",
      "[user@login-00 ~] sacct\n",
      "       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode\n",
      "------------ ---------- ---------- ---------- ---------- ---------- --------\n",
      "159637       ompi_char+   parallel  hpc_admin         80  COMPLETED      0:0\n",
      "159637.batch      batch             hpc_admin          1  COMPLETED      0:0\n",
      "159637.0          orted             hpc_admin          3  COMPLETED      0:0\n",
      "159638       ompi_char+   parallel  hpc_admin        400    TIMEOUT      0:1\n",
      "159638.batch      batch             hpc_admin          1  CANCELLED     0:15\n",
      "159638.0          orted             hpc_admin         19  CANCELLED  255:126\n",
      "\n",
      "\n",
      "If it’s still not clear why your job was killed, please contact us and send us the output from sacct.\n",
      "\n",
      "\n",
      "\n",
      "How can I submit a job to the HPC cluster?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Jobs can be submitted with sbatch and srun to the HPC.\n",
      "\n",
      "See also\n",
      "Batch Jobs: sbatch and Interactive Jobs: srun Command\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "How can I check the status of my job?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "You can check the status of your job using the squeue -u $USER command, which will display your\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/faq.html\n",
      "\n",
      "Content:  or job names.\n",
      "scontrol hold <jobid>\n",
      "\n",
      "\n",
      "Release a held job, i.e., permit specified job to start (see hold).\n",
      "scontrol release <jobid>\n",
      "\n",
      "\n",
      "Re-queue a completed, failed, or canceled job\n",
      "scontrol requeue <jobid>\n",
      "\n",
      "\n",
      "For more information on the commands listed above, along with a complete list of scontrol commands run below:\n",
      "scontrol --help\n",
      "\n",
      "\n",
      "\n",
      "Syntax: scontrol#\n",
      "scontrol [command] [options]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Example: scontrol#\n",
      "scontrol show jobid -d <JOBID>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Options and Usage: scontrol#\n",
      "\n",
      "update: used to modify job or system configuration\n",
      "hold jobid=<job_id>: hold a specific job\n",
      "release jobid=<job_id>: release a specific job\n",
      "requeue jobid=<job_id>: requeue a specific job\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Examples using scontrol#\n",
      "View information about a specific node:\n",
      "scontrol show node -d <node_name>\n",
      "\n",
      "\n",
      "For information on all reservations, this command will show information about a specific node in the cluster, including the node name, state, number of CPUs, and amount of memory:\n",
      "scontrol show reservations\n",
      "\n",
      "\n",
      "View information about a specific job. This command will show information about a specific job, including the job ID, state, username, and partition name:\n",
      "scontrol show job <job_id>\n",
      "\n",
      "\n",
      "To view information about a specific reservation (e.g., found via scontrol show res listed above), and print information about a specific reservation in the cluster, including the reservation name, start time, end time, and nodes included in the reservation:\n",
      "scontrol show reservation <reservation_name>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Job Management#\n",
      "Managing jobs in a Slurm-based HPC environment involves monitoring running jobs, modifying job parameters, and canceling jobs when necessary. This section will cover the commands and techniques you can use for these tasks.\n",
      "\n",
      "Monitoring Jobs#\n",
      "The squeue command allows you to monitor the state of jobs in the queue. It provides information such as the job ID, the partition it is running on, the job name, and more.\n",
      "\n",
      "Syntax: squeue#\n",
      "squeue [options]\n",
      "\n",
      "\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmmonitoringandmanaging.html\n",
      "\n",
      "Content:  configuration can significantly alleviate the issues you may experience with job runtime. By defining default and maximum time limits, you can establish a predefined window for job execution without explicitly specifying the runtime for each job.\n",
      "However, note that even with the default and maximum time configuration in place, there will always be a time equal to the default time limit where explicitly specifying the job’s runtime becomes helpful. This allows for better control and management of job scheduling within the available resources.\n",
      "If you want to set up the default and maximum time configuration on your partition or have any concerns or questions regarding job runtime management, please let us know. We are here to assist you further.\n",
      "Following these instructions ensures that your job scripts consider the maintenance period and set appropriate time limits. If you have any further questions, feel free to ask!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Next\n",
      "\n",
      "Account Manager\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Previous\n",
      "\n",
      "Getting Started\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                Copyright © 2023, RC\n",
      "            \n",
      "            Made with \n",
      "            Furo\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            On this page\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Getting Access\n",
      "Request Account\n",
      "Sponsor Approval Process\n",
      "\n",
      "\n",
      "Cluster Usage\n",
      "Routine Cluster Maintenance\n",
      "MGHPCC Annual Shutdown\n",
      "IT Statuspage\n",
      "Preparing Cluster Maintenance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/gettingstarted/get_access.html\n",
      "---------------------\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: What is the Scheduling Policies for HPC cluster?\n",
      "Assistant: The scheduling policies for an HPC (High-Performance Computing) cluster include:\n",
      "1. FIFO (First-In-First-Out): Jobs are executed in the order they are submitted, which may result in long wait times for large jobs if smaller jobs are constantly being submitted.\n",
      "2. Fair Share: This policy ensures that all users receive a fair share of cluster resources over time, even if some users have high resource usage.\n",
      "3. Priority-Based: Jobs are assigned priorities based on user-defined criteria or system-wide rules. Higher-priority jobs are executed before lower-priority ones, allowing for resource allocation based on user requirements.\n",
      "\n",
      "Job priorities in an HPC cluster can be determined by factors such as user priority, resource requirements, and walltime limit.\n",
      "\n",
      "Sources:\n",
      "- \"Job Scheduling Policies and Priorities\" from https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\n",
      "- \"Frequently Asked Questions (FAQs)\" from https://rc-docs.northeastern.edu/en/latest/faq.html\n",
      "Human: How do I check Job Status?\n",
      "Assistant: SOURCES:\n",
      "- \"Job Scheduling Policies and Priorities\" from https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\n",
      "- \"Frequently Asked Questions (FAQs)\" from https://rc-docs.northeastern.edu/en/latest/faq.html\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Question from user :  How do I check Job Status? \n",
      "\n",
      "Reply from ChatBot :  You can check the status of your job by using the `sacct` command. Running `sacct` will display the JobID, JobName, Partition, Account, AllocCPUS, State, and ExitCode of your job. The State column will indicate the current status of your job, such as COMPLETED, TIMEOUT, or CANCELLED. If you need further clarification on why your job was killed, you can contact the HPC support team and provide them with the output from `sacct`.\n",
      "\n",
      "SOURCES:\n",
      "- \"Frequently Asked Questions (FAQs)\" from https://rc-docs.northeastern.edu/en/latest/faq.html\n"
     ]
    }
   ],
   "source": [
    "query = \"How do I check Job Status?\"\n",
    "result = chain({\"question\": query})\n",
    "\n",
    "\n",
    "print(\"Question from user : \" , query ,\"\\n\")\n",
    "print(\"Reply from ChatBot : \" , result['answer'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nLT4uLaXZCR_",
    "outputId": "eb839c76-9585-4665-d251-64245f03f0ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain.memory.token_buffer.ConversationTokenBufferMemory"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xoAgKYZNZJ1s",
    "outputId": "98ba51d6-d959-4f17-c2e0-72880d02d197"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationTokenBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='What is the Scheduling Policies for HPC cluster?'), AIMessage(content='The scheduling policies for an HPC (High-Performance Computing) cluster include:\\n1. FIFO (First-In-First-Out): Jobs are executed in the order they are submitted, which may result in long wait times for large jobs if smaller jobs are constantly being submitted.\\n2. Fair Share: This policy ensures that all users receive a fair share of cluster resources over time, even if some users have high resource usage.\\n3. Priority-Based: Jobs are assigned priorities based on user-defined criteria or system-wide rules. Higher-priority jobs are executed before lower-priority ones, allowing for resource allocation based on user requirements.\\n\\nJob priorities in an HPC cluster can be determined by factors such as user priority, resource requirements, and walltime limit.\\n\\nSources:\\n- \"Job Scheduling Policies and Priorities\" from https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\\n- \"Frequently Asked Questions (FAQs)\" from https://rc-docs.northeastern.edu/en/latest/faq.html'), HumanMessage(content='How do I check Job Status?'), AIMessage(content='SOURCES:\\n- \"Job Scheduling Policies and Priorities\" from https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\\n- \"Frequently Asked Questions (FAQs)\" from https://rc-docs.northeastern.edu/en/latest/faq.html'), HumanMessage(content='How do I check Job Status?'), AIMessage(content='You can check the status of your job by using the `sacct` command. Running `sacct` will display the JobID, JobName, Partition, Account, AllocCPUS, State, and ExitCode of your job. The State column will indicate the current status of your job, such as COMPLETED, TIMEOUT, or CANCELLED. If you need further clarification on why your job was killed, you can contact the HPC support team and provide them with the output from `sacct`.\\n\\nSOURCES:\\n- \"Frequently Asked Questions (FAQs)\" from https://rc-docs.northeastern.edu/en/latest/faq.html')]), input_key='question', return_messages=True, llm=ChatOpenAI(verbose=True, callbacks=[<langchain.callbacks.stdout.StdOutCallbackHandler object at 0x2b06d01f82b0>], client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', streaming=True), memory_key='chat_history', max_token_limit=1000)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "5mrfHVdAZQer"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nConversation Memory Buffer & Input Details:\\n------------------------------------------\\n- Input Key: question\\n- Memory Key: chat_history\\n- Maximum Token Limit: 1000\\n- Return Messages: True\\n\\nChatOpenAI Configuration:\\n-------------------------\\n- Verbose: True\\n- Streaming: True\\n- OpenAI API Key: [REDACTED]\\n- OpenAI API Base: (No Value Provided)\\n- OpenAI Organization: (No Value Provided)\\n- OpenAI Proxy: (No Value Provided)\\n- Client: ChatCompletion\\n- Callbacks: FlyteCallbackHandler (Object Reference: 0x7832be39c9d0)\\n\\nConversation History:\\n---------------------\\n1. User: What is Globus at Northeastern?\\n\\n   Assistant: Globus is a data management system that allows you to transfer and share files.\\n   Northeastern has a subscription to Globus, and you can set up a Globus account using your Northeastern credentials.\\n   You can also link your other accounts, whether they are personal or from another institution, with your Globus account.\\n   To use Globus, you will need to set up an account and install Globus Connect on your local computer.\\n   After completing these initial setup procedures, you can use the Globus web app to perform file transfers.\\n\\n   Sources:\\n   - RCDocs (Using Globus)\\n   - RCDocs (Globus Account Set Up)\\n   - RCDocs (To use Globus)\\n\\n2. User: How do i set up an account with Globus?\\n\\n   Assistant: Globus is a data management system that allows you to transfer and share files.\\n   Northeastern has a subscription to Globus, and you can set up an account with Globus using your Northeastern credentials.\\n   You can also link your other accounts, whether they are personal or from another institution, with your Globus account.\\n   To set up an account with Globus, follow these steps:\\n   1. Go to the Globus website.\\n   2. Click on \"Log In\".\\n   3. Select \"Northeastern University\" from the options under \"Use your existing organizational login\" and click \"Continue\".\\n   4. Enter your Northeastern username and password.\\n   5. If you don\\'t have a previous Globus account, click \"Continue\". If you have an existing account, click \"Link to an existing account\".\\n   6. Check the agreement checkbox and click \"Continue\".\\n   7. Click \"Allow\" to permit Globus to access your files.\\n   After setting up your account, you can access the Globus File Manager app.\\n\\n   Sources:\\n   - RCDocs (Using Globus)\\n   - RCDocs (Globus Account Set Up)\\n'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Conversation Memory Buffer & Input Details:\n",
    "------------------------------------------\n",
    "- Input Key: question\n",
    "- Memory Key: chat_history\n",
    "- Maximum Token Limit: 1000\n",
    "- Return Messages: True\n",
    "\n",
    "ChatOpenAI Configuration:\n",
    "-------------------------\n",
    "- Verbose: True\n",
    "- Streaming: True\n",
    "- OpenAI API Key: [REDACTED]\n",
    "- OpenAI API Base: (No Value Provided)\n",
    "- OpenAI Organization: (No Value Provided)\n",
    "- OpenAI Proxy: (No Value Provided)\n",
    "- Client: ChatCompletion\n",
    "- Callbacks: FlyteCallbackHandler (Object Reference: 0x7832be39c9d0)\n",
    "\n",
    "Conversation History:\n",
    "---------------------\n",
    "1. User: What is Globus at Northeastern?\n",
    "\n",
    "   Assistant: Globus is a data management system that allows you to transfer and share files.\n",
    "   Northeastern has a subscription to Globus, and you can set up a Globus account using your Northeastern credentials.\n",
    "   You can also link your other accounts, whether they are personal or from another institution, with your Globus account.\n",
    "   To use Globus, you will need to set up an account and install Globus Connect on your local computer.\n",
    "   After completing these initial setup procedures, you can use the Globus web app to perform file transfers.\n",
    "\n",
    "   Sources:\n",
    "   - RCDocs (Using Globus)\n",
    "   - RCDocs (Globus Account Set Up)\n",
    "   - RCDocs (To use Globus)\n",
    "\n",
    "2. User: How do i set up an account with Globus?\n",
    "\n",
    "   Assistant: Globus is a data management system that allows you to transfer and share files.\n",
    "   Northeastern has a subscription to Globus, and you can set up an account with Globus using your Northeastern credentials.\n",
    "   You can also link your other accounts, whether they are personal or from another institution, with your Globus account.\n",
    "   To set up an account with Globus, follow these steps:\n",
    "   1. Go to the Globus website.\n",
    "   2. Click on \"Log In\".\n",
    "   3. Select \"Northeastern University\" from the options under \"Use your existing organizational login\" and click \"Continue\".\n",
    "   4. Enter your Northeastern username and password.\n",
    "   5. If you don't have a previous Globus account, click \"Continue\". If you have an existing account, click \"Link to an existing account\".\n",
    "   6. Check the agreement checkbox and click \"Continue\".\n",
    "   7. Click \"Allow\" to permit Globus to access your files.\n",
    "   After setting up your account, you can access the Globus File Manager app.\n",
    "\n",
    "   Sources:\n",
    "   - RCDocs (Using Globus)\n",
    "   - RCDocs (Globus Account Set Up)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jo16MuocEVXD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "OqU88pUbEVS1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-09 17:32:00.269435: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /device:GPU:0 with 13605 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:3b:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/device:GPU:0']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-09 17:31:31.349324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /device:GPU:0 with 13605 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:3b:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/device:GPU:0']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Chatbot Environment",
   "language": "python",
   "name": "chatbot_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
