{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"COHERE_API_KEY\"] = \"mVc5EhnC3eDPNFwV3d7ai8IudTmsfijju7h7oxQP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mq7rbJww6Cjo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom langchain.document_loaders import WebBaseLoader\\n\\n\\nurls = [\\n\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/get_access.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/accountmanager.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/connectingtocluster/mac.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/connectingtocluster/windows.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/first_steps/passwordlessssh.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/first_steps/shellenvironment.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/first_steps/usingbash.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/hardware/hardware_overview.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/hardware/partitions.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/using-ood/introduction.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/using-ood/accessingood.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/desktopood.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/fileexplore.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/jupyterlab.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/runningjobs/understandingqueuing.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/runningjobs/workingwithgpus.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/runningjobs/recurringjobs.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/runningjobs/debuggingjobs.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/datamanagement/discovery_storage.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/datamanagement/transferringdata.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/datamanagement/globus.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/datamanagement/databackup.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/datamanagement/securityandcompliance.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/modules.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/mpi.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/r.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/matlab.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/software/packagemanagers/conda.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/software/packagemanagers/spack.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/software/fromsource/makefile.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/software/fromsource/cmake.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/slurmguide/introductiontoslurm.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmcommands.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmrunningjobs.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmmonitoringandmanaging.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmscripts.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmarray.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmbestpractices.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/classroom/class_use.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/classroom/cps_ood.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/classroom/classroomexamples.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/best-practices/homequota.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/best-practices/checkpointing.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/best-practices/optimizingperformance.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/best-practices/software.html\",\\n\"https://rc-docs.northeastern.edu/en/latest/tutorialsandtraining/canvasandgithub.html\",\\n]\\nloader = WebBaseLoader(urls)\\ndata = loader.load()\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "\n",
    "urls = [\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/get_access.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/accountmanager.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/connectingtocluster/mac.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/gettingstarted/connectingtocluster/windows.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/first_steps/passwordlessssh.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/first_steps/shellenvironment.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/first_steps/usingbash.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/hardware/hardware_overview.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/hardware/partitions.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/introduction.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/accessingood.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/desktopood.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/fileexplore.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/jupyterlab.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/understandingqueuing.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/workingwithgpus.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/recurringjobs.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/runningjobs/debuggingjobs.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/discovery_storage.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/transferringdata.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/globus.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/databackup.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/datamanagement/securityandcompliance.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/modules.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/mpi.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/r.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/systemwide/matlab.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/packagemanagers/conda.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/packagemanagers/spack.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/fromsource/makefile.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/software/fromsource/cmake.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/introductiontoslurm.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmcommands.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmrunningjobs.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmmonitoringandmanaging.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmscripts.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmarray.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmbestpractices.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/classroom/class_use.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/classroom/cps_ood.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/classroom/classroomexamples.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/best-practices/homequota.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/best-practices/checkpointing.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/best-practices/optimizingperformance.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/best-practices/software.html\",\n",
    "\"https://rc-docs.northeastern.edu/en/latest/tutorialsandtraining/canvasandgithub.html\",\n",
    "]\n",
    "loader = WebBaseLoader(urls)\n",
    "data = loader.load()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport re\\nfrom langchain.text_splitter import TokenTextSplitter\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\n# Split the data string by URLs\\npattern = r'(https://[^\\\\s]+)\\\\s(.*?)($|https://)'\\nmatches = re.findall(pattern, data, re.DOTALL)\\n\\n# Organize data into a dictionary {url: content}\\ndata_dict = {match[0]: match[1].strip() for match in matches}\\n\\n# Use RecursiveCharacterTextSplitter to split the content\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\n\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    chunk_size = 700,\\n    chunk_overlap  = 70,\\n    length_function = len,\\n    add_start_index = True,\\n)\\n\\nall_docs = []\\n\\nfor url, content in data_dict.items():\\n    docs = text_splitter.create_documents([content])\\n    for doc in docs:\\n        doc.metadata['source'] = url\\n        all_docs.append(doc)\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file=open(\"rcdocs_processed.txt\",\"r\")\n",
    "data = file.read()\n",
    "file.close()\n",
    "\n",
    "'''\n",
    "import re\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# Split the data string by URLs\n",
    "pattern = r'(https://[^\\s]+)\\s(.*?)($|https://)'\n",
    "matches = re.findall(pattern, data, re.DOTALL)\n",
    "\n",
    "# Organize data into a dictionary {url: content}\n",
    "data_dict = {match[0]: match[1].strip() for match in matches}\n",
    "\n",
    "# Use RecursiveCharacterTextSplitter to split the content\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 700,\n",
    "    chunk_overlap  = 70,\n",
    "    length_function = len,\n",
    "    add_start_index = True,\n",
    ")\n",
    "\n",
    "all_docs = []\n",
    "\n",
    "for url, content in data_dict.items():\n",
    "    docs = text_splitter.create_documents([content])\n",
    "    for doc in docs:\n",
    "        doc.metadata['source'] = url\n",
    "        all_docs.append(doc)\n",
    "'''    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "#Local mode, without using the Qdrant server, may also store your vectors on disk so they're persisted between runs.\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "from langchain.chains.conversation.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "\n",
    "from langchain.schema.vectorstore import VectorStoreRetriever\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "import time\n",
    "\n",
    "\n",
    "#from langchain.vectorstores import Pinecone\n",
    "\n",
    "\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BAbvsetCTEIL"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "'''\n",
    "PromptTemplates = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"question\"],\n",
    "    template=\"\"\"\n",
    "Below is a summary of the conversation so far, and a new question asked by the user that needs to be answered by searching in a knowledge base.\n",
    "Generate a question based on the conversation chat history and the new question.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "\n",
    "New Question:\n",
    "{question}\n",
    "\n",
    "Generated question:\"\"\"\n",
    ")\n",
    "'''\n",
    "\n",
    "PromptTemplates = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"question\"],\n",
    "    template=\"\"\"\n",
    "[GENERATE QUESTION]\n",
    "\n",
    "***Conversation Summary***\n",
    "Chat History:\n",
    "{chat_history}\n",
    "\n",
    "User's New Question:\n",
    "{question}\n",
    "\n",
    "Given the Chat History and a User's New Question, rephrase the new question to be a standalone question. \"\"\"\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "CzcMctULXMY9"
   },
   "outputs": [],
   "source": [
    "\n",
    "Answer_Generator_Prompt= '''\n",
    "<Instructions>\n",
    "Important:\n",
    "Answer like a technical assistant with the facts from the context and chat history given below. If there isn't enough information in context, say you don't know.\n",
    "If asking a clarifying question to the user would help, ask the question.\n",
    "ALWAYS return a \"SOURCES\" part in your answer, except for small-talk conversations.\n",
    "\n",
    "Question: {question}\n",
    "Context:\n",
    "---------------------\n",
    "    {summaries}\n",
    "---------------------\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "\n",
    "</Instructions>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer_Generator_Prompt = '''\n",
    "[TECHNICAL ASSISTANT RESPONSE]\n",
    "\n",
    "***Instructions***\n",
    "    - Respond with factual information from the provided context and chat history.\n",
    "- If there's insufficient context, mention you don't have enough information.\n",
    "- If a clarifying question would help, don't hesitate to ask.\n",
    "- Always include a \"SOURCES\" section in the response, unless it's a small-talk.\n",
    "\n",
    "***Details***\n",
    "Question: {question}\n",
    "\n",
    "Context:\n",
    "---------------------\n",
    "{summaries}\n",
    "---------------------\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "chunk_sizes = [400 ,800 , 1000]\n",
    "chunk_overlaps = [30 , 100 , 200 ]\n",
    "\n",
    "k_values = [3,5,7]\n",
    "fetch_k_values = []\n",
    "lambda_mult_values = [ 0.5 ,0.75  , 0.9]\n",
    "filter_values = []\n",
    "search_params_values = []\n",
    "score_threshold_values= [0.6,0.8,0.9]\n",
    "\n",
    "\n",
    "input_path = 'temp_sample_questions.xlsx'\n",
    "df_input = pd.read_excel(input_path)\n",
    "queries = df_input['queries'].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5mrfHVdAZQer",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current values are chunk_size =  400  chunk_overlap =  80  k_value =  5  lambda_mult_value =  0.75\n",
      "Qdrantdb with  RC_documents1  completed\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "running query  What is the Scheduling Policies for HPC cluster?\n",
      "output_chunk_size_400_overlap_80_k_value_5_lambda_mult_value_0.75\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "[TECHNICAL ASSISTANT RESPONSE]\n",
      "\n",
      "***Instructions***\n",
      "    - Respond with factual information from the provided context and chat history.\n",
      "- If there's insufficient context, mention you don't have enough information.\n",
      "- If a clarifying question would help, don't hesitate to ask.\n",
      "- Always include a \"SOURCES\" section in the response, unless it's a small-talk.\n",
      "\n",
      "***Details***\n",
      "Question: What is the Scheduling Policies for HPC cluster?\n",
      "\n",
      "Context:\n",
      "---------------------\n",
      "Content: Scheduling Policies**#\n",
      "Our cluster uses a fair-share scheduling policy. This means that usage is tracked for each user or group, and the system attempts to balance resource allocation over time. If a user or group has been using many resources, their job priority may be temporarily reduced to allow others to use the system. Conversely, users or groups that have used fewer resources will have their jobs prioritized.\n",
      "The following policies ensure fair use of the cluster resources:\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/runningjobs/understandingqueuing.html\n",
      "\n",
      "Content: How do I get access?\n",
      "\n",
      "The HPC resources are available to Northeastern University research faculty, and classes that require high-performance computing for their coursework, and students. For detailed information about reporting requirements and any access restrictions, please refer to our Getting Access page. If you have questions about our access policy or procedures, please contact RC.\n",
      "\n",
      "What is my cluster username?\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/tutorialsandtraining/../faq.html\n",
      "\n",
      "Content: How can I get information on the HPC such as how busy it is?\n",
      "\n",
      "From a login node, you can run sinfo -p short to get the state of the HPC.\n",
      "\n",
      "\n",
      "Back to the top\n",
      "\n",
      "Data Transfer#\n",
      "\n",
      "How can I transfer data to/from the HPC cluster?\n",
      "\n",
      "Data transfer can be done using various methods like scp, rsync, or Globus. Refer to the ‘Transferring Data’ section in ‘Data Management’ for detailed instructions.\n",
      "\n",
      "\n",
      "\n",
      "What Linux commands can I use to transfer files to/from the cluster?\n",
      "\n",
      "Smaller files can be transferred to/from the cluster using scp, sftp, and rsync as well as standard FTP tools utilizing xfer.discovery.neu.edu.\n",
      "Larger files should be moved using Globus.\n",
      "\n",
      "\n",
      "\n",
      "Back to the top\n",
      "\n",
      "Job Management#\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/tutorialsandtraining/../faq.html\n",
      "---------------------\n",
      "\n",
      "Chat History:\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "answer is  User: What is the Scheduling Policies for HPC cluster?\n",
      "\n",
      "Assistant: The scheduling policies for the HPC cluster at Northeastern University follow a fair-share scheduling policy. This means that usage is tracked for each user or group, and the system attempts to balance resource allocation over time. If a user or group has been using many resources, their job priority may be temporarily reduced to allow others to use the system. Conversely, users or groups that have used fewer resources will have their jobs prioritized. You can find more information about the scheduling policies on the Northeastern University Research Computing website. [^1^]\n",
      "\n",
      "SOURCES:\n",
      "[^1^]: https://rc-docs.northeastern.edu/en/latest/runningjobs/understandingqueuing.html\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "running query  Can you tell me about FIFO?\n",
      "output_chunk_size_400_overlap_80_k_value_5_lambda_mult_value_0.75\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "[GENERATE QUESTION]\n",
      "\n",
      "***Conversation Summary***\n",
      "Chat History:\n",
      "\n",
      "Human: What is the Scheduling Policies for HPC cluster?\n",
      "Assistant: User: What is the Scheduling Policies for HPC cluster?\n",
      "\n",
      "Assistant: The scheduling policies for the HPC cluster at Northeastern University follow a fair-share scheduling policy. This means that usage is tracked for each user or group, and the system attempts to balance resource allocation over time. If a user or group has been using many resources, their job priority may be temporarily reduced to allow others to use the system. Conversely, users or groups that have used fewer resources will have their jobs prioritized. You can find more information about the scheduling policies on the Northeastern University Research Computing website. [^1^]\n",
      "\n",
      "SOURCES:\n",
      "[^1^]: https://rc-docs.northeastern.edu/en/latest/runningjobs/understandingqueuing.html\n",
      "\n",
      "User's New Question:\n",
      "Can you tell me about FIFO?\n",
      "\n",
      "Given the Chat History and a User's New Question, rephrase the new question to be a standalone question. \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "[TECHNICAL ASSISTANT RESPONSE]\n",
      "\n",
      "***Instructions***\n",
      "    - Respond with factual information from the provided context and chat history.\n",
      "- If there's insufficient context, mention you don't have enough information.\n",
      "- If a clarifying question would help, don't hesitate to ask.\n",
      "- Always include a \"SOURCES\" section in the response, unless it's a small-talk.\n",
      "\n",
      "***Details***\n",
      "Question: Can you tell me about FIFO?\n",
      "\n",
      "Context:\n",
      "---------------------\n",
      "Content: Additionally, we offer a selection of modified standard applications intended to support specific coursework. These applications are under the Courses menu on the OOD web interface. Please note that these course-specific applications are only accessible to students enrolled in the respective courses.\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/using-ood/interactiveapps/index.html\n",
      "\n",
      "Content: MPI Communication: Send and Receive Operations#\n",
      "MPI allows processes to communicate by sending and receiving messages. These messages can contain any type of data. Here is a simple example of using MPI_Send and MPI_Recv to send a number from one process to another:\n",
      "#include <mpi.h>\n",
      "#include <stdio.h>\n",
      "\n",
      "int main(int argc, char** argv) {\n",
      "    MPI_Init(NULL, NULL);\n",
      "\n",
      "    int world_rank;\n",
      "    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/hardware/hardware_overview.html\n",
      "\n",
      "Content: What is the policy on fair use of resources?\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/tutorialsandtraining/../faq.html\n",
      "---------------------\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: What is the Scheduling Policies for HPC cluster?\n",
      "Assistant: User: What is the Scheduling Policies for HPC cluster?\n",
      "\n",
      "Assistant: The scheduling policies for the HPC cluster at Northeastern University follow a fair-share scheduling policy. This means that usage is tracked for each user or group, and the system attempts to balance resource allocation over time. If a user or group has been using many resources, their job priority may be temporarily reduced to allow others to use the system. Conversely, users or groups that have used fewer resources will have their jobs prioritized. You can find more information about the scheduling policies on the Northeastern University Research Computing website. [^1^]\n",
      "\n",
      "SOURCES:\n",
      "[^1^]: https://rc-docs.northeastern.edu/en/latest/runningjobs/understandingqueuing.html\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "answer is  The scheduling policies for the HPC cluster at Northeastern University follow a fair-share scheduling policy. This means that usage is tracked for each user or group, and the system attempts to balance resource allocation over time. If a user or group has been using many resources, their job priority may be temporarily reduced to allow others to use the system. Conversely, users or groups that have used fewer resources will have their jobs prioritized. You can find more information about the scheduling policies on the Northeastern University Research Computing website. [^1^]\n",
      "\n",
      "SOURCES:\n",
      "[^1^]: https://rc-docs.northeastern.edu/en/latest/runningjobs/understandingqueuing.html\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "running query  How do i check the job queue?\n",
      "output_chunk_size_400_overlap_80_k_value_5_lambda_mult_value_0.75\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "[GENERATE QUESTION]\n",
      "\n",
      "***Conversation Summary***\n",
      "Chat History:\n",
      "\n",
      "Human: What is the Scheduling Policies for HPC cluster?\n",
      "Assistant: User: What is the Scheduling Policies for HPC cluster?\n",
      "\n",
      "Assistant: The scheduling policies for the HPC cluster at Northeastern University follow a fair-share scheduling policy. This means that usage is tracked for each user or group, and the system attempts to balance resource allocation over time. If a user or group has been using many resources, their job priority may be temporarily reduced to allow others to use the system. Conversely, users or groups that have used fewer resources will have their jobs prioritized. You can find more information about the scheduling policies on the Northeastern University Research Computing website. [^1^]\n",
      "\n",
      "SOURCES:\n",
      "[^1^]: https://rc-docs.northeastern.edu/en/latest/runningjobs/understandingqueuing.html\n",
      "Human: Can you tell me about FIFO?\n",
      "Assistant: The scheduling policies for the HPC cluster at Northeastern University follow a fair-share scheduling policy. This means that usage is tracked for each user or group, and the system attempts to balance resource allocation over time. If a user or group has been using many resources, their job priority may be temporarily reduced to allow others to use the system. Conversely, users or groups that have used fewer resources will have their jobs prioritized. You can find more information about the scheduling policies on the Northeastern University Research Computing website. [^1^]\n",
      "\n",
      "SOURCES:\n",
      "[^1^]: https://rc-docs.northeastern.edu/en/latest/runningjobs/understandingqueuing.html\n",
      "\n",
      "User's New Question:\n",
      "How do i check the job queue?\n",
      "\n",
      "Given the Chat History and a User's New Question, rephrase the new question to be a standalone question. \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "[TECHNICAL ASSISTANT RESPONSE]\n",
      "\n",
      "***Instructions***\n",
      "    - Respond with factual information from the provided context and chat history.\n",
      "- If there's insufficient context, mention you don't have enough information.\n",
      "- If a clarifying question would help, don't hesitate to ask.\n",
      "- Always include a \"SOURCES\" section in the response, unless it's a small-talk.\n",
      "\n",
      "***Details***\n",
      "Question: How do i check the job queue?\n",
      "\n",
      "Context:\n",
      "---------------------\n",
      "Content: How can I get information on the HPC such as how busy it is?\n",
      "\n",
      "From a login node, you can run sinfo -p short to get the state of the HPC.\n",
      "\n",
      "\n",
      "Back to the top\n",
      "\n",
      "Data Transfer#\n",
      "\n",
      "How can I transfer data to/from the HPC cluster?\n",
      "\n",
      "Data transfer can be done using various methods like scp, rsync, or Globus. Refer to the ‘Transferring Data’ section in ‘Data Management’ for detailed instructions.\n",
      "\n",
      "\n",
      "\n",
      "What Linux commands can I use to transfer files to/from the cluster?\n",
      "\n",
      "Smaller files can be transferred to/from the cluster using scp, sftp, and rsync as well as standard FTP tools utilizing xfer.discovery.neu.edu.\n",
      "Larger files should be moved using Globus.\n",
      "\n",
      "\n",
      "\n",
      "Back to the top\n",
      "\n",
      "Job Management#\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/tutorialsandtraining/../faq.html\n",
      "\n",
      "Content: Jobs can be submitted with sbatch and srun to the HPC.\n",
      "\n",
      "\n",
      "\n",
      "How can I check the status of my job?\n",
      "\n",
      "You can check the status of your job using the squeue -u $USER command, which will display your current jobs in the queue.\n",
      "\n",
      "\n",
      "\n",
      "What should I do if my job fails?\n",
      "\n",
      "Check the output and error files for any error messages if your job fails. These files are usually located in your job’s working directory. If you cannot resolve the issue yourself, contact Research Computing with the details of the error message.\n",
      "\n",
      "When will my job start?\n",
      "\n",
      "You can list information on your job’s start time using the squeuegit  command:\n",
      "squeue -u $USER --start\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/tutorialsandtraining/../faq.html\n",
      "\n",
      "Content: How do I reset my password?\n",
      "\n",
      "Access to the HPC cluster requires a valid Northeastern University email account. Your HPC password is the same as your Northeastern University account password so in order to reset it, reset your Northeastern University account password.\n",
      "\n",
      "\n",
      "\n",
      "How can I view .pdf or .csv files on the cluster?\n",
      "\n",
      "You can view .pdf and .csv files utilizing the File Explorer application on Open OnDemand and the Open OnDemand Desktop Application.\n",
      "\n",
      "\n",
      "\n",
      "How do I request all CPUs on a node with more than one GPU?\n",
      "Source: https://rc-docs.northeastern.edu/en/latest/tutorialsandtraining/../faq.html\n",
      "---------------------\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: What is the Scheduling Policies for HPC cluster?\n",
      "Assistant: User: What is the Scheduling Policies for HPC cluster?\n",
      "\n",
      "Assistant: The scheduling policies for the HPC cluster at Northeastern University follow a fair-share scheduling policy. This means that usage is tracked for each user or group, and the system attempts to balance resource allocation over time. If a user or group has been using many resources, their job priority may be temporarily reduced to allow others to use the system. Conversely, users or groups that have used fewer resources will have their jobs prioritized. You can find more information about the scheduling policies on the Northeastern University Research Computing website. [^1^]\n",
      "\n",
      "SOURCES:\n",
      "[^1^]: https://rc-docs.northeastern.edu/en/latest/runningjobs/understandingqueuing.html\n",
      "Human: Can you tell me about FIFO?\n",
      "Assistant: The scheduling policies for the HPC cluster at Northeastern University follow a fair-share scheduling policy. This means that usage is tracked for each user or group, and the system attempts to balance resource allocation over time. If a user or group has been using many resources, their job priority may be temporarily reduced to allow others to use the system. Conversely, users or groups that have used fewer resources will have their jobs prioritized. You can find more information about the scheduling policies on the Northeastern University Research Computing website. [^1^]\n",
      "\n",
      "SOURCES:\n",
      "[^1^]: https://rc-docs.northeastern.edu/en/latest/runningjobs/understandingqueuing.html\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "answer is  The scheduling policies for the HPC cluster at Northeastern University follow a fair-share scheduling policy. This means that usage is tracked for each user or group, and the system attempts to balance resource allocation over time. If a user or group has been using many resources, their job priority may be temporarily reduced to allow others to use the system. Conversely, users or groups that have used fewer resources will have their jobs prioritized. You can find more information about the scheduling policies on the Northeastern University Research Computing website. [^1^]\n",
      "\n",
      "SOURCES:\n",
      "[^1^]: https://rc-docs.northeastern.edu/en/latest/runningjobs/understandingqueuing.html\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 103\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer is \u001b[39m\u001b[38;5;124m\"\u001b[39m , result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    102\u001b[0m                     \u001b[38;5;66;03m#answers.append(result['answer'])\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m                     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m                 \u001b[38;5;66;03m#df_output = pd.DataFrame({'queries': queries, 'answers': answers})\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \n\u001b[1;32m    107\u001b[0m                 \u001b[38;5;66;03m# Save the output to an Excel sheet\u001b[39;00m\n\u001b[1;32m    108\u001b[0m                 \u001b[38;5;66;03m#output_filename = f'output_chunk_size_{chunk_size}_overlap_{chunk_overlap}_k_value_{k_value}_lambda_mult_value_{lambda_mult_value}.xlsx'\u001b[39;00m\n\u001b[1;32m    109\u001b[0m                 \u001b[38;5;66;03m#df_output.to_excel(output_filename, index=False)\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished processing all combinations!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "for chunk_size in chunk_sizes:\n",
    "    for chunk_overlap in chunk_overlaps:\n",
    "        for k_value in k_values:\n",
    "            for lambda_mult_value in lambda_mult_values:\n",
    "                \n",
    "            \n",
    "                print(\"current values are chunk_size = \" , chunk_size,\" chunk_overlap = \" ,chunk_overlap ,\" k_value = \",k_value,\" lambda_mult_value = \" ,lambda_mult_value)\n",
    "                i = i + 1 \n",
    "\n",
    "                #text_splitter = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "                #docs = text_splitter.split_documents(data)\n",
    "                \n",
    "                import re\n",
    "                from langchain.text_splitter import TokenTextSplitter\n",
    "                from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "                # Split the data string by URLs\n",
    "                pattern = r'(https://[^\\s]+)\\s(.*?)($|https://)'\n",
    "                matches = re.findall(pattern, data, re.DOTALL)\n",
    "\n",
    "                # Organize data into a dictionary {url: content}\n",
    "                data_dict = {match[0]: match[1].strip() for match in matches}\n",
    "\n",
    "                # Use RecursiveCharacterTextSplitter to split the content\n",
    "                from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "                text_splitter = RecursiveCharacterTextSplitter(\n",
    "                    chunk_size = chunk_size,\n",
    "                    chunk_overlap  = chunk_overlap,\n",
    "                    length_function = len,\n",
    "                    add_start_index = True,\n",
    "                )\n",
    "\n",
    "                docs = []\n",
    "\n",
    "                for url, content in data_dict.items():\n",
    "                    docs_new = text_splitter.create_documents([content])\n",
    "                    for doc in docs_new:\n",
    "                        doc.metadata['source'] = url\n",
    "                        docs.append(doc)\n",
    "        \n",
    "        \n",
    "                name  = \"RC_documents\" + str(i)\n",
    "                Qdrantdb = Qdrant.from_documents(\n",
    "                    docs,\n",
    "                    embeddings,\n",
    "                    path=\"/work/rc/projects/chatbot/chatbotrc/notebooks/RAG/tmp/local_qdrant\"+name,\n",
    "                    collection_name=name ,\n",
    "                )\n",
    "                print(\"Qdrantdb with \" , name , \" completed\" )\n",
    "\n",
    "\n",
    "                retriever = VectorStoreRetriever(vectorstore=Qdrantdb, search_type=\"mmr\", search_kwargs={'k': k_value, 'fetch_k': 10 , 'lambda_mult' : lambda_mult_value},)\n",
    "                compressor = CohereRerank()\n",
    "\n",
    "                reranker = ContextualCompressionRetriever(\n",
    "                    base_compressor=compressor, base_retriever=retriever)\n",
    "\n",
    "                context_callback = StdOutCallbackHandler()\n",
    "\n",
    "                llm = ChatOpenAI(temperature=0.5, verbose=True, openai_api_key = os.environ.get(\"OPENAI_API_KEY\"), streaming=True, callbacks=[context_callback])\n",
    "                memory = ConversationSummaryBufferMemory(llm=llm,memory_key=\"chat_history\", return_messages=True,input_key='question',max_token_limit=1000)\n",
    "                system_message_prompt = SystemMessagePromptTemplate(prompt=PromptTemplates)\n",
    "\n",
    "                chat_prompt_for_ques = ChatPromptTemplate.from_messages([system_message_prompt])\n",
    "                question_generator = LLMChain(llm=llm, prompt=chat_prompt_for_ques, verbose=True)\n",
    "                chat_prompt = PromptTemplate(template=Answer_Generator_Prompt, input_variables=[\"question\", \"summaries\",\"chat_history\"])\n",
    "                answer_chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\", verbose=True,prompt=chat_prompt)\n",
    "                \n",
    "                chain = ConversationalRetrievalChain(\n",
    "                    retriever=reranker,\n",
    "                    question_generator=question_generator,\n",
    "                    combine_docs_chain=answer_chain,\n",
    "                    verbose=True,\n",
    "                    memory=memory,\n",
    "                    rephrase_question=False )\n",
    "                \n",
    "                \n",
    "                    \n",
    "                \n",
    "                \n",
    "                '''\n",
    "                model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.3)\n",
    "                embeddings = OpenAIEmbeddings()\n",
    "                vectordb = Chroma(embedding_function=embeddings, persist_directory=directory)\n",
    "                memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "                qa = ConversationalRetrievalChain.from_llm(\n",
    "                    model,\n",
    "                    vectordb.as_retriever(),\n",
    "                    condense_question_prompt=CUSTOM_QUESTION_PROMPT,\n",
    "                    memory=memory\n",
    "                )\n",
    "                '''\n",
    "\n",
    "                # Iterate over queries and save results\n",
    "                answers = []\n",
    "                for query in queries:\n",
    "                    print(\"\\n\\n\\n\")\n",
    "                    print(\"running query \" , query)\n",
    "                    print(f'output_chunk_size_{chunk_size}_overlap_{chunk_overlap}_k_value_{k_value}_lambda_mult_value_{lambda_mult_value}')\n",
    "                    result = chain({\"question\": query})\n",
    "                    print(\"answer is \" , result['answer'])\n",
    "                    #answers.append(result['answer'])\n",
    "                    time.sleep(60)\n",
    "\n",
    "                #df_output = pd.DataFrame({'queries': queries, 'answers': answers})\n",
    "\n",
    "                # Save the output to an Excel sheet\n",
    "                #output_filename = f'output_chunk_size_{chunk_size}_overlap_{chunk_overlap}_k_value_{k_value}_lambda_mult_value_{lambda_mult_value}.xlsx'\n",
    "                #df_output.to_excel(output_filename, index=False)\n",
    "                del memory\n",
    "print(\"Finished processing all combinations!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jo16MuocEVXD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OqU88pUbEVS1"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "_values\n",
    "k_values = [2,4,6,8]\n",
    "fetch_k_values = []\n",
    "lambda_mult_values = [0.5 , 0.75 , 0.9]\n",
    "filter_values = []\n",
    "search_params_values = []\n",
    "score_threshold_values = [0.6,0.8,0.9]\n",
    "\n",
    "#retriever = VectorStoreRetriever(vectorstore=Qdrantdb, search_type=\"mmr\", search_kwargs={'k': k_values, 'fetch_k': fetch_k_values},)\n",
    "\n",
    "param k: Number of Documents to return. Defaults to 4. :\n",
    "param fetch_k: Number of Documents to fetch to pass to MMR algorithm. Defaults to 20. :\n",
    "param lambda_mult: Number between 0 and 1 that determines the degree of diversity among the results with 0 corresponding to maximum diversity and 1 to minimum diversity. Defaults to 0.5. :\n",
    "param filter: Filter by metadata. Defaults to None. :\n",
    "param search_params: Additional search params :\n",
    "param score_threshold: Define a minimal score threshold for the result. If defined, less similar results will not be returned. \n",
    "\n",
    "docsearch.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={'k': 6, 'lambda_mult': 0.25}\n",
    ")\n",
    "\n",
    "# Fetch more documents for the MMR algorithm to consider\n",
    "# But only return the top 5\n",
    "docsearch.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={'k': 5, 'fetch_k': 50}\n",
    ")\n",
    "\n",
    "# Only retrieve documents that have a relevance score\n",
    "# Above a certain threshold\n",
    "docsearch.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={'score_threshold': 0.8}\n",
    ")\n",
    "\n",
    "# Only get the single most similar document from the dataset\n",
    "docsearch.as_retriever(search_kwargs={'k': 1})\n",
    "\n",
    "# Use a filter to only retrieve documents from a specific paper\n",
    "docsearch.as_retriever(\n",
    "    search_kwargs={'filter': {'paper_title':'GPT-4 Technical Report'}}\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Chatbot Environment",
   "language": "python",
   "name": "chatbot_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
