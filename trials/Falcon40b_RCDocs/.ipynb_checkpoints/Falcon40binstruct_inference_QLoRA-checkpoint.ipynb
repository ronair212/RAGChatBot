{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac44f0ce-247e-49e0-9789-8f5bbed25f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/r.nair/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "CUDA SETUP: CUDA runtime path found: /shared/centos7/cuda/11.8/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /home/r.nair/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/r.nair/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /shared/centos7/anaconda3/2022.05 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import pprint\n",
    "import bitsandbytes as bnb\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe30c8c0-c580-4ce9-9e4a-22de904a11c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import(\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb09e7da-8c67-4271-8815-5cbe9462d4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61832ca0-111e-4265-8aab-6d054af6fffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2958e2f79fc493d8ebf80b37019ae2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06a3702d-cf22-441b-b904-392535f9f959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a configuration object for BitsAndBytes quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, #4bit quantizaition - load_in_4bit is used to load models in 4-bit quantization \n",
    "    bnb_4bit_use_double_quant=True, #nested quantization technique for even greater memory efficiency without sacrificing performance. This technique has proven beneficial, especially when fine-tuning large models\n",
    "    bnb_4bit_quant_type=\"nf4\", #quantization type used is 4 bit Normal Float Quantization- The NF4 data type is designed for weights initialized using a normal distribution\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, #modify the data type used during computation. This can result in speed improvements. \n",
    "    )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31c7508-4438-4360-82a3-0eb10ece668c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b26cf00-08ce-47a7-ad64-d6ff0a88e421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac88f5e-4d45-40f1-9c32-75b78af6a3b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf51d9e-fe80-40f4-8c95-7b7e4d2e2eb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed1996c7-1e3c-44a1-a8c1-cf5bf8c84e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model directly from the saved directory.\n",
    "model_dir = \"Falcon40b-instruct_RCdataset-trained-model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8962698-aa0e-4022-b514-9c3432c5943e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the configuration for the trained model\n",
    "config = PeftConfig.from_pretrained(model_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b346d125-89a4-4921-883a-e247afe5d20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff11f5d807cd456abff82842e56bc295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the trained model using the loaded configuration and other parameters\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "\tconfig.base_model_name_or_path,\n",
    "\treturn_dict=True,\n",
    "\tquantization_config=bnb_config,\n",
    "\tdevice_map=\"auto\",\n",
    "\ttrust_remote_code=True,\n",
    "    cache_dir='/work/rc/projects/chatbot/models',\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cfbb1a2-c3ec-4f63-bf28-15b2000c4ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer for the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d2e84fd-56b3-46ff-bbe8-83caa4e62a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the padding token of the tokenizer to its end-of-sentence token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04fcbcea-fac2-4a42-8f5b-9eb079d298fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set and configure the generation settings for the model\n",
    "model =  PeftModel.from_pretrained(model, model_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1af00276-8c04-40a2-bac4-7d6fdb969c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 200\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "071fb85e-6510-4a2d-98f6-fa0ada2347d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda:0\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea7e826a-462e-4a74-8e2c-a005623a0d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-20 13:35:57.458830: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-20 13:35:59.359260: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human>: How do I run JupyterLab Notebook on OOD?\n",
      "<assistant>: To run JupyterLab Notebook on OOD, you can follow these steps:\n",
      "\n",
      "1. Open a terminal window.\n",
      "2. Type the command \"jupyter lab\" and press enter.\n",
      "3. Wait for the notebook to start up and then click on the \"Open in a new tab\" button.\n",
      "4. You should now be able to use JupyterLab Notebook on OOD.\n",
      "\n",
      "If you encounter any issues, please let us know and we will be happy to assist you.\n",
      "CPU times: user 32.7 s, sys: 22.1 s, total: 54.8 s\n",
      "Wall time: 57.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt = f\"\"\"\n",
    "<human>: How do I run JupyterLab Notebook on OOD?\n",
    "<assistant>:\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "with torch.inference_mode():\n",
    "\toutputs = model.generate(\n",
    "\t\tinput_ids=encoding.input_ids,\n",
    "\t\tattention_mask=encoding.attention_mask,\n",
    "\t\tgeneration_config=generation_config,\n",
    "\t)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26453c9d-3ccf-4005-94c5-6bddc173c953",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Chatbot Environment",
   "language": "python",
   "name": "chatbot_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
